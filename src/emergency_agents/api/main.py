#!/usr/bin/env python3
# Copyright 2025 msq
from __future__ import annotations

import inspect
import uuid
import asyncio
from contextlib import suppress
from datetime import datetime
from enum import Enum
from typing import Any, Awaitable, Callable, Dict, List, Mapping, Optional, Literal

import structlog
from fastapi import FastAPI, UploadFile, File, HTTPException, WebSocket, Request
from starlette.middleware.base import BaseHTTPMiddleware
from prometheus_fastapi_instrumentator import Instrumentator
from pydantic import BaseModel, Field
from prometheus_client import Counter, Histogram
from psycopg.rows import DictRow
from psycopg_pool import AsyncConnectionPool, ConnectionPool

from emergency_agents.config import AppConfig
from emergency_agents.logging import configure_logging, set_trace_id, clear_trace_id  # ç»Ÿä¸€æ—¥å¿—é…ç½®
from emergency_agents.api.voice_chat import handle_voice_chat, voice_chat_handler
from emergency_agents.api import rescue as rescue_api
from emergency_agents.api import plan as plan_api
from emergency_agents.api import overall_rescue as overall_rescue_api
from emergency_agents.api import recon_priority as recon_priority_api
from emergency_agents.api import recon_batch_weather as recon_batch_weather_api
from emergency_agents.api import sitrep as sitrep_api
from emergency_agents.api import reports as reports_api
from emergency_agents.api import recon as recon_api
from emergency_agents.external.adapter_client import AdapterHubClient
from emergency_agents.external.amap_client import AmapClient
from emergency_agents.external.orchestrator_client import OrchestratorClient
from emergency_agents.graph.app import build_app
from emergency_agents.graph.intent_orchestrator_app import build_intent_orchestrator_graph
from emergency_agents.graph.voice_control_app import build_voice_control_graph
from emergency_agents.graph.sitrep_app import build_sitrep_graph
from emergency_agents.graph.recon_app import build_recon_graph_async
from emergency_agents.external.recon_gateway import PostgresReconGateway
from emergency_agents.planner.recon_llm import OpenAIReconLLMEngine, ReconLLMConfig
from emergency_agents.planner.recon_pipeline import ReconPipeline
from emergency_agents.memory.conversation_manager import (
    ConversationManager,
    ConversationNotFoundError,
    MessageRecord,
)
from emergency_agents.memory.mem0_facade import Mem0Config, MemoryFacade
from emergency_agents.llm.client import get_openai_client
from emergency_agents.llm.factory import LLMClientFactory
from emergency_agents.rag.pipe import RagPipeline, RagChunk
from emergency_agents.graph.kg_service import KGService, KGConfig
from emergency_agents.db.dao import (
    IncidentDAO,
    IncidentRepository,
    IncidentSnapshotRepository,
    RescueTaskRepository,
    TaskDAO,
    RescueDAO,
)
from emergency_agents.audit.logger import get_audit_logger, log_human_approval
from langgraph.types import Command
from emergency_agents.voice.asr.service import ASRService
from emergency_agents.voice.asr.base import ASRConfig as ASRConfigModel
from emergency_agents.intent.classifier import intent_classifier_node
from emergency_agents.intent.prompt_missing import prompt_missing_slots_node
from emergency_agents.intent.registry import IntentHandlerRegistry
from emergency_agents.context.service import ContextService
from emergency_agents.intent.validator import validate_and_prompt_node
from emergency_agents.api.intent_processor import (
    IntentProcessResult,
    Mem0Metrics,
    process_intent_core,
)
from emergency_agents.control import VoiceControlPipeline
from emergency_agents.external.device_directory import PostgresDeviceDirectory
from emergency_agents.risk import RiskCacheManager, RiskDataRepository, RiskPredictor
from emergency_agents.services import RescueDraftService


class Domain(str, Enum):
    """å—æ”¯æŒçš„çŸ¥è¯†åŸŸã€‚"""
    SPEC = "è§„èŒƒ"
    CASE = "æ¡ˆä¾‹"
    GEO = "åœ°ç†"
    EQUIP = "è£…å¤‡"


app = FastAPI(title="AI Emergency Brain API")
_cfg = AppConfig.load_from_env()
if not _cfg.postgres_dsn:
    raise RuntimeError("POSTGRES_DSN æœªé…ç½®ï¼Œæ— æ³•å¯åŠ¨æœåŠ¡ã€‚")
_graph_app: Any | None = None
_intent_graph: Any | None = None
_voice_control_graph: Any | None = None
_sitrep_graph: Any | None = None
_recon_graph: Any | None = None
_graph_closers: list[Callable[[], Awaitable[None]]] = []
_context_service: ContextService | None = None


# ========== Trace-IDä¸­é—´ä»¶ï¼šè‡ªåŠ¨æ³¨å…¥è¯·æ±‚è¿½è¸ªID ==========
class TraceIDMiddleware(BaseHTTPMiddleware):
    """
    ä¸ºæ¯ä¸ªHTTPè¯·æ±‚æ³¨å…¥trace-idåˆ°æ—¥å¿—ä¸Šä¸‹æ–‡

    æ”¯æŒï¼š
    1. å®¢æˆ·ç«¯ä¼ å…¥ X-Trace-Id è¯·æ±‚å¤´ï¼ˆå¤ç”¨trace-idï¼‰
    2. è‡ªåŠ¨ç”Ÿæˆ UUID trace-idï¼ˆæ–°è¯·æ±‚ï¼‰
    3. å“åº”å¤´è¿”å› X-Trace-Idï¼ˆä¾¿äºå®¢æˆ·ç«¯æ—¥å¿—å…³è”ï¼‰

    å‚è€ƒï¼šemergency_agents.logging æ¨¡å—æ–‡æ¡£
    """

    async def dispatch(self, request: Request, call_next):
        # æå–æˆ–ç”Ÿæˆtrace-id
        trace_id = request.headers.get("X-Trace-Id") or str(uuid.uuid4())
        set_trace_id(trace_id)

        try:
            response = await call_next(request)
            response.headers["X-Trace-Id"] = trace_id
            return response
        finally:
            # æ¸…ç†ä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢æ³„æ¼
            clear_trace_id()


app.add_middleware(TraceIDMiddleware)


# metrics
Instrumentator().instrument(app).expose(app)

# custom metrics
_assist_counter = Counter('assist_answer_total', 'Total /assist/answer requests')
_assist_failures = Counter('assist_answer_failures_total', 'Total /assist/answer failures')
_assist_latency = Histogram('assist_answer_seconds', 'Latency of /assist/answer')

_mem0_search_success = Counter("mem0_search_success_total", "mem0æ£€ç´¢æˆåŠŸæ¬¡æ•°")
_mem0_search_failure = Counter(
    "mem0_search_failure_total",
    "mem0æ£€ç´¢å¤±è´¥æ¬¡æ•°",
    ["reason"],
)
_mem0_search_duration = Histogram(
    "mem0_search_duration_seconds",
    "mem0æ£€ç´¢è€—æ—¶ï¼ˆç§’ï¼‰",
    buckets=(0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0),
)
_mem0_add_success = Counter("mem0_add_success_total", "mem0å†™å…¥æˆåŠŸæ¬¡æ•°")
_mem0_add_failure = Counter(
    "mem0_add_failure_total",
    "mem0å†™å…¥å¤±è´¥æ¬¡æ•°",
    ["reason"],
)

# memory facade singleton
_mem = MemoryFacade(
    Mem0Config(
        qdrant_url=_cfg.qdrant_url or "http://192.168.1.40:6333",
        qdrant_api_key=_cfg.qdrant_api_key,
        qdrant_collection="mem0_collection",
        embedding_model=_cfg.embedding_model,
        embedding_dim=_cfg.embedding_dim,
        neo4j_uri=_cfg.neo4j_uri or "bolt://192.168.1.40:7687",
        neo4j_user=_cfg.neo4j_user or "neo4j",
        # pragma: allowlist secret - placeholder credential for development
        neo4j_password=_cfg.neo4j_password or "example-neo4j",
        openai_base_url=_cfg.openai_base_url,
        # pragma: allowlist secret - placeholder credential for development
        openai_api_key=_cfg.openai_api_key,
        graph_llm_model=_cfg.llm_model,
    )
)

# rag pipeline singleton
_rag = RagPipeline(
    qdrant_url=_cfg.qdrant_url or "http://192.168.1.40:6333",
    qdrant_api_key=_cfg.qdrant_api_key,
    embedding_model=_cfg.embedding_model,
    embedding_dim=_cfg.embedding_dim,
    openai_base_url=_cfg.openai_base_url,
    openai_api_key=_cfg.openai_api_key,
    llm_model=_cfg.llm_model,
)

# kg service singleton
_kg = KGService(
    KGConfig(
        uri=_cfg.neo4j_uri or "bolt://192.168.1.40:7687",
        user=_cfg.neo4j_user or "neo4j",
        # pragma: allowlist secret - placeholder credential for development
        password=_cfg.neo4j_password or "example-neo4j",
    )
)

if not _cfg.postgres_dsn:
    raise RuntimeError("å¿…é¡»é…ç½®POSTGRES_DSNä»¥å¯ç”¨ä¼šè¯æœåŠ¡")

_pg_pool: AsyncConnectionPool[DictRow] = AsyncConnectionPool(
    conninfo=_cfg.postgres_dsn,
    min_size=1,
    max_size=4,
    open=False,
)
_conversation_manager = ConversationManager(_pg_pool)

_incident_repository = IncidentRepository.create(_pg_pool)
_incident_snapshot_repository = IncidentSnapshotRepository.create(_pg_pool)
_rescue_task_repository = RescueTaskRepository.create(_pg_pool)
_rescue_draft_service = RescueDraftService(
    incident_repository=_incident_repository,
    snapshot_repository=_incident_snapshot_repository,
    task_repository=_rescue_task_repository,
)

_llm_factory = LLMClientFactory(_cfg)
_llm_clients = {
    "default": _llm_factory.get_sync("default"),
    "rescue": _llm_factory.get_sync("rescue"),
    "intent": _llm_factory.get_sync("intent"),
    "strategic": _llm_factory.get_sync("strategic"),
}
app.state.llm_clients = _llm_clients
app.state.rescue_draft_service = _rescue_draft_service
_llm_client_default = _llm_clients["default"]
_llm_client_rescue = _llm_clients["rescue"]
_intent_llm_client = _llm_clients["intent"]
_llm_client_strategic = _llm_clients["strategic"]

_adapter_client = AdapterHubClient(
    base_url=_cfg.adapter_base_url,
    timeout=_cfg.adapter_timeout,
)

_device_directory_pool: ConnectionPool | None = None
_device_directory: PostgresDeviceDirectory | None = None
_voice_control_pipeline: VoiceControlPipeline | None = None
_recon_sync_pool: ConnectionPool | None = None

_amap_client = AmapClient(
    api_key=_cfg.amap_api_key or "",
    backup_key=_cfg.amap_backup_key,
    base_url=_cfg.amap_base_url or "https://restapi.amap.com",
    connect_timeout=_cfg.amap_connect_timeout,
    read_timeout=_cfg.amap_read_timeout,
)

_orchestrator_client = OrchestratorClient()

# IntentHandlerRegistryéœ€è¦å¼‚æ­¥åˆå§‹åŒ–ï¼Œåœ¨startup_eventä¸­å®Œæˆ
_intent_registry: IntentHandlerRegistry | None = None

_risk_cache_manager: RiskCacheManager | None = None
_risk_refresh_task: asyncio.Task[None] | None = None
_risk_predictor: RiskPredictor | None = None
_risk_predict_task: asyncio.Task[None] | None = None

app.include_router(rescue_api.router)
app.include_router(sitrep_api.router, prefix="/sitrep", tags=["sitrep"])
app.include_router(reports_api.router)
app.include_router(recon_api.router)
app.include_router(overall_rescue_api.router)
app.include_router(recon_priority_api.router)
app.include_router(recon_batch_weather_api.router)


def _classifier_wrapper(state: Dict[str, Any]) -> Dict[str, Any]:
    """æ„å›¾åˆ†ç±»å™¨åŒ…è£…å™¨"""
    return intent_classifier_node(
        state,
        llm_client=_intent_llm_client,
        llm_model=_cfg.intent_llm_model or _cfg.llm_model,
    )


def _validator_wrapper(state: Dict[str, Any], llm_client: Any, llm_model: str) -> Dict[str, Any]:
    # é¢„å½’ä¸€åŒ–ï¼šå°†åˆ«å/ä¸‹åˆ’çº¿å½¢å¼ç»Ÿä¸€æˆçŸ­æ¨ªçº¿ï¼Œé¿å…schemaæŸ¥æ‰¾é—æ¼
    intent = state.get("intent") or {}
    itype_raw = str((intent.get("intent_type") or "").strip())
    norm = itype_raw.replace(" ", "").replace("_", "-").lower()
    if norm in ("video-analysis", "video-analyze"):
        state = state | {"intent": intent | {"intent_type": "video-analysis"}}
    if norm in ("task-progress-query", "taskprogressquery"):
        state = state | {"intent": intent | {"intent_type": "task-progress-query"}}
    return validate_and_prompt_node(state, llm_client, llm_model)


def _prompt_wrapper(state: Dict[str, Any], llm_client: Any, llm_model: str) -> Dict[str, Any]:
    # å¢å¼ºï¼šå½“è§†é¢‘åˆ†æç¼ºå°‘è®¾å¤‡åæ—¶ï¼Œæä¾›ç»“æ„åŒ–å€™é€‰ï¼ˆDAO+Mem0ï¼‰
    # ä¸ºè§†é¢‘åˆ†ææ„é€ ç»“æ„åŒ–å€™é€‰ï¼ˆåŒæ­¥æŸ¥è¯¢ device_video_linkï¼‰
    try:
        intent = state.get("intent") or {}
        itype = str((intent.get("intent_type") or "").strip()).lower()
        itype_norm = itype.replace(" ", "").replace("_", "-")
        if itype_norm in ("video-analysis", "video-analyze"):
            import psycopg
            from emergency_agents.intent.prompt_missing import prompt_missing_slots_enhanced
            from emergency_agents.intent.schemas import ClarifyOption

            options: list[ClarifyOption] = []
            # åŒæ­¥æŸ¥è¯¢ï¼šä¼˜å…ˆ device_video_linkï¼Œå…¶æ¬¡ device_detail.stream_url
            sql = (
                "WITH candidates AS (\n"
                "  SELECT d.id::text AS id, d.name AS name\n"
                "    FROM operational.device d\n"
                "    JOIN operational.device_video_link dvl ON dvl.id = d.id\n"
                "   WHERE COALESCE(NULLIF(dvl.video_link, ''), '') <> ''\n"
                "  UNION ALL\n"
                "  SELECT d.id::text AS id, d.name AS name\n"
                "    FROM operational.device d\n"
                "    JOIN operational.device_detail dd ON dd.device_id = d.id\n"
                "   WHERE COALESCE(NULLIF(dd.device_detail->>'stream_url', ''), '') <> ''\n"
                ") SELECT id, name FROM candidates GROUP BY id, name ORDER BY name ASC LIMIT 10"
            )
            with psycopg.connect(_cfg.postgres_dsn) as conn:
                with conn.cursor() as cur:
                    cur.execute(sql)
                    for dev_id, name in cur.fetchall():
                        options.append({"label": name or dev_id, "device_id": dev_id})
            return prompt_missing_slots_enhanced(state, llm_client, llm_model, options=options)
    except Exception:
        pass
    # å…¶å®ƒæ„å›¾é€€å›é€šç”¨æ–‡æœ¬æ¾„æ¸…
    return prompt_missing_slots_node(state, llm_client, llm_model)


_asr = ASRService()
logger = structlog.get_logger(__name__)


def _require_conversation_manager() -> ConversationManager:
    if _conversation_manager is None:
        raise RuntimeError("ConversationManager æœªåˆå§‹åŒ–")
    return _conversation_manager


def _require_intent_registry() -> IntentHandlerRegistry:
    if _intent_registry is None:
        raise RuntimeError("IntentHandlerRegistry æœªåˆå§‹åŒ–")
    return _intent_registry

def _require_rescue_graph() -> Any:
    if _graph_app is None:
        raise RuntimeError("æ•‘æ´ç¼–æ’å­å›¾æœªåˆå§‹åŒ–")
    return _graph_app


def _require_intent_graph() -> Any:
    if _intent_graph is None:
        raise RuntimeError("æ„å›¾ç¼–æ’å­å›¾æœªæ„å»º")
    return _intent_graph


def _require_voice_control_graph() -> Any:
    graph = getattr(app.state, "voice_control_graph", None)
    if graph is None:
        if _voice_control_graph is None:
            raise RuntimeError("è¯­éŸ³æ§åˆ¶å­å›¾æœªåˆå§‹åŒ–")
        app.state.voice_control_graph = _voice_control_graph
        return _voice_control_graph
    return graph


def _build_history(records: List[MessageRecord]) -> List[Dict[str, Any]]:
    history: List[Dict[str, Any]] = []
    for record in records:
        history.append(
            {
                "id": record.id,
                "role": record.role,
                "content": record.content,
                "intent_type": record.intent_type,
                "event_time": record.event_time.isoformat(),
            }
        )
    return history


def _mem0_metrics_factory() -> Mem0Metrics:
    return Mem0Metrics(
        inc_search_success=_mem0_search_success.inc,
        inc_search_failure=lambda reason: _mem0_search_failure.labels(reason=reason).inc(),
        observe_search_duration=_mem0_search_duration.observe,
        inc_add_success=_mem0_add_success.inc,
        inc_add_failure=lambda reason: _mem0_add_failure.labels(reason=reason).inc(),
    )


def _register_graph_close(resource: Any) -> None:
    """è®°å½•éœ€è¦åœ¨shutdowné˜¶æ®µå…³é—­çš„å›¾èµ„æºã€‚"""
    close_cb = getattr(resource, "_checkpoint_close", None)
    if callable(close_cb):
        _graph_closers.append(close_cb)


@app.on_event("startup")
async def startup_event():
    # ğŸ”§ ç»Ÿä¸€æ—¥å¿—é…ç½®ï¼ˆç”Ÿäº§ç¯å¢ƒä½¿ç”¨JSONæ ¼å¼ï¼Œå¼€å‘ç¯å¢ƒä½¿ç”¨æ§åˆ¶å°ï¼‰
    import os
    json_logs = os.getenv("LOG_JSON", "false").lower() == "true"
    log_level = os.getenv("LOG_LEVEL", "INFO")
    configure_logging(json_logs=json_logs, log_level=log_level)

    global _graph_app
    global _intent_graph
    global _voice_control_graph
    global _sitrep_graph
    global _graph_closers
    global _risk_cache_manager
    global _risk_refresh_task
    global _risk_predictor
    global _risk_predict_task
    global _intent_registry  # æ–°å¢
    global _context_service
    global _recon_sync_pool

    await _pg_pool.open()
    logger.info("api_startup_pg_pool_opened")
    await _asr.start_health_check()
    await voice_chat_handler.start_background_tasks()
    _graph_closers = []

    # å»¶è¿Ÿåˆå§‹åŒ–è®¾å¤‡ç›®å½•è¿æ¥æ± ï¼Œé¿å…æ¨¡å—å¯¼å…¥é˜¶æ®µå ç”¨æ•°æ®åº“è¿æ¥
    global _device_directory_pool
    global _device_directory
    global _voice_control_pipeline
    if _device_directory_pool is None:
        _device_directory_pool = ConnectionPool(
            conninfo=_cfg.postgres_dsn,
            min_size=1,
            max_size=3,
            open=True,
        )
        _device_directory_pool.wait(timeout=60.0)
        _device_directory = PostgresDeviceDirectory(_device_directory_pool)
        _voice_control_pipeline = VoiceControlPipeline(
            default_robotdog_id=_cfg.default_robotdog_id,
            device_directory=_device_directory,
        )

    # åˆå§‹åŒ–IntentHandlerRegistryï¼ˆå¼‚æ­¥åˆå§‹åŒ–ï¼ŒåŒ…å«ScoutTaskGenerationHandleræ‡’åŠ è½½ï¼‰
    _intent_registry = await IntentHandlerRegistry.build(
        pool=_pg_pool,
        amap_client=_amap_client,
        device_directory=_device_directory,
        video_stream_map=_cfg.video_stream_map,
        kg_service=_kg,
        rag_pipeline=_rag,
        llm_client=_llm_client_rescue,
        llm_model=_cfg.llm_model,
        adapter_client=_adapter_client,
        default_robotdog_id=_cfg.default_robotdog_id,
        orchestrator_client=_orchestrator_client,
        rag_timeout=_cfg.rag_analysis_timeout,
        postgres_dsn=_cfg.postgres_dsn,
        vllm_url=_cfg.openai_base_url,  # GLM-4V ä½¿ç”¨ç›¸åŒçš„ OpenAI å…¼å®¹ç«¯ç‚¹
    )
    _intent_registry.attach_rescue_draft_service(_rescue_draft_service)
    logger.info("api_intent_registry_initialized")

    _graph_app = await build_app(_cfg.checkpoint_sqlite_path, _cfg.postgres_dsn)
    _register_graph_close(_graph_app)
    logger.info("api_graph_ready", graph="rescue_app")

    _intent_graph = await build_intent_orchestrator_graph(
        cfg=_cfg,
        llm_client=_intent_llm_client,
        llm_model=_cfg.intent_llm_model or _cfg.llm_model,
        classifier_node=_classifier_wrapper,
        validator_node=_validator_wrapper,
        prompt_node=_prompt_wrapper,
    )
    _register_graph_close(_intent_graph)
    logger.info("api_graph_ready", graph="intent_orchestrator")

    # ä¼šè¯ä¸Šä¸‹æ–‡æœåŠ¡ï¼ˆåŸºäºå…¨å±€ AsyncConnectionPoolï¼‰
    _context_service = ContextService(pool=_pg_pool)

    if _voice_control_pipeline is None:
        raise RuntimeError("VoiceControlPipeline æœªåˆå§‹åŒ–")

    _voice_control_graph = await build_voice_control_graph(
        pipeline=_voice_control_pipeline,
        adapter_client=_adapter_client,
        postgres_dsn=_cfg.postgres_dsn,
    )
    _register_graph_close(_voice_control_graph)
    app.state.voice_control_graph = _voice_control_graph
    logger.info("api_graph_ready", graph="voice_control")

    # åˆå§‹åŒ–RiskCacheManagerï¼ˆSITREPä¾èµ–å®ƒï¼‰
    incident_dao = IncidentDAO.create(_pg_pool)
    _risk_cache_manager = RiskCacheManager(
        incident_dao=incident_dao,
        ttl_seconds=_cfg.risk_cache_ttl_seconds,
    )
    await _risk_cache_manager.prefetch()
    app.state.risk_cache = _risk_cache_manager
    refresh_interval = _cfg.risk_refresh_interval_seconds
    _risk_refresh_task = asyncio.create_task(
        _risk_cache_manager.periodic_refresh(refresh_interval)
    )
    repository = RiskDataRepository(incident_dao)
    _risk_predictor = RiskPredictor(repository, _risk_cache_manager)
    await _risk_predictor.analyze()
    app.state.risk_predictor = _risk_predictor
    _risk_predict_task = asyncio.create_task(
        _risk_predictor.run_periodic(refresh_interval)
    )

    # ç»‘å®š /ai/plan è·¯ç”±ï¼ˆæˆ˜ç•¥/æ•‘æ´æ–¹æ¡ˆï¼‰
    # ä¾èµ–æ³¨å…¥: ä¾› /ai/plan/from-progress ä½¿ç”¨
    plan_api._pg_pool_async = _pg_pool  # type: ignore[assignment]
    app.include_router(plan_api.router)
    # ç»‘å®š /ai/rescue è·¯ç”±ï¼ˆæ•´ä½“æ•‘æ´æ–¹æ¡ˆï¼‰
    overall_rescue_api._pg_pool_async = _pg_pool  # type: ignore[assignment]
    recon_priority_api._pg_pool_async = _pg_pool  # type: ignore[assignment]
    recon_batch_weather_api._pg_pool_async = _pg_pool  # type: ignore[assignment]

    # ========== åˆå§‹åŒ–SITREPå­å›¾ ==========
    task_dao = TaskDAO.create(_pg_pool)
    rescue_dao = RescueDAO.create(_pg_pool)
    snapshot_repo = IncidentSnapshotRepository.create(_pg_pool)

    # åˆ›å»ºcheckpointerï¼ˆå¤ç”¨PostgreSQLè¿æ¥æ± ï¼‰
    from emergency_agents.graph.checkpoint_utils import create_async_postgres_checkpointer
    sitrep_checkpointer, sitrep_close_cb = await create_async_postgres_checkpointer(
        dsn=_cfg.postgres_dsn,
        schema="sitrep_checkpoint",
        min_size=1,
        max_size=1,
    )

    _sitrep_graph = await build_sitrep_graph(
        incident_dao=incident_dao,
        task_dao=task_dao,
        risk_cache_manager=_risk_cache_manager,  # å¤ç”¨å·²åˆå§‹åŒ–çš„risk_cache_manager
        rescue_dao=rescue_dao,
        snapshot_repo=snapshot_repo,
        llm_client=_llm_client_rescue,
        llm_model=_cfg.llm_model,
        checkpointer=sitrep_checkpointer,
    )
    _sitrep_graph._checkpoint_close = sitrep_close_cb  # æ³¨å†Œå…³é—­å›è°ƒ
    _register_graph_close(_sitrep_graph)
    logger.info("api_graph_ready", graph="sitrep")

    # å°†ä¾èµ–æ³¨å…¥åˆ°sitrep_apiæ¨¡å—
    sitrep_api._sitrep_graph = _sitrep_graph
    sitrep_api._incident_dao = incident_dao
    sitrep_api._snapshot_repo = snapshot_repo
    # ========== SITREPå­å›¾åˆå§‹åŒ–å®Œæˆ ==========

    # ========== æ³¨å…¥Reports APIä¾èµ– ==========
    reports_api._kg_service = _kg
    reports_api._rag_pipeline = _rag
    logger.info("api_reports_dependencies_injected")
    # ========== Reports APIä¾èµ–æ³¨å…¥å®Œæˆ ==========

    if _intent_registry is not None:
        _intent_registry.attach_risk_cache(_risk_cache_manager)
    logger.info(
        "risk_cache_startup_initialized",
        ttl_seconds=_cfg.risk_cache_ttl_seconds,
        refresh_interval_seconds=refresh_interval,
    )

    # ========== åˆå§‹åŒ–RECONå­å›¾(ä¾¦å¯Ÿæ–¹æ¡ˆ) ==========
    # ä½¿ç”¨ä¸å…¨å±€ä¸€è‡´çš„æ•°æ®åº“,ä½†åœ¨checkpointå±‚ä½¿ç”¨ç‹¬ç«‹schema(recon_checkpoint)
    if not _cfg.recon_llm_model or not _cfg.recon_llm_base_url or not _cfg.recon_llm_api_key:
        # éµå¾ªâ€œä¸å…œåº•/ä¸é™çº§â€: æ˜ç¡®è¦æ±‚é…ç½®RECON LLMå‚æ•°
        raise RuntimeError("RECON_LLM_* æœªé…ç½®,æ— æ³•åˆå§‹åŒ–ä¾¦å¯Ÿå­å›¾")

    # æ„é€ åŒæ­¥ConnectionPoolç”¨äºç½‘å…³(ä¸Asyncè¿æ¥æ± å¹¶å­˜,èŒè´£åˆ†ç¦»)
    _recon_sync_pool = ConnectionPool(
        conninfo=_cfg.postgres_dsn,
        min_size=1,
        max_size=1,
        open=True,
    )
    recon_gateway = PostgresReconGateway(_recon_sync_pool)

    # æŒ‰éœ€å›ºå®šä¾¦å¯ŸLLMæ¨¡å‹ï¼ˆæ¥å£æ˜ç¡®è¦æ±‚å›ºå®šä½¿ç”¨ glm4.6ï¼‰
    recon_llm = OpenAIReconLLMEngine(
        config=ReconLLMConfig(
            model="glm-4.6",
            base_url=_cfg.recon_llm_base_url,
            api_key=_cfg.recon_llm_api_key,
            temperature=0.2,
            timeout_seconds=_cfg.llm_request_timeout_seconds,
        )
    )
    recon_pipeline = ReconPipeline(gateway=recon_gateway, llm_engine=recon_llm)

    _recon_graph = await build_recon_graph_async(
        pipeline=recon_pipeline,
        gateway=recon_gateway,
        dsn=_cfg.postgres_dsn,
        schema="recon_checkpoint",
    )
    _register_graph_close(_recon_graph)
    app.state.recon_graph = _recon_graph
    app.state.recon_gateway = recon_gateway
    plan_api._recon_gateway = recon_gateway  # type: ignore[assignment]
    logger.info("api_graph_ready", graph="recon")


@app.on_event("shutdown")
async def shutdown_event():
    global _graph_app
    global _intent_graph
    global _voice_control_graph
    global _sitrep_graph
    global _recon_graph
    global _graph_closers
    global _risk_cache_manager
    global _risk_refresh_task
    global _risk_predictor
    global _risk_predict_task
    global _recon_sync_pool
    await voice_chat_handler.stop_background_tasks()
    await _asr.stop_health_check()
    await _adapter_client.aclose()
    await _amap_client.close()
    _orchestrator_client.close()
    logger.info("api_shutdown_services_stopped")
    for close_cb in _graph_closers:
        await close_cb()
    _graph_closers.clear()
    _graph_app_attr = _graph_app if _graph_app is not None else None
    for resource in (_graph_app_attr, _intent_graph, _voice_control_graph, _sitrep_graph):
        if resource is not None and hasattr(resource, "_checkpoint_close"):
            setattr(resource, "_checkpoint_close", None)
    logger.info("api_shutdown_graphs_closed")
    _graph_app = None
    _intent_graph = None
    _voice_control_graph = None
    _sitrep_graph = None
    _recon_graph = None

    if hasattr(_intent_registry, "close"):
        close_result = _intent_registry.close()
        if inspect.isawaitable(close_result):
            await close_result
    await _pg_pool.close()
    if _device_directory_pool is not None:
        _device_directory_pool.close()
    if _recon_sync_pool is not None:
        try:
            _recon_sync_pool.close()
        except Exception:
            pass
        _recon_sync_pool = None
    for task in (_risk_refresh_task, _risk_predict_task):
        if task is not None:
            task.cancel()
            with suppress(asyncio.CancelledError):
                await task
    _risk_refresh_task = None
    _risk_predict_task = None
    if _intent_registry is not None:
        _intent_registry.attach_risk_cache(None)
    _risk_cache_manager = None
    if hasattr(app.state, "risk_cache"):
        delattr(app.state, "risk_cache")
    if hasattr(app.state, "risk_predictor"):
        delattr(app.state, "risk_predictor")
    _risk_predictor = None


class RagDoc(BaseModel):
    """RAG æ–‡æ¡£è¾“å…¥ã€‚"""
    id: str
    text: str
    meta: Optional[Dict[str, Any]] = None


class RagIndexRequest(BaseModel):
    """æ‰¹é‡ç´¢å¼•è¯·æ±‚ã€‚"""
    domain: Domain
    docs: List[RagDoc]


class RagQueryRequest(BaseModel):
    """RAG æ£€ç´¢è¯·æ±‚ã€‚"""
    domain: Domain
    question: str
    top_k: int = Field(3, ge=1, le=10, description="è¿”å›ç‰‡æ®µæ•°é‡ä¸Šé™")


class KGRecommendRequest(BaseModel):
    """è£…å¤‡æ¨èè¯·æ±‚ã€‚"""
    hazard: str
    environment: Optional[str] = None
    top_k: int = Field(5, ge=1, le=20)


class KGCaseSearchRequest(BaseModel):
    """æ¡ˆä¾‹æ£€ç´¢è¯·æ±‚ã€‚"""
    keywords: str
    top_k: int = Field(5, ge=1, le=20)


class IntentProcessRequest(BaseModel):
    """æ„å›¾å¤„ç†è¯·æ±‚ä½“ã€‚"""

    user_id: str
    thread_id: str
    message: str
    metadata: Dict[str, Any] | None = None
    incident_id: Optional[str] = None
    channel: Literal["voice", "text", "system"] = "text"


class ConversationHistoryRequest(BaseModel):
    """ä¼šè¯å†å²æŸ¥è¯¢è¯·æ±‚ã€‚"""

    user_id: str
    thread_id: str
    limit: int = Field(20, ge=1, le=100)


class AssistAnswerRequest(BaseModel):
    """æ™ºèƒ½å›ç­”è¯·æ±‚ã€‚"""
    user_id: str
    run_id: str
    domain: Domain
    question: str
    top_k: int = Field(3, ge=1, le=10)


class StartThreadRequest(BaseModel):
    """å¯åŠ¨æ•‘æ´çº¿ç¨‹è¯·æ±‚"""
    raw_report: str = Field(..., description="éç»“æ„åŒ–ç¾æƒ…æŠ¥å‘Š")
    user_id: Optional[str] = Field(None, description="ç”¨æˆ·ID")


@app.post("/threads/start")
async def start_thread(rescue_id: str, req: StartThreadRequest):
    """åˆ›å»ºæ•‘æ´çº¿ç¨‹å¹¶è¿›å…¥äººå·¥å®¡æ‰¹ä¸­æ–­ç‚¹ã€‚"""
    init_state = {
        "rescue_id": rescue_id,
        "user_id": req.user_id or "unknown",
        "raw_report": req.raw_report
    }
    result = _require_rescue_graph().invoke(
        init_state,
        config={
            "configurable": {
                "thread_id": f"rescue-{rescue_id}",
                "checkpoint_ns": f"tenant-{init_state['user_id']}",
            },
            "durability": "sync",  # é•¿æµç¨‹ï¼ˆæ•‘æ´çº¿ç¨‹ï¼‰ï¼ŒåŒæ­¥ä¿å­˜checkpointç¡®ä¿é«˜å¯é æ€§
        },
    )
    return {"rescue_id": rescue_id, "state": result}


class ApproveRequest(BaseModel):
    """å®¡æ‰¹è¯·æ±‚"""
    approved_ids: List[str] = Field(..., description="æ‰¹å‡†çš„ææ¡ˆIDåˆ—è¡¨")
    comment: Optional[str] = Field(None, description="å®¡æ‰¹æ„è§")


@app.post("/threads/approve")
async def approve_thread(rescue_id: str, user_id: str, req: ApproveRequest):
    """å®¡æ‰¹æ•‘æ´æ–¹æ¡ˆã€‚"""
    log_human_approval(
        rescue_id=rescue_id,
        user_id=user_id,
        approved_ids=req.approved_ids,
        comment=req.comment,
        thread_id=f"rescue-{rescue_id}"
    )
    
    # åŠ¨æ€ä¸­æ–­æ¢å¤ï¼šä½¿ç”¨ Command(resume=[...]) å°†æ‰¹å‡†çš„IDæ³¨å…¥ await èŠ‚ç‚¹
    result = _require_rescue_graph().invoke(
        Command(resume=req.approved_ids),
        config={
            "configurable": {"thread_id": f"rescue-{rescue_id}"},
            "durability": "sync",  # é•¿æµç¨‹ï¼ˆæ•‘æ´çº¿ç¨‹ï¼‰ï¼ŒåŒæ­¥ä¿å­˜checkpointç¡®ä¿é«˜å¯é æ€§
        },
    )
    return {"rescue_id": rescue_id, "approved": True, "state": result}


@app.post("/threads/resume")
async def resume_thread(rescue_id: str):
    """ç»§ç»­æ‰§è¡ŒæŒ‡å®šæ•‘æ´çº¿ç¨‹ã€‚"""
    result = _require_rescue_graph().invoke(
        None,
        config={
            "configurable": {"thread_id": f"rescue-{rescue_id}"},
            "durability": "sync",  # é•¿æµç¨‹ï¼ˆæ•‘æ´çº¿ç¨‹ï¼‰ï¼ŒåŒæ­¥ä¿å­˜checkpointç¡®ä¿é«˜å¯é æ€§
        },
    )
    return {"rescue_id": rescue_id, "state": result}


@app.get("/audit/trail/{rescue_id}")
async def get_audit_trail(rescue_id: str):
    """è·å–å®¡è®¡è½¨è¿¹ã€‚"""
    audit_logger = get_audit_logger()
    trail = audit_logger.get_trail(rescue_id)
    return {"rescue_id": rescue_id, "trail": trail, "count": len(trail)}


@app.get("/healthz")
async def healthz():
    """å¥åº·æ¢é’ˆã€‚"""
    return {"status": "ok"}


@app.post("/memory/add")
async def memory_add(content: str, user_id: str, run_id: Optional[str] = None, agent_id: Optional[str] = None):
    """æ–°å¢è®°å¿†è®°å½•ã€‚"""
    _mem.add(content=content, user_id=user_id, run_id=run_id, agent_id=agent_id)
    return {"ok": True}


@app.get("/memory/search")
async def memory_search(query: str, user_id: str, run_id: Optional[str] = None, agent_id: Optional[str] = None, top_k: int = 5):
    """æ£€ç´¢è®°å¿†è®°å½•ã€‚"""
    res = _mem.search(query=query, user_id=user_id, run_id=run_id, agent_id=agent_id, top_k=top_k)
    return {"results": res}


@app.post("/rag/index")
async def rag_index(req: RagIndexRequest):
    """æ‰¹é‡ç´¢å¼•æ–‡æ¡£åˆ° RAG å­˜å‚¨ã€‚"""
    _rag.index_documents(req.domain.value, [d.model_dump() for d in req.docs])
    return {"ok": True, "count": len(req.docs), "trace_id": str(uuid.uuid4())}


@app.post("/rag/query")
async def rag_query(req: RagQueryRequest):
    """æ‰§è¡Œ RAG ç›¸ä¼¼åº¦æ£€ç´¢ã€‚"""
    chunks: List[RagChunk] = _rag.query(req.question, req.domain.value, req.top_k)
    return {"trace_id": str(uuid.uuid4()), "results": [
        {"text": c.text, "source": c.source, "loc": c.loc} for c in chunks
    ]}


@app.post("/asr/recognize")
async def asr_recognize(file: UploadFile = File(...), sample_rate: int = 16000, fmt: str = "pcm"):
    """ä¸Šä¼ å•æ®µéŸ³é¢‘å¹¶è¿”å›è¯†åˆ«æ–‡æœ¬ã€‚

    ASR ä½¿ç”¨è‡ªåŠ¨æ•…éšœè½¬ç§»æœºåˆ¶:Aliyun(ä¸»)â†’ Local FunASR(å¤‡),åå°å¥åº·æ£€æŸ¥æ¯ 30 ç§’æ‰§è¡Œä¸€æ¬¡ã€‚
    æ”¯æŒ PCM/WAVï¼ˆè‹¥ä¸º WAVï¼Œåº”ç”±è°ƒç”¨æ–¹ä¼ å…¥è£¸éŸ³é¢‘æ•°æ®æˆ–ç¡®ä¿æœåŠ¡ç«¯è·å–åˆ° PCM æ•°æ®ï¼‰ã€‚
    """

    try:
        data = await file.read()
        if not data:
            raise HTTPException(status_code=400, detail="empty file")
        cfg = ASRConfigModel(format=fmt, sample_rate=sample_rate)
        result = await _asr.recognize(data, cfg)
        return {
            "provider": _asr.provider_name,
            "text": result.text,
            "latency_ms": result.latency_ms,
            "confidence": result.confidence,
            "metadata": result.metadata or {},
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/kg/recommend")
async def kg_recommend(req: KGRecommendRequest):
    """æ ¹æ®å±å®³/ç¯å¢ƒæ¨èè£…å¤‡ã€‚"""
    items = _kg.recommend_equipment(hazard=req.hazard, environment=req.environment, top_k=req.top_k)
    return {"trace_id": str(uuid.uuid4()), "recommendations": items}


@app.post("/kg/cases/search")
async def kg_search(req: KGCaseSearchRequest):
    """æŒ‰å…³é”®è¯æ£€ç´¢æ¡ˆä¾‹ã€‚"""
    items = _kg.search_cases(keywords=req.keywords, top_k=req.top_k)
    return {"trace_id": str(uuid.uuid4()), "results": items}


@app.post("/assist/answer")
async def assist_answer(req: AssistAnswerRequest):
    """ç»“åˆ RAG ä¸è®°å¿†ç”Ÿæˆå¸¦å¼•ç”¨çš„å›ç­”ã€‚"""
    _assist_counter.inc()
    trace_id = str(uuid.uuid4())
    with _assist_latency.time():
        try:
            # 1) æ£€ç´¢ RAG ç‰‡æ®µ
            rag_chunks: List[RagChunk] = _rag.query(req.question, req.domain.value, req.top_k)
            # 2) æ£€ç´¢ Mem0 è®°å¿†
            mem_results = _mem.search(query=req.question, user_id=req.user_id, run_id=req.run_id, top_k=req.top_k)
            # 3) æ±‡æ€»è¯æ®å¹¶ç”Ÿæˆå›ç­”
            client = get_openai_client(_cfg)
            context_parts: List[str] = []
            for c in rag_chunks:
                context_parts.append(f"[RAG] {c.source}@{c.loc}: {c.text}")
            for m in mem_results or []:
                if isinstance(m, dict):
                    meta = m.get('metadata', {})
                    content = m.get('content', '')
                    context_parts.append(f"[MEM] {meta.get('run_id','')}: {content}")
                else:
                    context_parts.append(f"[MEM] {str(m)}")
            context = "\n".join(context_parts)
            messages = [
                {"role": "system", "content": "ä½ æ˜¯æ•‘æ´åº”æ€¥å¤§è„‘ï¼Œè¯·åŸºäºç»™å®šè¯æ®å›ç­”ï¼Œå¹¶åœ¨æœ«å°¾åˆ—å‡ºå¼•ç”¨ã€‚"},
                {"role": "user", "content": f"é—®é¢˜: {req.question}\n\nè¯æ®:\n{context}"},
            ]
            resp = client.chat.completions.create(model=_cfg.llm_model, messages=messages, temperature=0)
            answer = resp.choices[0].message.content if getattr(resp, 'choices', None) else ""
            return {
                "trace_id": trace_id,
                "answer": answer,
                "evidence": {
                    "rag": [{"text": c.text, "source": c.source, "loc": c.loc} for c in rag_chunks],
                    "memory": mem_results,
                },
            }
        except Exception:
            _assist_failures.inc()
            raise


@app.post("/intent/process")
async def intent_process(req: IntentProcessRequest):
    """ç»Ÿä¸€æ„å›¾å¤„ç†å…¥å£ï¼ˆè¯­éŸ³ä¸æ–‡æœ¬å¤ç”¨ï¼‰ã€‚"""
    manager = _require_conversation_manager()
    registry = _require_intent_registry()
    metadata = dict(req.metadata or {})
    if req.incident_id:
        metadata.setdefault("incident_id", req.incident_id)

    result: IntentProcessResult = await process_intent_core(
        user_id=req.user_id,
        thread_id=req.thread_id,
        message=req.message,
        metadata=metadata,
        manager=manager,
        registry=registry,
        orchestrator_graph=_require_intent_graph(),
        voice_control_graph=_require_voice_control_graph(),
        mem=_mem,
        build_history=_build_history,
        mem0_metrics=_mem0_metrics_factory(),
        channel=req.channel,
        context_service=_context_service,
    )
    return {
        "status": result.status,
        "intent": result.intent,
        "result": result.result,
        "history": result.history,
        "memory_hits": result.memory_hits,
        "audit_log": result.audit_log,
        "ui_actions": result.ui_actions,
    }


@app.post("/conversations/history")
async def conversation_history(req: ConversationHistoryRequest):
    """æŸ¥è¯¢æŒ‡å®šä¼šè¯å†å²è®°å½•ã€‚"""
    manager = _require_conversation_manager()
    conversation = await manager.fetch_conversation(req.thread_id)
    if conversation is None:
        return {
            "history": [],
            "total": 0,
            "user_id": req.user_id,
            "thread_id": req.thread_id,
        }
    if conversation.user_id != req.user_id:
        raise HTTPException(status_code=403, detail="thread owner mismatch")
    records = await manager.get_history(req.thread_id, limit=req.limit)
    history_payload = _build_history(records)
    return {
        "history": history_payload,
        "total": len(history_payload),
        "user_id": req.user_id,
        "thread_id": req.thread_id,
    }

# WebSocket è¯­éŸ³å¯¹è¯è·¯ç”±
@app.websocket("/ws/voice/chat")
async def voice_chat_endpoint(websocket: WebSocket) -> None:
    await handle_voice_chat(websocket)
