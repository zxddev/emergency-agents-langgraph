# Copyright 2025 msq
"""ç»Ÿä¸€æ„å›¾å¤„ç†é›†æˆæµ‹è¯•ï¼ˆçœŸå®LLMè°ƒç”¨ï¼‰ã€‚

æµ‹è¯•è¦†ç›–ï¼š
1. ç«¯åˆ°ç«¯æ„å›¾è¯†åˆ«æµç¨‹ï¼ˆunified modeï¼‰
2. ä¸“å®¶å’¨è¯¢è§¦å‘ï¼ˆä½ç½®ä¿¡åº¦å’ŒæœªçŸ¥æ„å›¾ï¼‰
3. ä¸intent_processor.pyçš„é›†æˆ
4. é…ç½®åˆ‡æ¢ï¼ˆunified vs legacyï¼‰
5. æ€§èƒ½åŸºå‡†æµ‹è¯•

æ³¨æ„ï¼š
- è¿™äº›æµ‹è¯•ä¼šè°ƒç”¨çœŸå®çš„LLM API
- éœ€è¦é…ç½® OPENAI_BASE_URL å’Œ OPENAI_API_KEY
- æ ‡è®°ä¸º @pytest.mark.integration

å‚è€ƒï¼š
- openspec/changes/unify-intent-processing/tasks.md (Phase 2.3)
- openspec/changes/unify-intent-processing/specs/intent-processing/spec.md
"""
import os
import time

import pytest

from emergency_agents.config import AppConfig
from emergency_agents.intent.expert_consult import expert_consult_node
from emergency_agents.intent.unified_intent import unified_intent_node
from emergency_agents.llm.client import get_openai_client


@pytest.fixture(scope="module")
def llm_client():
    """åˆ›å»ºçœŸå®LLMå®¢æˆ·ç«¯"""
    cfg = AppConfig.load_from_env()
    return get_openai_client(cfg)


@pytest.fixture(scope="module")
def llm_model():
    """è·å–LLMæ¨¡å‹åç§°"""
    return os.getenv("LLM_MODEL", "glm-4.5-air")


@pytest.mark.integration
class TestUnifiedIntentIntegration:
    """ç»Ÿä¸€æ„å›¾å¤„ç†é›†æˆæµ‹è¯•"""

    def test_valid_rescue_intent_recognition(self, llm_client, llm_model):
        """æµ‹è¯•æœ‰æ•ˆæ•‘æ´æ„å›¾è¯†åˆ«ï¼ˆå®Œæ•´åœºæ™¯ï¼‰"""
        state = {
            "messages": [
                {
                    "role": "user",
                    "content": "ç°åœ¨å››å·èŒ‚å¿å‘ç”Ÿäº†åœ°éœ‡ï¼Œéœ€è¦å»å‰çªå¤„ç½®ï¼Œåæ ‡æ˜¯103.85,31.68"
                }
            ]
        }

        t_start = time.time()
        result_state = unified_intent_node(state, llm_client, llm_model)
        t_end = time.time()

        duration_ms = int((t_end - t_start) * 1000)
        print(f"\nâ±ï¸  Unified intent processing: {duration_ms}ms")

        unified_intent = result_state["unified_intent"]

        # éªŒè¯æ„å›¾ç±»å‹
        assert unified_intent["intent_type"] in [
            "RESCUE_TASK_GENERATION",
            "HAZARD_REPORT"
        ], f"Unexpected intent_type: {unified_intent['intent_type']}"

        # éªŒè¯ç½®ä¿¡åº¦
        assert unified_intent["confidence"] >= 0.7, \
            f"Confidence too low: {unified_intent['confidence']}"

        # éªŒè¯éªŒè¯çŠ¶æ€
        assert unified_intent["validation_status"] in ["valid", "invalid"], \
            f"Unexpected validation_status: {unified_intent['validation_status']}"

        # éªŒè¯æ§½ä½æå–
        slots = unified_intent["slots"]
        mission_type = slots.get("mission_type")
        assert mission_type, "Missing mission_type slot"
        assert any(keyword in str(mission_type) for keyword in ["å‰çª", "æ•‘æ´", "å¤„ç½®"]), \
            f"mission_typeä¸æ­£ç¡®: {mission_type}"

        # å¦‚æœéªŒè¯é€šè¿‡ï¼Œåº”è¯¥æœ‰location
        if unified_intent["validation_status"] == "valid":
            location_value = slots.get("location_name") or slots.get("location_text") or slots.get("location")
            assert location_value, "Missing location slot"
            assert "èŒ‚å¿" in str(location_value) or "å››å·" in str(location_value), \
                f"locationä¸æ­£ç¡®: {location_value}"

        print(f"âœ… Intent: {unified_intent['intent_type']}")
        print(f"âœ… Confidence: {unified_intent['confidence']}")
        print(f"âœ… Validation: {unified_intent['validation_status']}")
        print(f"âœ… Slots: {slots}")

        # æ€§èƒ½è¦æ±‚ï¼šâ‰¤18ç§’ï¼ˆP95ï¼‰
        assert duration_ms <= 18000, \
            f"Performance requirement not met: {duration_ms}ms > 18000ms"

    def test_invalid_intent_missing_fields(self, llm_client, llm_model):
        """æµ‹è¯•ç¼ºå°‘å¿…å¡«å­—æ®µçš„æ— æ•ˆæ„å›¾"""
        state = {
            "messages": [{"role": "user", "content": "åœ°éœ‡å‘ç”Ÿäº†"}]
        }

        result_state = unified_intent_node(state, llm_client, llm_model)
        unified_intent = result_state["unified_intent"]

        # éªŒè¯æ„å›¾ç±»å‹
        assert unified_intent["intent_type"] != "UNKNOWN", \
            "åº”è¯¥è¯†åˆ«ä¸ºå…·ä½“æ„å›¾ç±»å‹ï¼Œä¸æ˜¯UNKNOWN"

        # éªŒè¯éªŒè¯çŠ¶æ€
        assert unified_intent["validation_status"] == "invalid", \
            f"Expected invalid, got: {unified_intent['validation_status']}"

        # éªŒè¯ç¼ºå¤±å­—æ®µ
        missing_fields = set(unified_intent["missing_fields"])
        assert missing_fields, "Should have missing_fields"
        assert "location" in missing_fields, \
            f"ç¼ºå¤±å­—æ®µåº”åŒ…å«locationï¼Œå®é™…ä¸º: {unified_intent['missing_fields']}"

        # éªŒè¯æç¤ºç”Ÿæˆ
        assert unified_intent["prompt"], \
            "Should generate prompt for missing fields"
        assert "åœ°ç‚¹" in unified_intent["prompt"], \
            f"Prompt should guide toè¡¥å……åœ°ç‚¹, got: {unified_intent['prompt']}"

        print(f"âœ… Invalid intent detected")
        print(f"âœ… Missing fields: {unified_intent['missing_fields']}")
        print(f"âœ… Prompt: {unified_intent['prompt']}")

    def test_unknown_intent_professional_question(self, llm_client, llm_model):
        """æµ‹è¯•æœªçŸ¥æ„å›¾ï¼ˆä¸“ä¸šåº”æ€¥é—®é¢˜ï¼‰"""
        state = {
            "messages": [
                {"role": "user", "content": "ä»€ä¹ˆæƒ…å†µä¸‹éœ€è¦å¯åŠ¨ä¸€çº§å“åº”ï¼Ÿ"}
            ]
        }

        result_state = unified_intent_node(state, llm_client, llm_model)
        unified_intent = result_state["unified_intent"]

        # å¯èƒ½è¯†åˆ«ä¸ºUNKNOWNæˆ–ä½ç½®ä¿¡åº¦
        if unified_intent["confidence"] < 0.7 or unified_intent["validation_status"] == "unknown":
            print(f"âœ… Triggered expert consultation")
            print(f"   Confidence: {unified_intent['confidence']}")
            print(f"   Validation: {unified_intent['validation_status']}")

            # æµ‹è¯•ä¸“å®¶å’¨è¯¢èŠ‚ç‚¹
            result_state = expert_consult_node(result_state, llm_client, llm_model)
            expert_consult = result_state["expert_consult"]

            # éªŒè¯ä¸“å®¶å“åº”
            assert expert_consult["response"], "Expert response should not be empty"
            assert len(expert_consult["response"]) > 50, \
                "Expert response should be substantial"
            assert expert_consult["source"] == "emergency_expert_system"
            assert expert_consult["trigger_reason"] in [
                "low_confidence", "unknown_intent"
            ]

            # éªŒè¯ä¸“ä¸šæ€§ï¼ˆåº”è¯¥åŒ…å«åº”æ€¥æœ¯è¯­ï¼‰
            response_text = expert_consult["response"]
            professional_keywords = ["åº”æ€¥", "å“åº”", "é¢„æ¡ˆ", "æ•‘æ´", "ç¾å®³", "æŒ‡æŒ¥"]
            has_keywords = any(kw in response_text for kw in professional_keywords)
            assert has_keywords, \
                f"Response should contain professional emergency terms: {response_text[:100]}"

            print(f"âœ… Expert response length: {len(expert_consult['response'])} chars")
            print(f"âœ… Trigger reason: {expert_consult['trigger_reason']}")
            print(f"âœ… Response preview: {expert_consult['response'][:200]}...")
        else:
            print(f"â„¹ï¸  Recognized as valid intent: {unified_intent['intent_type']}")

    def test_out_of_scope_refusal(self, llm_client, llm_model):
        """æµ‹è¯•è¶…èŒƒå›´é—®é¢˜çš„ç¤¼è²Œæ‹’ç»"""
        state = {
            "messages": [{"role": "user", "content": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}]
        }

        result_state = unified_intent_node(state, llm_client, llm_model)
        unified_intent = result_state["unified_intent"]

        # åº”è¯¥è¯†åˆ«ä¸ºæœªçŸ¥æˆ–ä½ç½®ä¿¡åº¦
        if unified_intent["confidence"] < 0.7 or unified_intent["validation_status"] == "unknown":
            result_state = expert_consult_node(result_state, llm_client, llm_model)
            expert_consult = result_state["expert_consult"]

            response_text = expert_consult["response"]

            # éªŒè¯æ‹’ç»è¡¨è¿°
            refusal_keywords = ["æŠ±æ­‰", "è¶…å‡º", "èŒƒå›´", "ä¸æ”¯æŒ", "ä¸æä¾›"]
            has_refusal = any(kw in response_text for kw in refusal_keywords)
            assert has_refusal, \
                f"Response should politely refuse out-of-scope questions: {response_text}"

            # éªŒè¯å¼•å¯¼å›åº”æ€¥é¢†åŸŸ
            emergency_keywords = ["åº”æ€¥", "æ•‘æ´", "ç¾å®³"]
            has_guidance = any(kw in response_text for kw in emergency_keywords)
            assert has_guidance, \
                f"Response should guide back to emergency domain: {response_text}"

            print(f"âœ… Out-of-scope question properly refused")
            print(f"âœ… Response: {response_text[:200]}...")
        else:
            print(f"â„¹ï¸  Unexpected: classified as {unified_intent['intent_type']}")

    def test_idempotency(self, llm_client, llm_model):
        """æµ‹è¯•å¹‚ç­‰æ€§ï¼šå¤šæ¬¡è°ƒç”¨åªæ‰§è¡Œä¸€æ¬¡LLM"""
        state = {
            "messages": [
                {"role": "user", "content": "å››å·èŒ‚å¿åœ°éœ‡ï¼Œéœ€è¦æ•‘æ´"}
            ]
        }

        # ç¬¬ä¸€æ¬¡è°ƒç”¨
        t1_start = time.time()
        result_state = unified_intent_node(state, llm_client, llm_model)
        t1_end = time.time()
        duration1_ms = int((t1_end - t1_start) * 1000)

        # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆåº”è¯¥ç›´æ¥è¿”å›ï¼Œä¸è°ƒç”¨LLMï¼‰
        t2_start = time.time()
        result_state2 = unified_intent_node(result_state, llm_client, llm_model)
        t2_end = time.time()
        duration2_ms = int((t2_end - t2_start) * 1000)

        print(f"â±ï¸  First call: {duration1_ms}ms")
        print(f"â±ï¸  Second call (cached): {duration2_ms}ms")

        # ç¬¬äºŒæ¬¡è°ƒç”¨åº”è¯¥éå¸¸å¿«ï¼ˆ<10msï¼‰
        assert duration2_ms < 10, \
            f"Second call should be instant (cached): {duration2_ms}ms"

        # ç»“æœåº”è¯¥å®Œå…¨ç›¸åŒ
        assert result_state2["unified_intent"] == result_state["unified_intent"]

        print(f"âœ… Idempotency verified: {duration1_ms}ms â†’ {duration2_ms}ms")

    def test_performance_target(self, llm_client, llm_model):
        """æµ‹è¯•æ€§èƒ½ç›®æ ‡ï¼šå•æ¬¡ç»Ÿä¸€è°ƒç”¨â‰¤18ç§’ï¼ˆP95ï¼‰"""
        test_inputs = [
            "å››å·èŒ‚å¿å‘ç”Ÿåœ°éœ‡ï¼Œåæ ‡103.85,31.68",
            "éœ€è¦ç´§æ€¥åŒ»ç–—æ•‘æ´",
            "è¯·æ±‚ç©ºä¸­æ”¯æ´",
        ]

        durations = []

        for input_text in test_inputs:
            state = {"messages": [{"role": "user", "content": input_text}]}

            t_start = time.time()
            result_state = unified_intent_node(state, llm_client, llm_model)
            t_end = time.time()

            duration_ms = int((t_end - t_start) * 1000)
            durations.append(duration_ms)

            print(f"â±ï¸  '{input_text[:30]}...': {duration_ms}ms")

        avg_duration = sum(durations) / len(durations)
        max_duration = max(durations)

        print(f"\nğŸ“Š Performance Summary:")
        print(f"   Average: {avg_duration:.0f}ms")
        print(f"   Max (P100): {max_duration}ms")
        print(f"   Min: {min(durations)}ms")

        # P95ç›®æ ‡ï¼šâ‰¤18ç§’
        assert max_duration <= 18000, \
            f"Performance target not met: {max_duration}ms > 18000ms"

        # æœŸæœ›å¹³å‡å€¼â‰¤15ç§’
        assert avg_duration <= 15000, \
            f"Average performance target not met: {avg_duration:.0f}ms > 15000ms"

        print(f"âœ… Performance targets met")


@pytest.mark.integration
class TestIntegrationWithProcessor:
    """æµ‹è¯•ä¸intent_processor.pyçš„é›†æˆ"""

    @pytest.mark.skip(reason="éœ€è¦å®Œæ•´çš„æ•°æ®åº“å’ŒæœåŠ¡ä¾èµ–ï¼Œåœ¨å®é™…ç¯å¢ƒä¸­æµ‹è¯•")
    def test_end_to_end_unified_mode(self):
        """ç«¯åˆ°ç«¯æµ‹è¯•ï¼šç»Ÿä¸€æ¨¡å¼å®Œæ•´æµç¨‹"""
        # è¿™ä¸ªæµ‹è¯•éœ€è¦ï¼š
        # 1. PostgreSQLæ•°æ®åº“
        # 2. Mem0æœåŠ¡
        # 3. Intent handleræ³¨å†Œè¡¨
        # 4. å®Œæ•´çš„process_intent_coreå‡½æ•°
        pass

    @pytest.mark.skip(reason="éœ€è¦å®Œæ•´çš„æ•°æ®åº“å’ŒæœåŠ¡ä¾èµ–ï¼Œåœ¨å®é™…ç¯å¢ƒä¸­æµ‹è¯•")
    def test_mode_switching(self):
        """æµ‹è¯•ç»Ÿä¸€æ¨¡å¼å’Œæ—§ç‰ˆæ¨¡å¼åˆ‡æ¢"""
        # æµ‹è¯•é€šè¿‡ç¯å¢ƒå˜é‡åˆ‡æ¢æ¨¡å¼
        pass


if __name__ == "__main__":
    # å¿«é€Ÿæ‰‹åŠ¨æµ‹è¯•
    import sys
    from pathlib import Path

    # åŠ è½½ç¯å¢ƒå˜é‡
    project_root = Path(__file__).parent.parent.parent
    env_path = project_root / "config" / "dev.env"

    if env_path.exists():
        with open(env_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

    print("æ‰‹åŠ¨æµ‹è¯• - ç»Ÿä¸€æ„å›¾å¤„ç†é›†æˆ")
    print("=" * 60)

    cfg = AppConfig.load_from_env()
    client = get_openai_client(cfg)
    model = os.getenv("LLM_MODEL", "glm-4.5-air")

    test_state = {
        "messages": [
            {"role": "user", "content": "å››å·èŒ‚å¿å‘ç”Ÿåœ°éœ‡ï¼Œéœ€è¦æ•‘æ´ï¼Œåæ ‡103.85,31.68"}
        ]
    }

    print(f"æµ‹è¯•è¾“å…¥: {test_state['messages'][0]['content']}")
    print(f"ä½¿ç”¨æ¨¡å‹: {model}")

    t_start = time.time()
    result = unified_intent_node(test_state, client, model)
    t_end = time.time()

    print(f"\nâ±ï¸  è€—æ—¶: {int((t_end - t_start) * 1000)}ms")
    print(f"ğŸ¯ ç»“æœ:")

    import json
    print(json.dumps(result["unified_intent"], ensure_ascii=False, indent=2))

    print("\nâœ… æ‰‹åŠ¨æµ‹è¯•å®Œæˆ")
