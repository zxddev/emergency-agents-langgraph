# 车载移动指挥系统 - 展会演示版技术方案

> **文档版本**: v2.0 Demo Edition
> **创建日期**: 2025-10-27
> **系统定位**: 基于现有架构的展会演示系统
> **核心目标**: 效果优先，专家验证，充分利用2×H100 GPU算力

---

## 目录
1. [系统定位与目标](#一系统定位与目标)
2. [现有架构分析](#二现有架构分析)
3. [演示场景设计](#三演示场景设计)
4. [技术架构扩展](#四技术架构扩展)
5. [双H100算力分配](#五双h100算力分配)
6. [核心功能实现](#六核心功能实现)
7. [炫酷效果设计](#七炫酷效果设计)
8. [部署架构](#八部署架构)
9. [演示数据预置](#九演示数据预置)
10. [风险与应对](#十风险与应对)

---

## 一、系统定位与目标

### 1.1 核心定位

**车载指挥演示系统 = 现有系统（80%） + 车载三阶段模块（20%）**

这不是独立的新系统，而是对现有应急救援系统的功能扩展和演示增强。

### 1.2 演示目标

| 目标 | 说明 | 验收标准 |
|------|------|---------|
| **展会吸引力** | 3D可视化、流畅动画、炫酷界面 | 观众驻足率 > 80% |
| **专家可信度** | 推理链透明、数据真实、结果准确 | 专家认可度 > 90% |
| **技术先进性** | 双GPU并行、秒级响应、多模态AI | AI推理 < 3秒 |
| **业务完整性** | 完整的救援指挥闭环演示 | 覆盖三阶段全流程 |
| **系统稳定性** | 连续运行不崩溃 | 演示成功率 100% |

### 1.3 非目标（本次不考虑）

❌ 离线能力（演示环境全程在线）
❌ 降级方案（使用最强模型，不考虑降级）
❌ 生产部署（只需要演示环境稳定）
❌ 海量并发（演示现场只有1-2个操作员）
❌ 数据安全（演示数据可以公开）

---

## 二、现有架构分析

### 2.1 四层架构概览

```
┌─────────────────────────────────────────────────────────────┐
│  Layer 4: 前端展示层 (emergency-rescue-brain)               │
│  React 18 + Mars3D (Cesium) + Ant Design                   │
│  已有页面: /command, /reconnaissance, /contact             │
└─────────────────────────────────────────────────────────────┘
                            ↓ HTTPS / WebSocket
┌─────────────────────────────────────────────────────────────┐
│  Layer 3: 业务服务层 (emergency-web-api)                    │
│  Spring Boot 3.5.5 + Java 21 + MyBatis-Plus                │
│  功能: 事件管理、侦察任务、救援任务、WebSocket推送          │
└─────────────────────────────────────────────────────────────┘
                            ↓ HTTP / Kafka
┌─────────────────────────────────────────────────────────────┐
│  Layer 2: AI智能层 (emergency-agents-langgraph)             │
│  FastAPI + Python 3.10 + LangGraph                          │
│  功能: 意图识别、RAG检索、KG推理、方案生成                  │
└─────────────────────────────────────────────────────────────┘
                            ↓ Kafka / MQTT
┌─────────────────────────────────────────────────────────────┐
│  Layer 1: 设备接入层 (emergency-adapter-hub)                │
│  Spring Boot 3.5.5 + Java 21 + Kafka                        │
│  功能: 多厂商设备适配（DJI、大疆狗、虎鲸船等）              │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 现有技术栈

| 层级 | 技术栈 | 状态 |
|------|--------|------|
| **前端** | React 18, Mars3D, Ant Design, Redux, WebSocket (STOMP) | ✅ 已完成 |
| **业务层** | Spring Boot 3.5.5, MyBatis-Plus, Redis, Kafka, JWT | ✅ 已完成 |
| **AI层** | FastAPI, LangGraph, Neo4j, Qdrant, PostgreSQL | ✅ 已完成 |
| **设备层** | Spring Boot, Kafka (Avro), MQTT, PostgreSQL | ✅ 已完成 |
| **中间件** | PostgreSQL 17, Redis 7.2, Kafka 3.6, Neo4j 5.0, Qdrant 1.7 | ✅ 已部署 |

### 2.3 关键发现

✅ **优势**：
- 完整的EDA架构（Kafka解耦）
- 成熟的Spring Boot业务层
- 已有3D可视化能力（Mars3D）
- WebSocket实时推送已实现

⚠️ **不足**：
- 缺少"车载指挥三阶段"业务模块
- 缺少GLM-4V视觉分析能力
- 缺少AI推理链可视化
- 缺少演示场景预设数据

**结论**：80%的基础已完成，只需扩展20%的车载专用功能！

---

## 三、演示场景设计

### 3.1 演示剧本（7-8分钟）

**故事线：余杭区6.5级地震救援指挥全流程**

#### 场景1：灾情突发（30秒）

**演示内容**：
- 大屏显示余杭区3D地图
- 地震动画效果（震动 + 波纹扩散）
- AI语音播报："余杭区发生6.5级地震，震中位于东经120.15度，北纬30.28度"
- 自动触发态势感知Agent

**技术要点**：
- Mars3D地震动画效果
- 浏览器Speech API语音播报
- WebSocket自动触发事件

**展示效果**：
```
┌─────────────────────────────────────┐
│ 余杭区3D地图                         │
│                                     │
│     震中 ● ～～～～ (波纹扩散)      │
│                                     │
│ AI分析中...                          │
│ ├─ 重点目标识别                     │
│ │   ├─ P0: 5个化工厂                │
│ │   ├─ P1: 12所学校                 │
│ │   └─ P2: 3座桥梁                  │
│ └─ 预估受灾范围: 50km²              │
└─────────────────────────────────────┘
```

#### 场景2：出发前智能规划（2分钟）

**演示内容**：
1. **AI装备推荐**（60秒）
   - 点击"生成装备清单"按钮
   - 实时显示AI推理链：
     ```
     RAG检索 → 知识图谱推理 → 历史案例验证 → 人文分析
     ```
   - 输出结果：
     - 基础装备：生命探测仪、破拆工具、担架
     - 特殊装备：防化服（A级）- 因为有化工厂生产氯气
     - 人文装备：藏语翻译设备 - 少数民族地区

2. **智能路线规划**（40秒）
   - 点击"规划路线"按钮
   - 3D地图显示：
     - 主路线（绿色）
     - 备用路线（黄色）
     - 风险区域（红色标注：L2滑坡易发区）
   - 显示预计行驶时间、风险评估

3. **指挥员审批**（20秒）
   - 指挥员点击"审批通过"按钮
   - 系统提示："车队准备就绪，即将出发"
   - 切换到下一场景

**技术要点**：
- 调用AI服务 `/api/v1/vehicle/planning/equipment`
- React Flow可视化推理链
- Mars3D路径规划可视化
- Redux状态管理阶段切换

#### 场景3：行进中实时监控（2分钟）

**演示内容**：
1. **车队实时追踪**（30秒）
   - 3D地图显示5辆车的实时位置
   - 车载中控视图（大字体、高对比度）
   - 显示车速、油量、车间距

2. **无人机前置侦查**（60秒）
   - 上传预设的无人机航拍图（显示前方道路塌方）
   - GLM-4V实时分析（视觉AI展示）：
     ```json
     {
       "road_status": "中断",
       "obstacle": "大面积塌方",
       "danger_level": "L3",
       "recommendation": "立即停车，切换备用路线"
     }
     ```
   - 系统发出L3危险预警（红色闪烁 + 警报音）

3. **动态路线调整**（30秒）
   - AI自动计算绕行方案
   - 显示新路线（蓝色虚线）
   - 显示延误时间：+15分钟
   - 指挥员确认切换

**技术要点**：
- WebSocket推送车辆位置（1次/秒）
- 调用GLM-4V视觉分析 `/api/v1/vision/analyze`
- Mars3D动态路径更新
- 危险分级UI展示

#### 场景4：现场指挥（2.5分钟）

**演示内容**：
1. **无人机自主侦查重点目标**（45秒）
   - 上传化工厂航拍图
   - GLM-4V分析：
     - 氯气储罐：完好
     - 泄漏检测：未发现
     - 建筑损毁：轻微裂缝
     - 危险等级：L1（低风险）
   - 在3D地图标注化工厂状态（黄色标记）

2. **AI任务智能分配**（60秒）
   - 输入：5个救援队
     - 消防队（20人，装备：破拆工具）
     - 武警队（15人，装备：生命探测仪）
     - 志愿者（10人，装备：医疗包）
   - AI推理任务分配：
     - 消防队 → 任务A：搜救被困人员（需要破拆资质）
     - 武警队 → 任务B：化工厂安全检查（需要防化资质）
     - 志愿者 → 任务C：物资发放（协调，非指挥）
   - 可视化：任务卡片拖拽分配

3. **向省级上报**（45秒）
   - 点击"生成报告"按钮
   - AI自动撰写格式化报告：
     ```
     余杭区6.5级地震救援进度报告

     一、灾情概况
     - 震级：6.5级，震中：东经120.15度，北纬30.28度
     - 受灾范围：50km²
     - 人员伤亡：初步统计10人死亡，50人受伤

     二、救援进度
     - 已部署救援队：5支，共45人
     - 已完成任务：2/10（20%）
     - 重点目标检查：化工厂安全，无泄漏

     三、资源需求
     - 急需：医疗队50人、血浆200袋
     - 装备：大型吊车2台、发电机10台
     ```
   - 附带现场照片、视频
   - 模拟发送到"浙江省应急指挥大厅"

**技术要点**：
- GLM-4V多次视觉分析
- AI任务优化算法（约束条件：资质、装备、距离）
- GLM-4生成报告（自然语言生成）
- HTTP调用省级API模拟

### 3.2 演示数据预置

为保证演示流畅，预置以下数据：

**地理数据**：
- 余杭区高精度地图瓦片（Mars3D）
- 3D建筑物模型（重点目标）
- 道路网络 + 地形高程

**场景数据**：
- 预设场景：余杭区6.5级地震
- 震中坐标：(120.15, 30.28)
- 重点目标：5个化工厂、12所学校、3座桥梁

**图像数据**：
- 5张预设无人机航拍图：
  1. 化工厂全景（完好）
  2. 道路塌方（L3危险）
  3. 学校建筑（轻微损毁）
  4. 桥梁裂缝（L2风险）
  5. 居民区（重度损毁）

**知识数据**：
- Neo4j知识图谱：300个装备节点、50个化学品节点、100个历史案例
- Qdrant向量库：5000个RAG文档（应急预案、装备手册）
- 历史案例：汶川、雅安、九寨沟地震完整案例

**模拟设备**：
- 5架无人机（DJI Phantom 4）
- 10辆车辆（车队编队）
- 20个救援队员（消防、武警、志愿者）

---

## 四、技术架构扩展

### 4.1 扩展原则

✅ **最小侵入**：不破坏现有架构
✅ **模块化**：新增功能独立模块
✅ **可复用**：复用现有组件和服务
✅ **演示优先**：优化演示体验，不考虑生产部署

### 4.2 Layer 1: Adapter Hub（改动量：0%）

**现状**：已完成，支持DJI无人机、大疆狗等设备

**演示方案**：
- 使用模拟设备数据（不需要真实设备）
- 通过HTTP接口推送预设图像数据

```bash
# 模拟无人机推送数据
curl -X POST http://localhost:8080/uplink/dji/phantom4 \
  -H "Content-Type: application/json" \
  -d '{
    "device_id": "UAV-001",
    "timestamp": 1735283000000,
    "latitude": 30.28,
    "longitude": 120.15,
    "altitude": 120,
    "battery": 85,
    "image_url": "/demo/images/chemical_plant.jpg"
  }'
```

**结论**：Adapter Hub无需改动，直接复用！

### 4.3 Layer 2: Web API（改动量：20%）

**新增模块**：`com.cykj.webapi.vehicle`

#### 4.3.1 VehicleController（车载指挥控制器）

```java
package com.cykj.webapi.vehicle.controller;

@RestController
@RequestMapping("/api/vehicle")
@Tag(name = "车载指挥", description = "车载移动指挥系统API")
public class VehicleController {

    @Autowired
    private VehiclePlanningService planningService;

    @Autowired
    private VehicleMonitoringService monitoringService;

    @Autowired
    private VehicleCommandingService commandingService;

    @Autowired
    private AIServiceClient aiServiceClient;

    // ========== 阶段1：出发前规划 ==========

    @PostMapping("/planning/equipment")
    @Operation(summary = "AI装备推荐")
    public ResponseEntity<EquipmentRecommendation> recommendEquipment(
        @RequestBody DisasterContext context
    ) {
        // 调用AI服务的RAG + KG推理
        EquipmentRecommendation result = aiServiceClient.callEquipmentReasoning(context);
        return ResponseEntity.ok(result);
    }

    @PostMapping("/planning/route")
    @Operation(summary = "智能路线规划")
    public ResponseEntity<RoutePlan> planRoute(
        @RequestBody RouteRequest request
    ) {
        // 调用AI服务的GIS路径优化
        RoutePlan plan = aiServiceClient.callRouteOptimization(request);
        return ResponseEntity.ok(plan);
    }

    @PostMapping("/planning/target/identify")
    @Operation(summary = "重点目标识别")
    public ResponseEntity<List<PriorityTarget>> identifyTargets(
        @RequestBody DisasterArea area
    ) {
        List<PriorityTarget> targets = planningService.identifyPriorityTargets(area);
        return ResponseEntity.ok(targets);
    }

    // ========== 阶段2：行进中监控 ==========

    @PostMapping("/monitoring/danger/predict")
    @Operation(summary = "危险预测（含视觉AI）")
    public ResponseEntity<DangerPrediction> predictDanger(
        @RequestBody @Valid DangerPredictRequest request
    ) {
        DangerPrediction prediction;

        // 如果包含图像，调用GLM-4V视觉分析
        if (request.hasImage()) {
            prediction = aiServiceClient.callVisionAnalysis(request.getImageUrl());
        } else {
            // 否则基于位置和传感器数据预测
            prediction = monitoringService.predictByLocation(request.getLocation());
        }

        return ResponseEntity.ok(prediction);
    }

    @PostMapping("/monitoring/route/adjust")
    @Operation(summary = "动态路线调整")
    public ResponseEntity<RouteAdjustment> adjustRoute(
        @RequestBody RouteAdjustRequest request
    ) {
        RouteAdjustment adjustment = aiServiceClient.callRouteAdjustment(request);
        return ResponseEntity.ok(adjustment);
    }

    @GetMapping("/monitoring/convoy/{convoyId}")
    @Operation(summary = "车队实时状态")
    public ResponseEntity<ConvoyStatus> getConvoyStatus(
        @PathVariable String convoyId
    ) {
        ConvoyStatus status = monitoringService.getConvoyStatus(convoyId);
        return ResponseEntity.ok(status);
    }

    // ========== 阶段3：现场指挥 ==========

    @PostMapping("/commanding/task/allocate")
    @Operation(summary = "AI任务智能分配")
    public ResponseEntity<TaskAllocation> allocateTask(
        @RequestBody @Valid TaskAllocationRequest request
    ) {
        // 调用AI服务的任务优化算法
        TaskAllocation allocation = aiServiceClient.callTaskOptimization(request);

        // 推送到WebSocket（实时通知救援队）
        messagingTemplate.convertAndSend(
            "/topic/tasks/" + request.getConvoyId(),
            allocation
        );

        return ResponseEntity.ok(allocation);
    }

    @PostMapping("/commanding/authority/takeover")
    @Operation(summary = "指挥权接管")
    public ResponseEntity<AuthorityLevel> takeoverCommand(
        @RequestBody RescuerInfo rescuer
    ) {
        AuthorityLevel authority = commandingService.assignAuthority(rescuer);
        return ResponseEntity.ok(authority);
    }

    @PostMapping("/commanding/report/generate")
    @Operation(summary = "向省级上报（AI生成报告）")
    public ResponseEntity<Report> generateReport(
        @RequestBody ReportRequest request
    ) {
        // 调用AI服务生成格式化报告
        Report report = aiServiceClient.callReportGeneration(request);

        // 模拟发送到省级API
        provincialApiClient.submitReport(report);

        return ResponseEntity.ok(report);
    }
}
```

#### 4.3.2 AIServiceClient（AI服务HTTP客户端）

```java
package com.cykj.webapi.vehicle.client;

@Service
public class AIServiceClient {

    @Value("${ai.service.base-url}")
    private String aiServiceBaseUrl; // http://localhost:8008

    private final RestTemplate restTemplate;

    public AIServiceClient(RestTemplateBuilder builder) {
        this.restTemplate = builder
            .setConnectTimeout(Duration.ofSeconds(30))
            .setReadTimeout(Duration.ofSeconds(60))
            .build();
    }

    /**
     * 调用装备推荐（RAG + KG推理）
     */
    public EquipmentRecommendation callEquipmentReasoning(DisasterContext context) {
        String url = aiServiceBaseUrl + "/api/v1/vehicle/planning/equipment";

        return restTemplate.postForObject(
            url,
            context,
            EquipmentRecommendation.class
        );
    }

    /**
     * 调用视觉分析（GLM-4V）
     */
    public DangerPrediction callVisionAnalysis(String imageUrl) {
        String url = aiServiceBaseUrl + "/api/v1/vehicle/monitoring/vision";

        Map<String, Object> request = Map.of(
            "image_url", imageUrl,
            "analysis_type", "danger_detection"
        );

        return restTemplate.postForObject(
            url,
            request,
            DangerPrediction.class
        );
    }

    /**
     * 调用任务优化（约束求解）
     */
    public TaskAllocation callTaskOptimization(TaskAllocationRequest request) {
        String url = aiServiceBaseUrl + "/api/v1/vehicle/commanding/optimize";

        return restTemplate.postForObject(
            url,
            request,
            TaskAllocation.class
        );
    }

    /**
     * 调用报告生成（GLM-4自然语言生成）
     */
    public Report callReportGeneration(ReportRequest request) {
        String url = aiServiceBaseUrl + "/api/v1/vehicle/commanding/report";

        return restTemplate.postForObject(
            url,
            request,
            Report.class
        );
    }
}
```

#### 4.3.3 数据库表扩展

```sql
-- 车队表
CREATE TABLE convoy (
    convoy_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    disaster_id UUID,
    convoy_name VARCHAR(100) NOT NULL,
    commander_id VARCHAR(50),
    status VARCHAR(20) DEFAULT 'preparing', -- preparing/traveling/arrived
    departure_location GEOMETRY(POINT, 4326),
    destination GEOMETRY(POINT, 4326),
    created_at TIMESTAMP DEFAULT NOW()
);

-- 车辆表
CREATE TABLE vehicle (
    vehicle_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    convoy_id UUID REFERENCES convoy(convoy_id),
    license_plate VARCHAR(20) NOT NULL,
    vehicle_type VARCHAR(50), -- command/supply/rescue
    current_location GEOMETRY(POINT, 4326),
    speed DECIMAL(5,2),
    fuel_percent DECIMAL(5,2),
    status VARCHAR(20) DEFAULT 'normal',
    updated_at TIMESTAMP DEFAULT NOW()
);

-- 阶段状态表
CREATE TABLE stage_state (
    convoy_id UUID PRIMARY KEY REFERENCES convoy(convoy_id),
    current_stage VARCHAR(20), -- planning/monitoring/commanding
    stage_data JSONB, -- 阶段专用数据
    updated_at TIMESTAMP DEFAULT NOW()
);

-- 演示场景表
CREATE TABLE demo_scenario (
    scenario_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    scenario_name VARCHAR(100) NOT NULL,
    disaster_type VARCHAR(50),
    epicenter GEOMETRY(POINT, 4326),
    magnitude DECIMAL(3,1),
    preset_data JSONB, -- 预设的图像URL、目标信息等
    created_at TIMESTAMP DEFAULT NOW()
);
```

**代码量估算**：
- Controller: 约250行
- Service: 约800行
- Client: 约200行
- DTO: 约300行
- **总计**: 约1550行新代码

### 4.4 Layer 3: AI服务（改动量：30%）

**新增模块**：`src/emergency_agents/vehicle/`

#### 4.4.1 装备推荐（RAG + KG混合推理）

```python
# src/emergency_agents/vehicle/planning.py
from emergency_agents.rag import RescueRAGPipeline
from emergency_agents.graph import KnowledgeGraphService

class EquipmentRecommender:
    def __init__(self):
        self.rag = RescueRAGPipeline()
        self.kg = KnowledgeGraphService()
        self.llm = OpenAI(base_url="http://localhost:8002/v1")  # GLM-4-Plus

    async def recommend(self, disaster_context: dict):
        """
        装备推荐（展示推理链）

        返回格式：
        {
            "basic_equipment": [...],
            "special_equipment": [...],
            "cultural_equipment": [...],
            "reasoning_chain": [  # 推理链（展示给专家）
                {
                    "step": 1,
                    "type": "RAG检索",
                    "source": "GB/T 33743-2017",
                    "result": "基础装备清单",
                    "duration_ms": 120
                },
                {
                    "step": 2,
                    "type": "知识图谱推理",
                    "reasoning": "化工厂→氯气→防化服",
                    "confidence": 0.95,
                    "duration_ms": 80
                },
                ...
            ]
        }
        """
        reasoning_chain = []
        equipment_list = {"basic": [], "special": [], "cultural": []}

        # 步骤1：RAG检索基础装备
        start_time = time.time()
        rag_results = await self.rag.retrieve(
            query=f"{disaster_context['type']}救援需要哪些装备？",
            domain="standards",
            top_k=5
        )
        duration_ms = int((time.time() - start_time) * 1000)

        for doc in rag_results["documents"]:
            equipment_list["basic"].extend(
                self._parse_equipment_from_standard(doc["text"])
            )

        reasoning_chain.append({
            "step": 1,
            "type": "RAG检索",
            "source": rag_results["documents"][0]["metadata"]["source"],
            "result": f"检索到{len(rag_results['documents'])}个相关文档",
            "duration_ms": duration_ms
        })

        # 步骤2：知识图谱推理特殊装备
        start_time = time.time()
        kg_equipment = await self.kg.infer_equipment(disaster_context)
        duration_ms = int((time.time() - start_time) * 1000)

        for item in kg_equipment:
            equipment_list["special"].append({
                "name": item["equipment"],
                "spec": item["spec"],
                "reason": item["reasoning"]
            })

        reasoning_chain.append({
            "step": 2,
            "type": "知识图谱推理",
            "reasoning": kg_equipment[0]["reasoning"] if kg_equipment else "",
            "confidence": 0.95,
            "duration_ms": duration_ms
        })

        # 步骤3：历史案例验证
        start_time = time.time()
        similar_cases = await self.kg.find_similar_cases(disaster_context, top_k=3)
        duration_ms = int((time.time() - start_time) * 1000)

        if similar_cases:
            case_equipment = self._extract_equipment_from_cases(similar_cases)
            equipment_list["basic"].extend(case_equipment)

            reasoning_chain.append({
                "step": 3,
                "type": "历史案例验证",
                "case": similar_cases[0]["name"],
                "result": f"参考{len(similar_cases)}个历史案例",
                "duration_ms": duration_ms
            })

        # 步骤4：人文因素分析
        if "ethnicity" in disaster_context:
            cultural_equipment = await self._get_cultural_equipment(
                disaster_context["ethnicity"]
            )
            equipment_list["cultural"] = cultural_equipment

            reasoning_chain.append({
                "step": 4,
                "type": "人文分析",
                "ethnicity": disaster_context["ethnicity"],
                "result": f"考虑{disaster_context['ethnicity']}习俗，增加文化装备",
                "duration_ms": 50
            })

        # 去重
        equipment_list["basic"] = list(set(equipment_list["basic"]))

        return {
            **equipment_list,
            "reasoning_chain": reasoning_chain
        }
```

#### 4.4.2 视觉分析（GLM-4V）

```python
# src/emergency_agents/vehicle/vision_analysis.py
import base64
import httpx

class VisionAnalyzer:
    def __init__(self):
        self.vllm_url = "http://localhost:8001/v1"  # GLM-4V-Plus

    async def analyze_drone_image(self, image_path: str):
        """
        分析无人机航拍图（危险检测）

        返回格式：
        {
            "people_count": 5,
            "vehicles": [{"type": "卡车", "count": 2}],
            "building_damage": "轻微",
            "road_status": "中断",
            "dangers": ["大面积塌方", "次生滑坡风险"],
            "danger_level": "L3",
            "analysis_time_ms": 2450
        }
        """
        start_time = time.time()

        # 读取图像并编码
        with open(image_path, 'rb') as f:
            image_base64 = base64.b64encode(f.read()).decode()

        # 构造多模态prompt
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_base64}"
                        }
                    },
                    {
                        "type": "text",
                        "text": """
你是应急救援专家。分析这张无人机航拍图，识别：
1. 人员数量和位置
2. 车辆类型和数量
3. 建筑物损毁情况
4. 道路通行状态
5. 危险因素（火灾、泄漏、倒塌、塌方）

返回JSON格式：
{
  "people_count": 整数,
  "vehicles": [{"type": "类型", "count": 数量}],
  "building_damage": "无损/轻微/严重/倒塌",
  "road_status": "畅通/受阻/中断",
  "dangers": ["危险1", "危险2"],
  "danger_level": "L0/L1/L2/L3"
}

危险等级定义：
- L0: 正常，无危险
- L1: 低风险，需注意
- L2: 高风险，建议绕行
- L3: 危急，立即停车
                        """
                    }
                ]
            }
        ]

        # 调用GLM-4V
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{self.vllm_url}/chat/completions",
                json={
                    "model": "glm-4v-plus",
                    "messages": messages,
                    "temperature": 0.1,  # 低温度，稳定输出
                    "max_tokens": 500
                }
            )

        result = response.json()
        content = result["choices"][0]["message"]["content"]

        # 解析JSON
        analysis = json.loads(content)

        # 添加性能指标
        duration_ms = int((time.time() - start_time) * 1000)
        analysis["analysis_time_ms"] = duration_ms

        return analysis
```

#### 4.4.3 任务分配优化

```python
# src/emergency_agents/vehicle/commanding.py
from ortools.sat.python import cp_model

class TaskAllocator:
    def __init__(self):
        self.kg = KnowledgeGraphService()

    async def allocate(
        self,
        tasks: List[dict],
        rescuers: List[dict]
    ):
        """
        AI任务智能分配（约束优化）

        约束条件：
        1. 资质匹配（消防队→破拆任务，武警→防化任务）
        2. 装备匹配（有破拆工具→破拆任务）
        3. 距离最优（最近的队伍优先）
        4. 负载均衡（避免某个队伍任务过多）
        """
        model = cp_model.CpModel()

        # 决策变量：任务i是否分配给救援队j
        assignments = {}
        for i, task in enumerate(tasks):
            for j, rescuer in enumerate(rescuers):
                assignments[(i, j)] = model.NewBoolVar(f'task_{i}_rescuer_{j}')

        # 约束1：每个任务必须分配给一个救援队
        for i in range(len(tasks)):
            model.Add(sum(assignments[(i, j)] for j in range(len(rescuers))) == 1)

        # 约束2：资质匹配
        for i, task in enumerate(tasks):
            required_skill = task["required_skill"]
            for j, rescuer in enumerate(rescuers):
                if required_skill not in rescuer["skills"]:
                    model.Add(assignments[(i, j)] == 0)

        # 约束3：负载均衡（每个队伍最多3个任务）
        for j in range(len(rescuers)):
            model.Add(sum(assignments[(i, j)] for i in range(len(tasks))) <= 3)

        # 目标函数：最小化总距离
        total_distance = []
        for i, task in enumerate(tasks):
            for j, rescuer in enumerate(rescuers):
                distance = self._calculate_distance(
                    task["location"],
                    rescuer["location"]
                )
                total_distance.append(assignments[(i, j)] * distance)

        model.Minimize(sum(total_distance))

        # 求解
        solver = cp_model.CpSolver()
        status = solver.Solve(model)

        if status == cp_model.OPTIMAL:
            allocations = []
            for i, task in enumerate(tasks):
                for j, rescuer in enumerate(rescuers):
                    if solver.Value(assignments[(i, j)]):
                        allocations.append({
                            "task_id": task["id"],
                            "task_name": task["name"],
                            "rescuer_id": rescuer["id"],
                            "rescuer_name": rescuer["name"],
                            "reason": f"{rescuer['name']}具备{task['required_skill']}技能，距离最近"
                        })

            return {
                "allocations": allocations,
                "total_distance": solver.ObjectiveValue(),
                "solve_time_ms": solver.WallTime()
            }
        else:
            raise Exception("无法找到可行的分配方案")
```

#### 4.4.4 API路由

```python
# src/emergency_agents/api/vehicle.py
from fastapi import APIRouter
from emergency_agents.vehicle import (
    EquipmentRecommender,
    VisionAnalyzer,
    TaskAllocator
)

router = APIRouter(prefix="/api/v1/vehicle", tags=["车载指挥"])

equipment_recommender = EquipmentRecommender()
vision_analyzer = VisionAnalyzer()
task_allocator = TaskAllocator()

@router.post("/planning/equipment")
async def recommend_equipment(context: DisasterContext):
    """装备推荐（RAG + KG）"""
    return await equipment_recommender.recommend(context.dict())

@router.post("/monitoring/vision")
async def analyze_vision(request: VisionRequest):
    """视觉分析（GLM-4V）"""
    return await vision_analyzer.analyze_drone_image(request.image_url)

@router.post("/commanding/optimize")
async def optimize_task(request: TaskOptimizationRequest):
    """任务分配优化"""
    return await task_allocator.allocate(
        request.tasks,
        request.rescuers
    )

@router.post("/commanding/report")
async def generate_report(request: ReportRequest):
    """生成上报报告（GLM-4自然语言生成）"""
    # 调用GLM-4生成格式化报告
    # ...实现细节
    pass
```

**代码量估算**：
- planning.py: 约400行
- vision_analysis.py: 约250行
- commanding.py: 约350行
- API路由: 约150行
- **总计**: 约1150行新代码

### 4.5 Layer 4: 前端（改动量：40%）

**新增路由**：`/vehicle/planning`, `/vehicle/monitoring`, `/vehicle/commanding`

#### 4.5.1 路由配置

```javascript
// src/router/index.jsx
import { createBrowserRouter } from 'react-router-dom';
import PlanningDashboard from '@/view/vehicle/PlanningDashboard';
import MonitoringDashboard from '@/view/vehicle/MonitoringDashboard';
import CommandingDashboard from '@/view/vehicle/CommandingDashboard';

const router = createBrowserRouter([
  // 现有路由
  { path: '/login', element: <Login /> },
  { path: '/command', element: <Command /> },
  { path: '/reconnaissance', element: <Reconnaissance /> },
  { path: '/contact', element: <Contact /> },

  // 新增车载路由
  {
    path: '/vehicle',
    element: <VehicleLayout />,
    children: [
      {
        path: 'planning',
        element: <PlanningDashboard />,
        meta: { title: '出发前规划', stage: 'planning' }
      },
      {
        path: 'monitoring',
        element: <MonitoringDashboard />,
        meta: { title: '行进中监控', stage: 'monitoring' }
      },
      {
        path: 'commanding',
        element: <CommandingDashboard />,
        meta: { title: '现场指挥', stage: 'commanding' }
      }
    ]
  }
]);
```

#### 4.5.2 出发前规划页面

```jsx
// src/view/vehicle/PlanningDashboard.jsx
import React, { useState } from 'react';
import { Card, Button, Spin, Timeline } from 'antd';
import { MapContainer } from '@/components/MapContainer';
import { ReasoningChainFlow } from '@/components/ReasoningChainFlow';
import { planningService } from '@/services/vehicle/planningService';

const PlanningDashboard = () => {
  const [loading, setLoading] = useState(false);
  const [equipment, setEquipment] = useState(null);
  const [route, setRoute] = useState(null);

  // 装备推荐
  const handleRecommendEquipment = async () => {
    setLoading(true);
    try {
      const result = await planningService.recommendEquipment({
        disaster_type: 'earthquake',
        magnitude: 6.5,
        location: { lat: 30.28, lon: 120.15 }
      });
      setEquipment(result);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="planning-dashboard">
      <div className="left-panel">
        {/* 3D地图 */}
        <MapContainer
          center={[120.15, 30.28]}
          zoom={10}
          layers={['earthquake', 'priority-targets', 'routes']}
        />
      </div>

      <div className="right-panel">
        {/* 装备推荐 */}
        <Card title="AI装备推荐">
          <Button
            type="primary"
            onClick={handleRecommendEquipment}
            loading={loading}
          >
            生成装备清单
          </Button>

          {equipment && (
            <>
              {/* 推理链可视化 */}
              <ReasoningChainFlow chain={equipment.reasoning_chain} />

              {/* 装备清单 */}
              <Timeline>
                <Timeline.Item color="green">
                  基础装备: {equipment.basic.join('、')}
                </Timeline.Item>
                <Timeline.Item color="orange">
                  特殊装备: {equipment.special.map(e => e.name).join('、')}
                </Timeline.Item>
                <Timeline.Item color="blue">
                  人文装备: {equipment.cultural.join('、')}
                </Timeline.Item>
              </Timeline>
            </>
          )}
        </Card>

        {/* 路线规划 */}
        <Card title="智能路线规划">
          {/* ...实现细节 */}
        </Card>

        {/* 审批按钮 */}
        <Button
          type="primary"
          size="large"
          block
          onClick={handleApprove}
        >
          指挥员审批通过，立即出发
        </Button>
      </div>
    </div>
  );
};
```

#### 4.5.3 推理链可视化组件

```jsx
// src/components/ReasoningChainFlow.jsx
import React from 'react';
import ReactFlow, { Background, Controls } from 'reactflow';
import 'reactflow/dist/style.css';

const ReasoningChainFlow = ({ chain }) => {
  // 构造节点
  const nodes = chain.map((step, idx) => ({
    id: `step-${idx}`,
    type: 'custom',
    data: {
      label: step.type,
      description: step.result,
      duration: `${step.duration_ms}ms`
    },
    position: { x: idx * 300, y: 100 }
  }));

  // 构造边（动画效果）
  const edges = chain.slice(0, -1).map((_, idx) => ({
    id: `edge-${idx}`,
    source: `step-${idx}`,
    target: `step-${idx + 1}`,
    animated: true,
    label: `${chain[idx + 1].duration_ms}ms`
  }));

  return (
    <div style={{ height: 300 }}>
      <ReactFlow
        nodes={nodes}
        edges={edges}
        fitView
      >
        <Background />
        <Controls />
      </ReactFlow>
    </div>
  );
};
```

#### 4.5.4 行进中监控页面

```jsx
// src/view/vehicle/MonitoringDashboard.jsx
import React, { useState, useEffect } from 'react';
import { Alert, Upload, Button } from 'antd';
import { MapContainer } from '@/components/MapContainer';
import { DangerLevelIndicator } from '@/components/DangerLevelIndicator';
import { monitoringService } from '@/services/vehicle/monitoringService';

const MonitoringDashboard = () => {
  const [dangerLevel, setDangerLevel] = useState('L0');
  const [convoyStatus, setConvoyStatus] = useState(null);

  // WebSocket实时订阅车队位置
  useEffect(() => {
    const ws = monitoringService.subscribeConvoyStatus('convoy-001', (status) => {
      setConvoyStatus(status);
    });

    return () => ws.close();
  }, []);

  // 上传无人机图像分析
  const handleImageUpload = async (file) => {
    const result = await monitoringService.analyzeDroneImage(file);
    setDangerLevel(result.danger_level);

    // 如果是L3危险，播放警报
    if (result.danger_level === 'L3') {
      playAlertSound();
      showCriticalAlert(result);
    }
  };

  return (
    <div className="monitoring-dashboard" style={{
      fontSize: '24px',  // 车载大字体
      backgroundColor: '#000'  // 黑色背景
    }}>
      {/* 顶部危险预警 */}
      {dangerLevel !== 'L0' && (
        <Alert
          type={dangerLevel === 'L3' ? 'error' : 'warning'}
          message="危险预警"
          description={getDangerMessage(dangerLevel)}
          banner
          style={{
            fontSize: 32,
            animation: dangerLevel === 'L3' ? 'blink 1s infinite' : 'none'
          }}
        />
      )}

      {/* 3D地图（车队追踪） */}
      <MapContainer
        center={convoyStatus?.leader.position}
        zoom={13}
        pitch={60}  // 3D倾斜
        layers={['route', 'convoy', 'danger-zones', 'drone-view']}
      />

      {/* 无人机侦查 */}
      <Card title="无人机前置侦查">
        <Upload
          accept="image/*"
          beforeUpload={handleImageUpload}
        >
          <Button>上传无人机图像</Button>
        </Upload>
      </Card>
    </div>
  );
};
```

#### 4.5.5 现场指挥页面

```jsx
// src/view/vehicle/CommandingDashboard.jsx
import React, { useState } from 'react';
import { Card, Button, Table, Tag } from 'antd';
import { DndProvider, useDrag, useDrop } from 'react-dnd';
import { commandingService } from '@/services/vehicle/commandingService';

const CommandingDashboard = () => {
  const [tasks, setTasks] = useState([]);
  const [rescuers, setRescuers] = useState([]);
  const [allocation, setAllocation] = useState(null);

  // AI任务智能分配
  const handleAutoAllocate = async () => {
    const result = await commandingService.allocateTask({
      tasks,
      rescuers
    });
    setAllocation(result);
  };

  // 生成上报报告
  const handleGenerateReport = async () => {
    const report = await commandingService.generateReport({
      convoy_id: 'convoy-001',
      disaster_id: 'disaster-001'
    });

    // 显示报告预览
    showReportModal(report);
  };

  return (
    <div className="commanding-dashboard">
      {/* 任务列表 */}
      <Card title="待分配任务">
        <Table
          dataSource={tasks}
          columns={[
            { title: '任务名称', dataKey: 'name' },
            { title: '优先级', dataKey: 'priority' },
            { title: '所需技能', dataKey: 'required_skill' }
          ]}
        />
        <Button onClick={handleAutoAllocate}>
          AI智能分配
        </Button>
      </Card>

      {/* 分配结果 */}
      {allocation && (
        <Card title="分配结果">
          {allocation.allocations.map(alloc => (
            <div key={alloc.task_id}>
              <Tag color="blue">{alloc.task_name}</Tag>
              →
              <Tag color="green">{alloc.rescuer_name}</Tag>
              <span>{alloc.reason}</span>
            </div>
          ))}
        </Card>
      )}

      {/* 向省级上报 */}
      <Card title="向省级上报">
        <Button
          type="primary"
          onClick={handleGenerateReport}
        >
          生成报告
        </Button>
      </Card>
    </div>
  );
};
```

**代码量估算**：
- PlanningDashboard: 约800行
- MonitoringDashboard: 约700行
- CommandingDashboard: 约900行
- 组件库: 约1000行
- 业务逻辑层: 约750行
- **总计**: 约4150行新代码

---

## 五、双H100算力分配

### 5.1 硬件配置

假设硬件环境：
- **服务器1**: H100 GPU #1 (80GB VRAM) + 32核CPU + 256GB RAM
- **服务器2**: H100 GPU #2 (80GB VRAM) + 32核CPU + 256GB RAM

### 5.2 GPU算力分配策略

**H100 GPU #1：多模态视觉推理**

```yaml
GPU: H100 #1 (80GB VRAM)
服务器: Server 1

部署模型:
  - GLM-4V-Plus (视觉理解)
    VRAM: 35GB
    用途: 无人机图像分析、危险检测
    端口: 8001

  - Qwen2-VL-72B (备用视觉模型)
    VRAM: 35GB
    用途: 视频流分析、多帧理解
    端口: 8003

  - BGE-CLIP (多模态嵌入)
    VRAM: 5GB
    用途: 图像-文本相似度检索
    端口: 8004

性能指标:
  - 图像分析延迟: 2-3秒
  - 吞吐量: 20 images/min
  - 批处理: 支持batch=4
```

**H100 GPU #2：文本生成 + 向量检索**

```yaml
GPU: H100 #2 (80GB VRAM)
服务器: Server 2

部署模型:
  - GLM-4-Plus (长上下文LLM)
    VRAM: 30GB
    用途: 装备推荐、报告生成、决策解释
    端口: 8002

  - BGE-M3 (嵌入模型)
    VRAM: 4GB
    用途: RAG向量化、语义检索
    端口: 8005

  - BGE-Reranker-v2-m3 (重排序)
    VRAM: 3GB
    用途: RAG结果重排序
    端口: 8006

性能指标:
  - 文本生成延迟: 500-800ms (512 tokens)
  - 吞吐量: 80 requests/min
  - 上下文长度: 32K tokens
```

### 5.3 vLLM部署配置

**服务器1部署脚本**

```bash
# GPU #1: GLM-4V-Plus
docker run -d --name vllm-glm4v \
  --gpus '"device=0"' \
  --shm-size 16g \
  -p 8001:8000 \
  -v /data/models:/models \
  vllm/vllm-openai:latest \
  --model /models/glm-4v-plus \
  --served-model-name glm-4v-plus \
  --gpu-memory-utilization 0.85 \
  --max-model-len 8192 \
  --trust-remote-code \
  --dtype auto

# 验证
curl http://localhost:8001/v1/models
```

**服务器2部署脚本**

```bash
# GPU #2: GLM-4-Plus
docker run -d --name vllm-glm4 \
  --gpus '"device=0"' \
  --shm-size 16g \
  -p 8002:8000 \
  -v /data/models:/models \
  vllm/vllm-openai:latest \
  --model /models/glm-4-plus \
  --served-model-name glm-4-plus \
  --gpu-memory-utilization 0.8 \
  --max-model-len 32768 \
  --trust-remote-code \
  --enable-prefix-caching \
  --dtype auto

# 验证
curl http://localhost:8002/v1/models
```

### 5.4 CPU资源分配

**服务器1 (32核CPU)**

```yaml
服务分配:
  - vLLM-Vision (GPU服务): 8核
  - PostgreSQL 17: 8核
  - Neo4j 5.0: 8核
  - Qdrant 1.7: 4核
  - Redis 7.2: 2核
  - 系统预留: 2核
```

**服务器2 (32核CPU)**

```yaml
服务分配:
  - vLLM-Text (GPU服务): 8核
  - FastAPI (AI服务): 8核
  - Spring Boot (Web API): 8核
  - Spring Boot (Adapter Hub): 4核
  - Nginx: 2核
  - 系统预留: 2核
```

### 5.5 并行推理示例

**场景**：用户上传无人机图像，同时请求装备推荐

```python
import asyncio

async def handle_planning_request(disaster_context, drone_image):
    # 并行任务1：视觉分析（GPU #1）
    vision_task = analyze_drone_image(drone_image)  # → GPU #1

    # 并行任务2：装备推荐（GPU #2）
    equipment_task = recommend_equipment(disaster_context)  # → GPU #2

    # 并行执行，等待两者完成
    vision_result, equipment_result = await asyncio.gather(
        vision_task,
        equipment_task
    )

    # 合并结果
    return {
        "vision_analysis": vision_result,
        "equipment_recommendation": equipment_result
    }
```

**性能优势**：
- 串行执行：2.5秒（视觉）+ 1.5秒（装备）= 4秒
- 并行执行：max(2.5秒, 1.5秒) = 2.5秒
- **提速**: 37.5%

---

## 六、核心功能实现

### 6.1 装备推荐（RAG + KG混合推理）

**推理流程**：

```
用户输入（灾害信息）
    ↓
【步骤1】RAG文档检索 (120ms)
    ├─ 查询Qdrant向量库
    ├─ BGE-M3嵌入
    ├─ 检索Top 5文档
    └─ 来源：GB/T 33743-2017等国标
    ↓
【步骤2】知识图谱推理 (80ms)
    ├─ Neo4j Cypher查询
    ├─ 灾害类型 → 受影响目标
    ├─ 目标 → 生产化学品
    ├─ 化学品 → 需要装备
    └─ 生成推理链
    ↓
【步骤3】历史案例验证 (150ms)
    ├─ 匹配相似案例（震级±1.0）
    ├─ 提取案例装备清单
    └─ 交叉验证
    ↓
【步骤4】人文因素补充 (50ms)
    ├─ 识别民族地区
    ├─ 查询文化数据库
    └─ 添加人文装备
    ↓
输出（装备清单 + 推理链）
```

**展示效果**：

前端React Flow可视化：

```
┌─────────────┐    120ms    ┌──────────────┐    80ms     ┌──────────────┐
│  RAG检索     │ ─────────→  │  KG推理       │ ─────────→ │  案例验证     │
│ 5个文档      │             │ 3层关系       │            │ 汶川经验      │
└─────────────┘             └──────────────┘            └──────────────┘
                                                                ↓ 150ms
                                                         ┌──────────────┐
                                                         │  人文补充     │
                                                         │  藏语翻译    │
                                                         └──────────────┘
                                                                ↓ 50ms
                                                         ┌──────────────┐
                                                         │  装备清单     │
                                                         │  (基础+特殊   │
                                                         │   +人文)      │
                                                         └──────────────┘
```

### 6.2 视觉分析（GLM-4V）

**输入**：无人机航拍图（1920×1080 JPEG）

**Prompt工程**：

```python
SYSTEM_PROMPT = """
你是应急救援专家，专门分析灾害现场航拍图像。

你的任务：
1. 准确计数人员和车辆（不要估计，要准确数）
2. 评估建筑物损毁程度（参考震害等级标准）
3. 判断道路通行状态（畅通/受阻/中断）
4. 识别潜在危险（火灾、泄漏、倒塌风险、塌方）
5. 给出危险等级（L0-L3）

危险等级定义：
- L0（正常）：无明显危险，可正常通行
- L1（低风险）：有轻微风险，需注意观察
- L2（高风险）：有明显危险，建议绕行
- L3（危急）：严重危险，立即停车避险

输出格式必须是JSON，不要有其他文字。
"""
```

**输出示例**：

```json
{
  "people_count": 5,
  "people_locations": [
    {"x": 0.3, "y": 0.4, "description": "站在路边"},
    {"x": 0.7, "y": 0.6, "description": "坐在地上"}
  ],
  "vehicles": [
    {"type": "卡车", "count": 2, "color": "白色"},
    {"type": "轿车", "count": 1, "color": "黑色"}
  ],
  "building_damage": "严重",
  "building_damage_details": "墙体开裂，部分坍塌，属于震害III级",
  "road_status": "中断",
  "road_obstacle": "大面积塌方，道路被碎石掩埋约50米",
  "dangers": [
    "次生滑坡风险高，降雨后可能再次坍塌",
    "塌方区域不稳定，禁止靠近"
  ],
  "danger_level": "L3",
  "confidence": 0.92,
  "analysis_time_ms": 2450
}
```

**前端展示**：

```jsx
<Card title="无人机视觉分析">
  <Image src={droneImage} />

  {/* 危险等级指示器 */}
  <DangerLevelBadge level={analysis.danger_level} />

  {/* 分析结果 */}
  <Descriptions>
    <Descriptions.Item label="人员">
      {analysis.people_count}人
    </Descriptions.Item>
    <Descriptions.Item label="车辆">
      {analysis.vehicles.map(v => `${v.count}辆${v.type}`).join('、')}
    </Descriptions.Item>
    <Descriptions.Item label="建筑损毁">
      {analysis.building_damage}
    </Descriptions.Item>
    <Descriptions.Item label="道路状态">
      {analysis.road_status}
    </Descriptions.Item>
  </Descriptions>

  {/* 危险警示 */}
  {analysis.dangers.map(danger => (
    <Alert type="error" message={danger} />
  ))}
</Card>
```

### 6.3 任务分配优化

**约束条件**：

```python
constraints = {
    # 1. 资质约束
    "skill_matching": {
        "破拆任务": ["消防员", "救援队"],
        "防化任务": ["防化兵", "消防员"],
        "医疗任务": ["医生", "护士"],
        "物资发放": ["志愿者", "任何人"]
    },

    # 2. 装备约束
    "equipment_required": {
        "破拆任务": ["破拆工具", "生命探测仪"],
        "防化任务": ["防化服", "气体检测仪"],
        "医疗任务": ["医疗包", "担架"]
    },

    # 3. 负载约束
    "max_tasks_per_rescuer": 3,

    # 4. 距离约束
    "max_distance_km": 10  # 任务点距离救援队不超过10km
}
```

**目标函数**：

```
minimize: 总距离 + 总耗时

subject to:
  - 每个任务分配给一个救援队
  - 救援队具备所需技能
  - 救援队具备所需装备
  - 每个救援队任务数 ≤ 3
  - 任务点距离 ≤ 10km
```

**展示效果**：

```
任务分配结果：

任务A：搜救被困人员（P0）
  分配给：消防队（20人）
  理由：
    ✓ 具备破拆技能
    ✓ 携带破拆工具、生命探测仪
    ✓ 距离任务点2.3km（最近）
  预计耗时：30分钟

任务B：化工厂安全检查（P0）
  分配给：武警防化队（15人）
  理由：
    ✓ 具备防化资质
    ✓ 携带防化服、气体检测仪
    ✓ 距离任务点4.1km
  预计耗时：45分钟

任务C：物资发放（P1）
  分配给：志愿者队（10人）
  理由：
    ✓ 无需特殊技能（协调任务）
    ✓ 距离任务点1.5km
  预计耗时：2小时

总距离：7.9km
总耗时：3小时15分钟（并行执行）
求解时间：85ms
```

---

## 七、炫酷效果设计

### 7.1 地震动画效果

**技术实现**：Mars3D + Cesium震动效果

```javascript
// src/services/map/effects/earthquake.js
export const playEarthquakeAnimation = (map, epicenter, magnitude) => {
  // 1. 地图震动效果
  const camera = map.camera;
  const originalPosition = camera.position.clone();

  const shakeDuration = 5000; // 5秒
  const shakeIntensity = magnitude * 0.1;

  const startTime = Date.now();
  const animate = () => {
    const elapsed = Date.now() - startTime;
    if (elapsed < shakeDuration) {
      // 随机偏移摄像机
      camera.position = originalPosition.add(new Cesium.Cartesian3(
        (Math.random() - 0.5) * shakeIntensity,
        (Math.random() - 0.5) * shakeIntensity,
        (Math.random() - 0.5) * shakeIntensity
      ));
      requestAnimationFrame(animate);
    } else {
      camera.position = originalPosition;
    }
  };
  animate();

  // 2. 波纹扩散效果
  map.addLayer({
    type: 'ripple',
    center: epicenter,
    maxRadius: 50000, // 50km
    speed: 500, // m/s
    color: Cesium.Color.RED.withAlpha(0.5),
    duration: 10000 // 10秒
  });

  // 3. 建筑物倒塌动画（预设的几个建筑）
  const buildings = map.getLayerById('buildings');
  buildings.entities.forEach(building => {
    if (isInDamageZone(building.position, epicenter, magnitude)) {
      animateCollapse(building);
    }
  });
};
```

### 7.2 无人机3D飞行动画

```javascript
// src/services/map/renderers/droneAnimation.js
export const flyDronePath = (map, path) => {
  // 加载无人机3D模型（GLTF）
  const drone = map.addModel({
    url: '/models/dji_phantom4.gltf',
    position: path[0],
    scale: 5,
    heading: 0,
    pitch: 0,
    roll: 0
  });

  // 设置飞行路径
  drone.path = {
    waypoints: path,
    speed: 20, // m/s
    altitude: 120 // m
  };

  // 摄像机跟随模式
  map.camera.followEntity(drone, {
    distance: 100,
    angle: -45, // 俯视角度
    heightOffset: 50
  });

  // 开始飞行动画
  drone.startAnimation({
    duration: path.length * 1000 / 20, // 根据速度计算
    loop: false,
    onUpdate: (progress) => {
      // 更新无人机姿态（俯仰、偏航）
      drone.pitch = -45; // 前倾
      drone.roll = calculateRoll(progress);
    },
    onComplete: () => {
      // 到达终点后悬停
      drone.hover();
      // 切换到下一场景
      switchToNextScene();
    }
  });
};
```

### 7.3 热力图叠加（危险区域）

```javascript
// src/services/map/layers/dangerHeatmap.js
import { HeatmapLayer } from 'mars3d-heatmap';

export const addDangerHeatmap = (map, dangerPoints) => {
  const heatLayer = new HeatmapLayer({
    positions: dangerPoints.map(p => [p.lon, p.lat]),
    weights: dangerPoints.map(p => getDangerWeight(p.level)),
    gradient: {
      0.0: 'rgba(0, 255, 0, 0)',      // L0透明
      0.25: 'rgba(255, 255, 0, 0.5)', // L1黄色
      0.6: 'rgba(255, 165, 0, 0.7)',  // L2橙色
      1.0: 'rgba(255, 0, 0, 1)'       // L3红色
    },
    radius: 50, // 50像素
    blur: 30,
    maxOpacity: 0.8,
    minOpacity: 0.3
  });

  map.addLayer(heatLayer);

  return heatLayer;
};

const getDangerWeight = (level) => {
  const weights = { 'L0': 0, 'L1': 0.3, 'L2': 0.7, 'L3': 1.0 };
  return weights[level] || 0;
};
```

### 7.4 AI推理链可视化（Sankey流程图）

```jsx
// src/components/ReasoningChainFlow.jsx
import ReactFlow, {
  Background,
  Controls,
  MiniMap,
  useNodesState,
  useEdgesState
} from 'reactflow';
import 'reactflow/dist/style.css';

const nodeTypes = {
  ragNode: RAGNode,
  kgNode: KGNode,
  llmNode: LLMNode
};

const ReasoningChainFlow = ({ chain }) => {
  // 构造节点
  const initialNodes = chain.map((step, idx) => ({
    id: `step-${idx}`,
    type: step.type === 'RAG检索' ? 'ragNode' :
          step.type === '知识图谱推理' ? 'kgNode' : 'llmNode',
    data: {
      label: step.type,
      source: step.source,
      result: step.result,
      duration: step.duration_ms,
      confidence: step.confidence
    },
    position: { x: idx * 350, y: 100 },
    style: {
      background: getNodeColor(step.type),
      color: 'white',
      border: '2px solid #1890ff',
      borderRadius: 10,
      padding: 15
    }
  }));

  // 构造边（动画效果）
  const initialEdges = chain.slice(0, -1).map((_, idx) => ({
    id: `edge-${idx}`,
    source: `step-${idx}`,
    target: `step-${idx + 1}`,
    animated: true,
    label: `${chain[idx + 1].duration_ms}ms`,
    labelStyle: { fill: '#1890ff', fontWeight: 600 },
    labelBgPadding: [8, 4],
    labelBgBorderRadius: 4,
    labelBgStyle: { fill: '#fff', stroke: '#1890ff' }
  }));

  const [nodes, setNodes, onNodesChange] = useNodesState(initialNodes);
  const [edges, setEdges, onEdgesChange] = useEdgesState(initialEdges);

  return (
    <div style={{ height: 400 }}>
      <ReactFlow
        nodes={nodes}
        edges={edges}
        onNodesChange={onNodesChange}
        onEdgesChange={onEdgesChange}
        nodeTypes={nodeTypes}
        fitView
      >
        <Background />
        <Controls />
        <MiniMap />
      </ReactFlow>
    </div>
  );
};

// 自定义节点组件
const RAGNode = ({ data }) => (
  <div className="rag-node">
    <strong>{data.label}</strong>
    <div>来源: {data.source}</div>
    <div>结果: {data.result}</div>
    <div>耗时: {data.duration}ms</div>
  </div>
);

const KGNode = ({ data }) => (
  <div className="kg-node">
    <strong>{data.label}</strong>
    <div>推理: {data.result}</div>
    <div>置信度: {(data.confidence * 100).toFixed(0)}%</div>
    <div>耗时: {data.duration}ms</div>
  </div>
);
```

### 7.5 实时数据流动画（粒子效果）

```jsx
// src/components/DataFlowParticles.jsx
import React, { useEffect, useRef } from 'react';
import * as THREE from 'three';

const DataFlowParticles = ({ from, to, data }) => {
  const canvasRef = useRef();

  useEffect(() => {
    const scene = new THREE.Scene();
    const camera = new THREE.PerspectiveCamera(75, 2, 0.1, 1000);
    const renderer = new THREE.WebGLRenderer({ canvas: canvasRef.current });

    // 创建粒子系统
    const particleCount = 100;
    const particles = new THREE.BufferGeometry();
    const positions = new Float32Array(particleCount * 3);

    for (let i = 0; i < particleCount; i++) {
      positions[i * 3] = from.x;
      positions[i * 3 + 1] = from.y;
      positions[i * 3 + 2] = from.z;
    }

    particles.setAttribute('position', new THREE.BufferAttribute(positions, 3));

    const material = new THREE.PointsMaterial({
      color: 0x00ff00,
      size: 3,
      transparent: true,
      opacity: 0.8
    });

    const particleSystem = new THREE.Points(particles, material);
    scene.add(particleSystem);

    // 动画循环
    const animate = () => {
      requestAnimationFrame(animate);

      // 更新粒子位置（从from流向to）
      const positions = particleSystem.geometry.attributes.position.array;
      for (let i = 0; i < particleCount; i++) {
        positions[i * 3] += (to.x - from.x) * 0.01;
        positions[i * 3 + 1] += (to.y - from.y) * 0.01;
        positions[i * 3 + 2] += (to.z - from.z) * 0.01;

        // 到达终点后重置
        if (Math.abs(positions[i * 3] - to.x) < 0.1) {
          positions[i * 3] = from.x;
          positions[i * 3 + 1] = from.y;
          positions[i * 3 + 2] = from.z;
        }
      }

      particleSystem.geometry.attributes.position.needsUpdate = true;
      renderer.render(scene, camera);
    };

    animate();

    return () => {
      renderer.dispose();
    };
  }, [from, to]);

  return <canvas ref={canvasRef} />;
};
```

### 7.6 AI语音播报（Text-to-Speech）

```javascript
// src/services/voice/tts.js
export const speak = (text, options = {}) => {
  if ('speechSynthesis' in window) {
    const utterance = new SpeechSynthesisUtterance(text);

    // 配置语音参数
    utterance.lang = options.lang || 'zh-CN';
    utterance.rate = options.rate || 1.2; // 播报速度（快20%）
    utterance.pitch = options.pitch || 1.0; // 音调
    utterance.volume = options.volume || 1.0; // 音量

    // 选择中文语音
    const voices = window.speechSynthesis.getVoices();
    const chineseVoice = voices.find(v => v.lang === 'zh-CN');
    if (chineseVoice) {
      utterance.voice = chineseVoice;
    }

    // 播报
    window.speechSynthesis.speak(utterance);
  }
};

// 使用示例
speak("检测到前方10公里发生大面积塌方，危险等级L3，建议立即停车并切换备用路线", {
  rate: 1.3, // 紧急情况，播报更快
  volume: 1.0
});
```

---

## 八、部署架构

### 8.1 双服务器部署拓扑

```
┌──────────────────────────────────────────────────────────────┐
│ 展会现场                                                      │
│ ├─ 大屏电脑 (浏览器访问 https://demo.emergency.com)           │
│ ├─ 演示笔记本 (操作人员)                                     │
│ └─ 平板 (模拟救援队APP)                                      │
└──────────────────┬───────────────────────────────────────────┘
                   │ HTTPS (443)
                   ↓
         ┌─────────────────────┐
         │ Nginx反向代理         │
         │ (服务器2)             │
         └─────────────────────┘
                   │
      ┌────────────┴────────────┐
      ↓                         ↓
┌───────────────┐         ┌───────────────┐
│ 服务器1        │←───────→│ 服务器2        │
│ (数据+GPU#1)   │  10Gbps │ (应用+GPU#2)   │
│               │  专线    │               │
│ GPU #1:       │         │ GPU #2:       │
│ GLM-4V-Plus   │         │ GLM-4-Plus    │
│               │         │               │
│ PostgreSQL    │         │ FastAPI       │
│ Neo4j         │         │ Web API       │
│ Qdrant        │         │ Adapter Hub   │
│ Redis         │         │ 前端          │
│ Kafka         │         │               │
└───────────────┘         └───────────────┘
```

### 8.2 服务器1详细配置

```yaml
服务器1 - 数据与视觉AI
━━━━━━━━━━━━━━━━━━━━━━━━━━

硬件：
  CPU: 32核 (AMD EPYC或Intel Xeon)
  RAM: 256GB DDR5
  GPU: NVIDIA H100 80GB (GPU #1)
  SSD: 2TB NVMe
  网卡: 10Gbps

GPU服务：
  ┌────────────────────────────────────────┐
  │ vLLM-Vision (Docker)                   │
  │ - 模型: GLM-4V-Plus                     │
  │ - VRAM: 35GB                           │
  │ - 端口: 8001                            │
  │ - 性能: 2-3秒/图                        │
  └────────────────────────────────────────┘

CPU服务：
  ┌────────────────────────────────────────┐
  │ PostgreSQL 17 (Docker)                 │
  │ - 端口: 5432                            │
  │ - 内存: 64GB                            │
  │ - 连接数: 200                           │
  │ - 数据: 业务表+LangGraph checkpoint     │
  └────────────────────────────────────────┘

  ┌────────────────────────────────────────┐
  │ Neo4j 5.0 (Docker)                     │
  │ - 端口: 7687                            │
  │ - 内存: 48GB                            │
  │ - 节点数: ~500                          │
  │ - 关系数: ~5000                         │
  └────────────────────────────────────────┘

  ┌────────────────────────────────────────┐
  │ Qdrant 1.7 (Docker)                    │
  │ - 端口: 6333                            │
  │ - 内存: 32GB                            │
  │ - 向量数: ~5000                         │
  │ - 维度: 1024 (BGE-M3)                  │
  └────────────────────────────────────────┘

  ┌────────────────────────────────────────┐
  │ Redis 7.2 (Docker)                     │
  │ - 端口: 6379                            │
  │ - 内存: 16GB                            │
  │ - 用途: 缓存+会话                       │
  └────────────────────────────────────────┘

  ┌────────────────────────────────────────┐
  │ Kafka 3.6 + Zookeeper (Docker)         │
  │ - 端口: 9092                            │
  │ - 主题: 5个                             │
  │ - 分区: 每主题3分区                      │
  └────────────────────────────────────────┘

Docker Compose配置：
  $ cat docker-compose-server1.yml
  version: '3.8'
  services:
    vllm-vision:
      image: vllm/vllm-openai:latest
      runtime: nvidia
      environment:
        - CUDA_VISIBLE_DEVICES=0
      ports:
        - "8001:8000"
      volumes:
        - /data/models:/models
      command: >
        --model /models/glm-4v-plus
        --gpu-memory-utilization 0.85
        --max-model-len 8192

    postgres:
      image: postgis/postgis:17-3.4
      ports:
        - "5432:5432"
      environment:
        POSTGRES_USER: rescue
        POSTGRES_PASSWORD: rescue_password
        POSTGRES_DB: rescue_system
      volumes:
        - postgres_data:/var/lib/postgresql/data

    neo4j:
      image: neo4j:5.0
      ports:
        - "7687:7687"
        - "7474:7474"
      environment:
        NEO4J_AUTH: neo4j/example-neo4j
      volumes:
        - neo4j_data:/data

    qdrant:
      image: qdrant/qdrant:v1.7.4
      ports:
        - "6333:6333"
      volumes:
        - qdrant_data:/qdrant/storage

    redis:
      image: redis:7.2-alpine
      ports:
        - "6379:6379"
      volumes:
        - redis_data:/data

    kafka:
      image: confluentinc/cp-kafka:7.5.3
      ports:
        - "9092:9092"
      environment:
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      depends_on:
        - zookeeper

    zookeeper:
      image: confluentinc/cp-zookeeper:7.5.3
      ports:
        - "2181:2181"
      environment:
        ZOOKEEPER_CLIENT_PORT: 2181

  volumes:
    postgres_data:
    neo4j_data:
    qdrant_data:
    redis_data:
```

### 8.3 服务器2详细配置

```yaml
服务器2 - 应用与文本AI
━━━━━━━━━━━━━━━━━━━━━━━━━━

硬件：
  CPU: 32核
  RAM: 256GB DDR5
  GPU: NVIDIA H100 80GB (GPU #2)
  SSD: 2TB NVMe
  网卡: 10Gbps

GPU服务：
  ┌────────────────────────────────────────┐
  │ vLLM-Text (Docker)                     │
  │ - 模型: GLM-4-Plus                      │
  │ - VRAM: 30GB                           │
  │ - 端口: 8002                            │
  │ - 性能: 500-800ms (512 tokens)         │
  └────────────────────────────────────────┘

CPU服务：
  ┌────────────────────────────────────────┐
  │ FastAPI (AI服务) (Docker)               │
  │ - 端口: 8008                            │
  │ - Workers: 8                           │
  │ - 语言: Python 3.10                     │
  │ - 框架: FastAPI + LangGraph            │
  └────────────────────────────────────────┘

  ┌────────────────────────────────────────┐
  │ Spring Boot (Web API) (Docker)         │
  │ - 端口: 28080                           │
  │ - Workers: 16                          │
  │ - 语言: Java 21                         │
  │ - 框架: Spring Boot 3.5.5              │
  └────────────────────────────────────────┘

  ┌────────────────────────────────────────┐
  │ Spring Boot (Adapter Hub) (Docker)     │
  │ - 端口: 8080                            │
  │ - Workers: 8                           │
  │ - 功能: 设备接入适配                    │
  └────────────────────────────────────────┘

  ┌────────────────────────────────────────┐
  │ Nginx (反向代理) (Docker)               │
  │ - 端口: 443 (HTTPS)                    │
  │ - 功能: SSL/TLS + 负载均衡             │
  └────────────────────────────────────────┘

  ┌────────────────────────────────────────┐
  │ React前端 (Nginx静态托管)               │
  │ - 端口: 80 (内部)                       │
  │ - 构建: Vite生产构建                    │
  └────────────────────────────────────────┘

Docker Compose配置：
  $ cat docker-compose-server2.yml
  version: '3.8'
  services:
    vllm-text:
      image: vllm/vllm-openai:latest
      runtime: nvidia
      environment:
        - CUDA_VISIBLE_DEVICES=0
      ports:
        - "8002:8000"
      volumes:
        - /data/models:/models
      command: >
        --model /models/glm-4-plus
        --gpu-memory-utilization 0.8
        --max-model-len 32768
        --enable-prefix-caching

    ai-service:
      build: ./emergency-agents-langgraph
      ports:
        - "8008:8008"
      environment:
        - VLLM_VISION_URL=http://server1:8001/v1
        - VLLM_TEXT_URL=http://localhost:8002/v1
        - POSTGRES_DSN=postgresql://rescue:rescue_password@server1:5432/rescue_system
        - NEO4J_URI=bolt://server1:7687
        - QDRANT_URL=http://server1:6333
      depends_on:
        - vllm-text

    web-api:
      build: ./emergency-web-api
      ports:
        - "28080:28080"
      environment:
        - AI_SERVICE_URL=http://localhost:8008
        - POSTGRES_URL=jdbc:postgresql://server1:5432/rescue_system
        - REDIS_URL=redis://server1:6379
        - KAFKA_BOOTSTRAP_SERVERS=server1:9092
      depends_on:
        - ai-service

    adapter-hub:
      build: ./emergency-adapter-hub
      ports:
        - "8080:8080"
      environment:
        - POSTGRES_URL=jdbc:postgresql://server1:5432/emergency
        - KAFKA_BOOTSTRAP_SERVERS=server1:9092

    nginx:
      image: nginx:1.25-alpine
      ports:
        - "443:443"
        - "80:80"
      volumes:
        - ./nginx.conf:/etc/nginx/nginx.conf
        - ./ssl:/etc/nginx/ssl
        - ./frontend/dist:/usr/share/nginx/html
      depends_on:
        - web-api
```

### 8.4 Nginx配置

```nginx
# nginx.conf
upstream web_api {
    server localhost:28080;
}

upstream ai_service {
    server localhost:8008;
}

server {
    listen 443 ssl http2;
    server_name demo.emergency.com;

    # SSL证书
    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;

    # 前端静态文件
    location / {
        root /usr/share/nginx/html;
        try_files $uri $uri/ /index.html;
    }

    # Web API代理
    location /api/ {
        proxy_pass http://web_api/api/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Request-ID $request_id;
    }

    # AI服务代理（可选，直接调用）
    location /ai/ {
        proxy_pass http://ai_service/api/v1/;
    }

    # WebSocket支持
    location /ws/ {
        proxy_pass http://web_api/ws/;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_read_timeout 600s;
    }
}
```

---

## 九、演示数据预置

### 9.1 地理数据

**余杭区地图瓦片**

```bash
# 下载余杭区高精度地图（OpenStreetMap）
wget -O yuhang.osm https://overpass-api.de/api/map?bbox=120.0,30.2,120.3,30.4

# 转换为Mars3D格式
python scripts/osm_to_mars3d.py yuhang.osm --output data/maps/yuhang/
```

**3D建筑物模型**

```javascript
// data/buildings/priority_targets.json
{
  "chemical_plants": [
    {
      "id": "cp001",
      "name": "余杭化工厂",
      "position": [120.15, 30.28, 50],
      "model": "/models/buildings/chemical_plant.gltf",
      "priority": "P0",
      "chemicals": ["氯气", "硫酸"],
      "capacity_tons": 500
    }
  ],
  "schools": [
    {
      "id": "sch001",
      "name": "余杭第一中学",
      "position": [120.16, 30.29, 30],
      "model": "/models/buildings/school.gltf",
      "priority": "P1",
      "students": 1200
    }
  ],
  "bridges": [
    {
      "id": "br001",
      "name": "余杭大桥",
      "position": [120.14, 30.27, 10],
      "model": "/models/buildings/bridge.gltf",
      "priority": "P2",
      "length_m": 800
    }
  ]
}
```

### 9.2 演示场景数据

```sql
-- 插入演示场景（余杭区6.5级地震）
INSERT INTO demo_scenario (
    scenario_id,
    scenario_name,
    disaster_type,
    epicenter,
    magnitude,
    preset_data
) VALUES (
    'scenario_yuhang_earthquake',
    '余杭区6.5级地震',
    'earthquake',
    ST_MakePoint(120.15, 30.28),
    6.5,
    '{
        "description": "2025年1月27日10:30，余杭区发生6.5级地震",
        "affected_area_km2": 50,
        "estimated_casualties": {
            "dead": 10,
            "injured": 50,
            "trapped": 20
        },
        "priority_targets": [
            {"id": "cp001", "type": "化工厂", "priority": "P0"},
            {"id": "sch001", "type": "学校", "priority": "P1"},
            {"id": "br001", "type": "桥梁", "priority": "P2"}
        ],
        "preset_images": [
            {"id": "img001", "url": "/demo/images/chemical_plant.jpg", "description": "化工厂完好"},
            {"id": "img002", "url": "/demo/images/landslide.jpg", "description": "道路塌方L3"},
            {"id": "img003", "url": "/demo/images/school_damage.jpg", "description": "学校轻微损毁"},
            {"id": "img004", "url": "/demo/images/bridge_crack.jpg", "description": "桥梁裂缝L2"},
            {"id": "img005", "url": "/demo/images/residential.jpg", "description": "居民区重度损毁"}
        ]
    }'
);
```

### 9.3 知识图谱数据

```cypher
// Neo4j数据初始化脚本
// 1. 创建灾害类型
CREATE (earthquake:DisasterType {
    name: "地震",
    characteristics: "突发性、破坏性强、次生灾害多",
    typical_casualties: "高"
});

// 2. 创建目标类型
CREATE (chemical_plant:TargetType {
    name: "化工厂",
    priority: "P0",
    typical_risks: ["化学泄漏", "爆炸", "中毒"]
});

// 3. 创建化学品
CREATE (chlorine:Chemical {
    name: "氯气",
    formula: "Cl2",
    toxicity: "剧毒",
    ld50: "293 ppm (1小时)",
    symptoms: ["呼吸困难", "肺水肿", "窒息"]
});

// 4. 创建装备
CREATE (hazmat_suit:Equipment {
    name: "防化服（A级）",
    category: "special",
    protection_level: "A",
    suitable_for: ["氯气", "氨气", "硫化氢"],
    max_exposure_time: "4小时",
    unit_price: 8000
});

// 5. 建立关系
MATCH (d:DisasterType {name: "地震"})
MATCH (t:TargetType {name: "化工厂"})
CREATE (d)-[:AFFECTS {probability: 0.8}]->(t);

MATCH (t:TargetType {name: "化工厂"})
MATCH (c:Chemical {name: "氯气"})
CREATE (t)-[:PRODUCES {quantity_tons: 500}]->(c);

MATCH (c:Chemical {name: "氯气"})
MATCH (e:Equipment {name: "防化服（A级）"})
CREATE (c)-[:REQUIRES {urgency: "critical"}]->(e);

// 6. 导入历史案例
CREATE (wenchuan:HistoricalCase {
    name: "汶川地震",
    date: "2008-05-12",
    magnitude: 8.0,
    location: "四川省汶川县",
    casualties: 69227,
    lessons: [
        "信息盲区严重，需要无人机侦查",
        "交通瓶颈致命，需要预置救援力量",
        "需要统一指挥平台"
    ]
});

CREATE (lesson1:Lesson {content: "信息盲区严重，需要无人机侦查"});
CREATE (wenchuan)-[:HAS_LESSON {importance: "high"}]->(lesson1);
```

### 9.4 RAG文档库

```python
# scripts/seed_qdrant.py
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
from sentence_transformers import SentenceTransformer

# 初始化
client = QdrantClient(url="http://server1:6333")
model = SentenceTransformer('BAAI/bge-m3')

# 创建collection
client.create_collection(
    collection_name="rescue_standards",
    vectors_config=VectorParams(
        size=1024,  # BGE-M3维度
        distance=Distance.COSINE
    )
)

# 准备文档
documents = [
    {
        "text": "地震救援基础装备包括：生命探测仪、破拆工具、担架、帐篷、发电机、照明设备、卫星电话。",
        "source": "GB/T 33743-2017 地震应急救援指南",
        "category": "装备标准"
    },
    {
        "text": "化工厂救援需要特殊防护装备：防化服（A级或B级）、正压呼吸器、气体检测仪、洗消设备。",
        "source": "GB 18218-2018 危险化学品重大危险源辨识",
        "category": "装备标准"
    },
    # ... 更多文档（共5000个）
]

# 向量化并插入
points = []
for idx, doc in enumerate(documents):
    embedding = model.encode(doc["text"])
    point = PointStruct(
        id=idx,
        vector=embedding.tolist(),
        payload={
            "text": doc["text"],
            "source": doc["source"],
            "category": doc["category"]
        }
    )
    points.append(point)

# 批量插入
client.upsert(
    collection_name="rescue_standards",
    points=points
)

print(f"已插入{len(points)}个文档向量")
```

### 9.5 模拟设备数据

```python
# scripts/simulate_devices.py
import requests
import time
from datetime import datetime

# 模拟无人机推送数据
def simulate_drone(drone_id, position):
    data = {
        "device_id": drone_id,
        "timestamp": int(datetime.now().timestamp() * 1000),
        "latitude": position[1],
        "longitude": position[0],
        "altitude": 120,
        "battery": 85,
        "status": "flying",
        "image_url": "/demo/images/chemical_plant.jpg"
    }

    response = requests.post(
        "http://localhost:8080/uplink/dji/phantom4",
        json=data
    )

    return response.json()

# 模拟车队移动
def simulate_convoy(convoy_id, path):
    for position in path:
        for vehicle in range(5):
            data = {
                "convoy_id": convoy_id,
                "vehicle_id": f"VEH-{vehicle:03d}",
                "position": position,
                "speed": 60.0,
                "fuel_percent": 85.0,
                "status": "normal"
            }

            # 推送到Kafka
            # ... 实现细节

        time.sleep(1)  # 每秒更新一次

# 运行模拟
if __name__ == "__main__":
    # 模拟5架无人机
    for i in range(5):
        simulate_drone(f"UAV-{i:03d}", [120.15, 30.28])

    # 模拟车队移动
    convoy_path = [
        [120.10, 30.25],
        [120.11, 30.26],
        [120.12, 30.27],
        # ... 路径点
    ]
    simulate_convoy("convoy-001", convoy_path)
```

---

## 十、风险与应对

### 10.1 风险矩阵

| 风险 | 概率 | 影响 | 等级 | 应对措施 |
|------|------|------|------|---------|
| AI推理延迟 | 高 | 中 | 🟡中 | 预计算 + 进度条 + 降低分辨率 |
| 网络中断 | 中 | 高 | 🟠高 | 有线网络 + 重连机制 |
| 异常输入导致崩溃 | 中 | 中 | 🟡中 | 输入校验 + 错误处理 |
| 数据不真实被质疑 | 低 | 高 | 🟠高 | 使用真实数据 + 来源标注 |
| 系统崩溃 | 低 | 高 | 🟠高 | 双机热备 + 自动重启 |
| 展会断电 | 低 | 高 | 🟠高 | UPS + 演示视频备份 |

### 10.2 详细应对方案

#### 风险1：AI推理延迟

**问题**：GLM-4V处理高分辨率图像可能需要5-10秒，专家等不及。

**应对措施**：
1. **预计算常见结果**
   ```python
   # 预先运行3个场景的所有推理
   precomputed_results = {
       "scenario_yuhang_earthquake": {
           "equipment_recommendation": {...},
           "route_plan": {...},
           "task_allocation": {...}
       }
   }

   # 用户点击时秒回
   if scenario_id in precomputed_results:
       return precomputed_results[scenario_id]["equipment_recommendation"]
   else:
       return await ai_service.recommend_equipment(...)
   ```

2. **增加推理中动画**
   ```jsx
   {loading && (
     <Spin tip="AI推理中...">
       <Progress
         type="circle"
         percent={progress}
         format={() => `${elapsed}ms`}
       />
       <div>正在分析图像...</div>
     </Spin>
   )}
   ```

3. **降低图像分辨率**
   ```python
   from PIL import Image

   def resize_image(image_path, max_size=1280):
       img = Image.open(image_path)
       if max(img.size) > max_size:
           img.thumbnail((max_size, max_size))
       return img
   ```

4. **批处理多张图**
   ```python
   # 同时分析多张图（利用vLLM批处理）
   results = await vision_analyzer.batch_analyze([
       "image1.jpg",
       "image2.jpg",
       "image3.jpg"
   ])
   ```

#### 风险2：网络中断

**问题**：展会现场WiFi不稳定，前后端失联。

**应对措施**：
1. **使用有线网络**
   - 所有服务器之间用10Gbps专线
   - 演示设备用网线连接交换机

2. **前端自动重连**
   ```javascript
   const connectWebSocket = () => {
     const ws = new WebSocket(WS_URL);

     ws.onclose = () => {
       console.log('连接断开，5秒后重连...');
       setTimeout(connectWebSocket, 5000);
     };

     ws.onerror = (error) => {
       console.error('连接错误:', error);
       ws.close();
     };
   };
   ```

3. **心跳检测**
   ```javascript
   setInterval(() => {
     if (ws.readyState === WebSocket.OPEN) {
       ws.send(JSON.stringify({ type: 'ping' }));
     }
   }, 30000); // 每30秒ping一次
   ```

#### 风险3：异常输入导致崩溃

**问题**：专家上传不相关图片（如猫狗照片），AI返回错误。

**应对措施**：
1. **前端限制上传**
   ```jsx
   <Upload
     accept="image/jpeg,image/png"
     beforeUpload={(file) => {
       // 只能从预设图库选择
       if (!PRESET_IMAGES.includes(file.name)) {
         message.error('请从预设图库选择图片');
         return false;
       }
       return true;
     }}
   >
     <Button>选择预设图片</Button>
   </Upload>
   ```

2. **后端输入校验**
   ```python
   from PIL import Image
   import clip

   async def validate_disaster_image(image_path: str) -> bool:
       # 使用CLIP模型判断图片是否与灾害相关
       image = preprocess(Image.open(image_path))
       text = clip.tokenize(["灾害现场", "建筑损毁", "道路中断"])

       with torch.no_grad():
           image_features = model.encode_image(image)
           text_features = model.encode_text(text)
           similarity = (image_features @ text_features.T).softmax(dim=-1)

       # 相似度 < 0.3 认为不相关
       if similarity.max() < 0.3:
           raise ValueError("图片与灾害场景不符，请上传灾害现场图片")

       return True
   ```

3. **优雅的错误处理**
   ```python
   try:
       result = await vision_analyzer.analyze_drone_image(image_path)
   except Exception as e:
       return {
           "error": str(e),
           "suggestion": "请上传清晰的灾害现场航拍图",
           "danger_level": "UNKNOWN"
       }
   ```

#### 风险4：数据不真实被质疑

**问题**：预置数据太假，专家质疑可信度。

**应对措施**：
1. **使用真实数据**
   - 余杭真实化工厂信息（从应急管理部数据库）
   - 真实的学校、医院、桥梁位置
   - 历史案例必须真实（汶川2008、雅安2013、九寨沟2017）

2. **数据来源标注**
   ```jsx
   <Descriptions>
     <Descriptions.Item label="数据来源">
       <Tag color="blue">中国地震局</Tag>
       <Tag color="green">应急管理部危化品登记系统</Tag>
     </Descriptions.Item>
     <Descriptions.Item label="更新时间">
       2025-01-27 10:30:00
     </Descriptions.Item>
   </Descriptions>
   ```

3. **推理过程透明化**
   - 展示完整的推理链
   - 标注每一步的数据来源
   - 显示置信度

#### 风险5：系统崩溃

**问题**：OOM、GPU崩溃、数据库连接池满。

**应对措施**：
1. **双机热备**
   ```yaml
   # 服务器1和服务器2互为备份
   # 如果服务器2的AI服务崩溃，自动切换到服务器1

   ai_service:
     primary: http://server2:8008
     backup: http://server1:8009
     health_check_interval: 30s
     failover_threshold: 3
   ```

2. **健康检查**
   ```python
   # 每30秒ping一次所有服务
   @app.get("/healthz")
   async def health_check():
       checks = {
           "vllm_vision": await ping_service("http://server1:8001/health"),
           "vllm_text": await ping_service("http://server2:8002/health"),
           "postgres": await ping_service("http://server1:5432"),
           "neo4j": await ping_service("http://server1:7687"),
           "qdrant": await ping_service("http://server1:6333")
       }

       if all(checks.values()):
           return {"status": "healthy", "checks": checks}
       else:
           return {"status": "unhealthy", "checks": checks}, 503
   ```

3. **自动重启脚本**
   ```bash
   #!/bin/bash
   # monitor.sh - 监控并自动重启崩溃的服务

   while true; do
       # 检查AI服务
       if ! curl -f http://localhost:8008/healthz > /dev/null 2>&1; then
           echo "AI服务崩溃，正在重启..."
           docker restart ai-service
           sleep 10
       fi

       # 检查Web API
       if ! curl -f http://localhost:28080/actuator/health > /dev/null 2>&1; then
           echo "Web API崩溃，正在重启..."
           docker restart web-api
           sleep 10
       fi

       sleep 30
   done
   ```

4. **提前演练**
   - 演示前3天进行完整演练
   - 演示前1天进行压力测试
   - 演示前1小时进行最终验证

#### 风险6：展会断电

**应对措施**：
1. **UPS不间断电源**
   ```
   服务器1 + 服务器2 + 交换机 → UPS (30分钟续航)
   ```

2. **快速保存状态**
   ```python
   import signal

   def handle_shutdown(signum, frame):
       print("检测到断电信号，正在保存状态...")

       # 保存当前演示状态
       save_demo_state({
           "current_scenario": current_scenario,
           "current_stage": current_stage,
           "user_inputs": user_inputs
       })

       # 停止所有服务
       sys.exit(0)

   signal.signal(signal.SIGTERM, handle_shutdown)
   ```

3. **演示视频备份**
   - 提前录制完整演示视频（15分钟）
   - 如果系统完全崩溃，直接播放视频
   - 视频存储在U盘（不依赖服务器）

---

## 十一、总结与下一步

### 11.1 方案总结

**核心定位**：
- 这是基于现有四层架构的**演示系统扩展**，而非独立新系统
- 复用80%现有代码，新增20%车载专用功能
- 目标：展会演示 + 专家验证，效果优先

**技术亮点**：
1. **双H100并行推理** - 视觉和文本同时处理，秒级响应
2. **AI推理链透明化** - 专家可见完整推理过程
3. **知识图谱防幻觉** - 基于真实数据和规则推理
4. **3D可视化** - 地震动画、无人机飞行、热力图
5. **完整业务闭环** - 出发前规划 → 行进中监控 → 现场指挥

**开发工作量**：
- 新增代码：约9100行（Java 3700行 + Python 1150行 + JavaScript 4250行）
- 开发时间：6周（1.5个月）
- 团队规模：4人（2后端 + 2前端）

**部署架构**：
- 服务器1：H100 GPU#1（视觉AI）+ 数据服务（PostgreSQL/Neo4j/Qdrant）
- 服务器2：H100 GPU#2（文本AI）+ 应用服务（FastAPI/Spring Boot/前端）
- 网络：10Gbps专线连接，有线网络避免WiFi不稳定

**演示剧本**（7-8分钟）：
- 场景1：灾情突发（30秒）
- 场景2：出发前规划（2分钟）
- 场景3：行进中监控（2分钟）
- 场景4：现场指挥（2.5分钟）

### 11.2 下一步行动

**Week 1-2：后端开发**
- [ ] Web API扩展（VehicleController + Service + DTO）
- [ ] AI服务扩展（装备推荐 + 视觉分析 + 任务优化）
- [ ] 数据库表创建和迁移
- [ ] AI服务HTTP客户端

**Week 3-5：前端开发**
- [ ] 新增三个页面（Planning + Monitoring + Commanding）
- [ ] 推理链可视化组件（React Flow）
- [ ] 地图效果增强（地震动画 + 无人机飞行）
- [ ] 危险预警UI组件

**Week 6：集成测试 + 数据预置**
- [ ] 部署双服务器环境
- [ ] 加载演示数据（地图 + 图像 + 知识图谱 + RAG文档）
- [ ] 端到端测试（完整演示流程）
- [ ] 性能优化（确保秒级响应）

**Week 7：演练与优化**
- [ ] 完整演示演练（3次）
- [ ] Bug修复
- [ ] 备份方案准备（演示视频录制）

**Week 8：展会准备**
- [ ] 现场设备调试（服务器 + 网络 + 显示器）
- [ ] 最终验证
- [ ] 应急预案演练

### 11.3 成功标准

✅ **技术指标**：
- AI推理延迟 < 3秒
- 系统连续运行 > 8小时不崩溃
- WebSocket推送延迟 < 100ms

✅ **业务指标**：
- 演示成功率 = 100%（无故障）
- 专家认可度 > 90%
- 观众驻足率 > 80%

✅ **展示效果**：
- 视觉震撼（3D动画流畅）
- 推理可信（数据真实，来源标注）
- 功能完整（三阶段闭环）

---

**文档结束**

> **下一步**: 开始Week 1-2的后端开发工作，优先实现AI服务的视觉分析和装备推荐功能。

