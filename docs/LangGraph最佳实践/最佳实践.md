# LangGraph 最佳实践（2025）

> 目标：为“AI 应急大脑”后续开发提供精准、可执行的工程指导。
> 依据：LangGraph 最新实践（exa 检索）、Mem0 Graph Memory 多租户规范（deepwiki），结合本项目场景。

---

## 1. 状态与数据（State First）

- 使用类型化状态（`TypedDict`/Pydantic/dataclass），字段最小化且语义清晰。
- 节点函数“纯函数化”：只返回需要变更的增量（partial state），不就地修改入参。
- 在节点边界做轻量校验（必需字段、有界范围），避免下游“幽灵异常”。
- 为循环/重试引入显式边界（如 `max_steps`/`error_count`）。

建议模板：
```python
from typing import TypedDict, Annotated, Optional
from langgraph.graph.message import add_messages

class RescueState(TypedDict, total=False):
	rescue_id: str
	user_id: str
	messages: Annotated[list, add_messages]
	status: str                 # init/awaiting_approval/running/completed/error
	error_count: int
	max_steps: int
	last_error: Optional[dict]
```

---

## 2. 流程与边（Edges）

- 线性步骤用普通边连接；确有分支时再用条件边（保持图简洁）。
- 循环要有“护栏”：最大重试次数、指数退避、无进展退出。
- 多智能体建议“Supervisor → Specialist”模式：小 supervisor 决定路由；各专长节点专注处理。

---

## 3. 持久化、线程与中断（Determinism）

- 生产必用 Postgres Checkpointer[^lg-persistence]：持久化状态、支持回放/检索、并发安全。
- `thread_id` 为一等公民[^lg-threadns]：`thread_id = rescue_id`，并加入命名空间：
  - `config.configurable.checkpoint_ns = f"tenant-{user_id}"`
  - `config.configurable.thread_id = f"rescue-{rescue_id}"`
- 人在回路（HITL）[^lg-interrupt]：在敏感节点使用 `interrupt` 暂停，携带审批 payload；恢复以同一 `thread_id` 继续。

---

## 4. 可靠性与降级（Resilience）

- 节点级错误→统一路由到 `error_handler`[^lg-error-routing]：有限重试（`MAX_RETRIES`）、回退（缓存/小模型/人工）。
- 所有有副作用节点必须幂等（恢复时不会重复执行）。
- 不可逆动作采用双轨日志（WAL → Commit），崩溃时可人工判定。

---

## 5. Streaming 与并行（UX / Throughput）

- 前端建议用 `stream_mode="updates"`[^lg-streaming] 推送状态增量（省带宽，响应敏捷）。
- 独立子任务用 Send API[^lg-send] 并行扇出，归并聚合（吞吐更好）。

---

## 6. 记忆与 RAG（一致性）

- Mem0 多租户[^mem0-multi]：`user_id` 必填；`agent_id`、`run_id(rescue_id)` 用于作用域与审计（deepwiki）。
- 统一嵌入：RAG 与 Mem0 使用同一 `EMBEDDING_MODEL/EMBEDDING_DIM`，索引阶段校验维度。
- Mem0 Graph Memory 开启[^mem0-graph]：新增/查询时默认按 `user_id/agent_id/run_id` 过滤；所有写入生成审计（hash + actor + rescue_id）。
- LlamaIndex：按“规范/案例/地理/装备”四域建索引→Qdrant；查询返回可引用片段（文本+来源）。

---

## 7. 可观测与SLO（Observability）

- API 暴露 `/metrics`（Prometheus）；接入 OTEL Trace（Jaeger/Tempo）。
- 关键 SLO：TTFT（P95<2s）、TBT（P95<0.1s/token）、错误率（<0.1%）、队列长度（P95<5）。
- vLLM 上线后追加其 `/metrics` 面板；开发期先监控 API/Qdrant/Neo4j。

---

## 8. 配置契约（契合多环境）

- 统一 OpenAI 兼容接口：`OPENAI_BASE_URL/OPENAI_API_KEY/LLM_MODEL`；本地/云只改配置，不改代码。
- 嵌入一致性：`EMBEDDING_MODEL/EMBEDDING_DIM` 为单一真值源（RAG/Mem0 共用）。
- 数据库：`POSTGRES_DSN/QDRANT_URL/NEO4J_URI`；默认本地 SQLite 仅用于单机开发。

---

## 9. 测试策略（Graph-level First）

- 图级测试：构造精简状态→invoke/ainvoke→断言选边与最终状态。
- 工具/LLM mock：隔离外部依赖，保证可重复性。
- 不变量检查：如“重试次数不超过上限、approved 与 rejected 不同时为真”。

---

## 10. 安全与合规

- 日志脱敏、最小权限（DB/向量库/图库）。
- 审计日志：所有写操作、不可逆动作、审批决策均留痕。
- 依赖安全扫描与版本锁定（pip-audit / lockfile / Dependabot）。

---

## 11. 供应商与适配

- 短期用智谱 OpenAI 兼容；避免依赖非兼容参数（严格 JSON 模式等）。
- 真需多供应商兼容时再引入适配层（如 LiteLLM），不要“一上来就加一层”。

---

## 12. 最小落地 Checklist（按优先级）

- [ ] 切 PostgresSaver；保留 SQLite 作为本地 fallback
- [ ] 审批/危险节点加 `interrupt`；同 `rescue_id` 恢复
- [ ] 统一 `error_handler`（重试→回退→人工），节点幂等
- [ ] API `/metrics` + OTEL trace；Grafana 看板
- [ ] Mem0 接入：add/search 强制过滤；图记忆→Neo4j；写入审计
- [ ] LlamaIndex→Qdrant 四域索引；检索返回可引用片段
- [ ] `EMBEDDING_MODEL/EMBEDDING_DIM` 单一真值源；索引时校验维度
- [ ] stream_mode="updates" 与 Send API 示例
- [ ] 图级测试（正常/错误/中断/回放）+ 外部依赖 mock

---

## 参考（工程方法）

- LangGraph（exa 检索）：
  - 类型化状态；纯函数节点；条件边与循环护栏
  - PostgresSaver；`thread_id` 与 `checkpoint_ns` 命名空间
  - `interrupt` HITL；error routing；Send API；streaming updates
- Mem0（deepwiki）：
  - 多租户过滤键：`user_id`（必）、`agent_id/run_id`（推荐）
  - Graph Memory：写入/查询均带过滤；节点/关系审计与时间戳

---

## 附：代码片段（可直接纳入项目）

1) PostgresSaver 接入：
```python
from langgraph.checkpoint.postgres import PostgresSaver
from psycopg_pool import ConnectionPool

pool = ConnectionPool(conninfo=POSTGRES_DSN, max_size=10)
with pool.connection() as conn:
	saver = PostgresSaver(conn)
	saver.setup()
	app = builder.compile(checkpointer=saver)
```

2) 中断节点示例：
```python
from langgraph.types import interrupt

def approval_node(state: RescueState):
	action = state.get("proposed_action", {})
	if action.get("risk_level") == "high":
		decision = interrupt({"action": action, "request": "approval"})
		return {"approved": decision.get("approved", False), "review_note": decision.get("note")}
	return {"approved": True}
```

3) 错误路由与重试：
```python
MAX_RETRIES = 2

def risky_node(state: RescueState):
	try:
		...
	except Exception as e:
		return {"status": "error", "last_error": {"detail": str(e)}, "error_count": state.get("error_count", 0) + 1}

def retry_or_fallback(state: RescueState) -> str:
	if state.get("error_count", 0) > MAX_RETRIES:
		return "fallback"
	return "risky_node"
```

---

## 引用

[^lg-persistence]: LangGraph docs — Persistence and Checkpointing: https://langchain-ai.github.io/langgraph/how-tos/persistence/
[^lg-threadns]: LangGraph docs — Threads and namespaces (thread_id, checkpoint_ns): https://langchain-ai.github.io/langgraph/how-tos/persistence/#threads-and-namespaces
[^lg-interrupt]: LangGraph docs — Human-in-the-loop with interrupts: https://langchain-ai.github.io/langgraph/how-tos/human-in-the-loop/
[^lg-error-routing]: LangGraph docs — Error handling and routing: https://langchain-ai.github.io/langgraph/how-tos/error-handling/
[^lg-streaming]: LangGraph docs — Streaming updates: https://langchain-ai.github.io/langgraph/how-tos/streaming/
[^lg-send]: LangGraph docs — Parallelism with Send API: https://langchain-ai.github.io/langgraph/how-tos/send/
[^mem0-multi]: Mem0 docs — Multi-tenancy keys (user_id, agent_id, run_id): https://docs.mem0.ai/
[^mem0-graph]: Mem0 docs — Graph Memory: https://docs.mem0.ai/graph-memory/
