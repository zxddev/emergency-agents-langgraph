# LangGraph + RAG æ·±åº¦é›†æˆæ¶æ„æ–¹æ¡ˆ

**åˆ†ææ—¶é—´**: 2025-10-27
**åˆ†ææ–¹æ³•**: LangGraphæ–‡æ¡£ç ”ç©¶ + ç°æœ‰ä»£ç å®¡æŸ¥ + æ¶æ„è®¾è®¡
**æ ¸å¿ƒé—®é¢˜**: å¦‚ä½•å°†å…ˆè¿›RAGæŠ€æœ¯ï¼ˆHybrid Search, ColBERT, Graph RAG, Self-RAGï¼‰æ­£ç¡®é›†æˆåˆ°LangGraphå·¥ä½œæµä¸­ï¼Ÿ

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

### æ ¸å¿ƒå‘ç°

**å½“å‰æ¶æ„ï¼ˆç®€å•ä¾èµ–æ³¨å…¥ï¼‰**:
```python
# src/emergency_agents/graph/app.py
def risk_prediction_node(state):
    return risk_predictor_agent(state, kg_service, rag_pipeline, llm_client, cfg.llm_model)
    #                                              ^^^^^^^^^^^^
    #                                         RAGä½œä¸ºå‚æ•°æ³¨å…¥
```

**é—®é¢˜åˆ†æ**:
1. âŒ RAGä¸LangGraphçŠ¶æ€æœºéš”ç¦»ï¼Œæ— æ³•åˆ©ç”¨checkpointing/interruptç‰¹æ€§
2. âŒ æ— æ³•å®ç°"æ£€ç´¢â†’è¯„ä¼°â†’é‡æ£€ç´¢"çš„Self-RAGå¾ªç¯
3. âŒ Graph RAGçš„KG+RAGèåˆé€»è¾‘éšè—åœ¨agentå†…éƒ¨ï¼Œä¸å¯è§‚æµ‹
4. âœ… ç®€å•ç›´æ¥ï¼Œæ˜“äºç†è§£ï¼ˆä½†å¯æ‰©å±•æ€§å·®ï¼‰

### æ¨èæ–¹æ¡ˆï¼ˆåˆ†å±‚æ¶æ„ï¼‰

**ä¸‰å±‚é›†æˆç­–ç•¥**ï¼ˆæ ¹æ®å¤æ‚åº¦é€’å¢ï¼‰:

| å±‚çº§ | æŠ€æœ¯ | é›†æˆæ–¹å¼ | ä¼˜å…ˆçº§ | å®æ–½æ—¶é—´ |
|------|------|---------|--------|---------|
| **L1: åº“å‡çº§** | Hybrid Search + ColBERT | å‡çº§`RagPipeline`å†…éƒ¨å®ç° | â­â­â­â­â­ | 3å¤© |
| **L2: èŠ‚ç‚¹åŒ–** | Self-RAG | æ–°å¢`self_rag_retrieve`æ¡ä»¶èŠ‚ç‚¹ | â­â­â­â­ | 5-7å¤© |
| **L3: å­å›¾** | Graph RAG | ç‹¬ç«‹`GraphRAGSubgraph`å­å›¾ | â­â­â­ | 8-10å¤© |

**æ ¸å¿ƒåŸåˆ™**:
1. **æ¸è¿›å¼æ”¹é€ **: ä¸ç ´åç°æœ‰å·¥ä½œæµ
2. **å¯è§‚æµ‹æ€§ä¼˜å…ˆ**: å…³é”®å†³ç­–ç‚¹æš´éœ²ä¸ºLangGraphèŠ‚ç‚¹
3. **ç±»å‹å®‰å…¨**: ä¿æŒPythonç±»å‹æ³¨è§£
4. **HITLå‹å¥½**: æ”¯æŒäººå·¥å®¡æ‰¹ä¸­æ–­

---

## ğŸ¯ L1å±‚ï¼šåº“å‡çº§ï¼ˆHybrid Search + ColBERTï¼‰

### ä¸ºä»€ä¹ˆä¸éœ€è¦æ”¹LangGraph?

**ç†ç”±**: Hybrid Searchå’ŒColBERTæ˜¯**çº¯æ£€ç´¢ä¼˜åŒ–**ï¼Œä¸æ¶‰åŠå·¥ä½œæµé€»è¾‘å˜åŒ–ã€‚

### å®æ–½æ–¹æ¡ˆ

#### å‡çº§å‰ï¼ˆç°çŠ¶ï¼‰
```python
# src/emergency_agents/rag/pipe.py
class RagPipeline:
    def query(self, question: str, domain: str, top_k: int = 3) -> List[RagChunk]:
        # âŒ åªæœ‰å¯†é›†å‘é‡æ£€ç´¢
        vector_store = self._vector_store(f"rag_{domain}")
        index = VectorStoreIndex.from_vector_store(vector_store)
        engine = index.as_query_engine(similarity_top_k=top_k)
        response = engine.query(question)
        return self._parse_response(response)
```

#### å‡çº§åï¼ˆHybrid + ColBERTï¼‰
```python
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.postprocessor.colbert_rerank import ColbertRerank

class EnhancedRagPipeline(RagPipeline):
    """å¢å¼ºå‹RAGç®¡é“ï¼šæ··åˆæ£€ç´¢ + ColBERTé‡æ’åº"""

    def __init__(self, *args, enable_hybrid: bool = True, enable_rerank: bool = True, **kwargs):
        super().__init__(*args, **kwargs)
        self.enable_hybrid = enable_hybrid
        self.enable_rerank = enable_rerank

        # ColBERTé‡æ’åºå™¨ï¼ˆä»…åœ¨éœ€è¦æ—¶åˆå§‹åŒ–ï¼‰
        if enable_rerank:
            self.reranker = ColbertRerank(
                model="colbert-ir/colbertv2.0",
                top_n=5,
                device="cuda"
            )

    def query(
        self,
        question: str,
        domain: str,
        top_k: int = 3,
        hybrid_alpha: float = 0.5  # 0=çº¯ç¨€ç–ï¼Œ1=çº¯å¯†é›†ï¼Œ0.5=å‡è¡¡
    ) -> List[RagChunk]:
        collection = f"rag_{domain}"
        vector_store = self._vector_store(collection)

        if self.enable_hybrid:
            # âœ… æ··åˆæ£€ç´¢ï¼ˆå¯†é›†+ç¨€ç–ï¼‰
            from llama_index.retrievers.bm25 import BM25Retriever
            from llama_index.core.retrievers import VectorIndexRetriever

            # å¯†é›†æ£€ç´¢å™¨
            dense_retriever = VectorIndexRetriever(
                index=VectorStoreIndex.from_vector_store(vector_store),
                similarity_top_k=top_k * 2  # ç²—æ’å¤šå–ä¸€äº›
            )

            # ç¨€ç–æ£€ç´¢å™¨ï¼ˆBM25ï¼‰
            sparse_retriever = BM25Retriever.from_defaults(
                docstore=vector_store.docstore,
                similarity_top_k=top_k * 2
            )

            # èåˆæ£€ç´¢å™¨
            retriever = QueryFusionRetriever(
                retrievers=[dense_retriever, sparse_retriever],
                similarity_top_k=top_k * 2,
                num_queries=1,  # ä¸åšæŸ¥è¯¢æ‰©å±•
                mode="reciprocal_rerank",  # å€’æ•°æ’åºèåˆ
                use_async=False
            )
        else:
            # ä¼ ç»Ÿå•ä¸€å‘é‡æ£€ç´¢
            index = VectorStoreIndex.from_vector_store(vector_store)
            retriever = index.as_retriever(similarity_top_k=top_k * 2)

        # ç²—æ’æ£€ç´¢
        nodes = retriever.retrieve(question)

        if self.enable_rerank and len(nodes) > top_k:
            # âœ… ColBERTé‡æ’åºï¼ˆç²¾æ’ï¼‰
            nodes = self.reranker.postprocess_nodes(nodes, query_str=question)

        # è½¬æ¢ä¸ºRagChunk
        return [
            RagChunk(
                text=node.get_content(),
                score=node.get_score(),
                metadata=node.metadata
            )
            for node in nodes[:top_k]
        ]
```

### LangGraphé›†æˆï¼ˆæ— éœ€ä¿®æ”¹ï¼‰

```python
# src/emergency_agents/graph/app.py
from emergency_agents.rag.enhanced_pipe import EnhancedRagPipeline

def build_app(...):
    # âœ… ç›´æ¥æ›¿æ¢ï¼ŒAPIå…¼å®¹
    rag_pipeline = EnhancedRagPipeline(
        qdrant_url=cfg.qdrant_url,
        enable_hybrid=True,      # å¯ç”¨æ··åˆæ£€ç´¢
        enable_rerank=True,      # å¯ç”¨ColBERT
        embedding_model=cfg.embedding_model,
        ...
    )

    # å…¶ä»–ä»£ç å®Œå…¨ä¸å˜
    def risk_prediction_node(state):
        return risk_predictor_agent(state, kg_service, rag_pipeline, llm_client, cfg.llm_model)

    graph.add_node("risk_prediction", risk_prediction_node)
```

### ä¼˜åŠ¿
- âœ… **é›¶ç ´åæ€§**: ç°æœ‰èŠ‚ç‚¹æ— éœ€ä¿®æ”¹
- âœ… **å¿«é€Ÿè§æ•ˆ**: 3å¤©å†…æ£€ç´¢ç²¾åº¦æå‡20-30%
- âœ… **å¯é…ç½®**: é€šè¿‡å‚æ•°æ§åˆ¶æ˜¯å¦å¯ç”¨
- âœ… **å‘åå…¼å®¹**: `EnhancedRagPipeline`ç»§æ‰¿è‡ª`RagPipeline`

---

## ğŸ¯ L2å±‚ï¼šèŠ‚ç‚¹åŒ–ï¼ˆSelf-RAGï¼‰

### ä¸ºä»€ä¹ˆéœ€è¦æ”¹LangGraph?

**ç†ç”±**: Self-RAGçš„æ ¸å¿ƒæ˜¯"æ£€ç´¢â†’è¯„ä¼°â†’å†³ç­–"å¾ªç¯ï¼Œè¿™æ˜¯**å·¥ä½œæµé€»è¾‘**ï¼Œå¿…é¡»ç”¨LangGraphæ¡ä»¶è¾¹è¡¨è¾¾ã€‚

### LangGraphæ ‡å‡†æ¨¡å¼

ä»LangGraphæ–‡æ¡£ä¸­å­¦åˆ°çš„RAGæœ€ä½³å®è·µï¼š

```python
# æ¥è‡ª langgraph/references/agents.md - Local RAG agent with LLaMA3
def router_node(state):
    """è·¯ç”±èŠ‚ç‚¹ï¼šå†³å®šä½¿ç”¨vectorstoreè¿˜æ˜¯web search"""
    router_instructions = """åˆ¤æ–­ç”¨æˆ·é—®é¢˜åº”è¯¥æŸ¥è¯¢å‘é‡åº“è¿˜æ˜¯ç½‘ç»œæœç´¢ã€‚
    è¿”å›JSON: {"datasource": "vectorstore" or "websearch"}"""

    decision = llm_json_mode.invoke([
        SystemMessage(content=router_instructions),
        HumanMessage(content=state["question"])
    ])
    return {"datasource": decision["datasource"]}

def route_question(state):
    """æ¡ä»¶è¾¹ï¼šæ ¹æ®router_nodeçš„å†³ç­–è·¯ç”±"""
    if state["datasource"] == "vectorstore":
        return "retrieve_from_rag"
    else:
        return "retrieve_from_web"

# æ„å»ºå›¾
graph.add_node("router", router_node)
graph.add_node("retrieve_from_rag", rag_retrieve_node)
graph.add_node("retrieve_from_web", web_search_node)
graph.add_conditional_edges("router", route_question, {
    "retrieve_from_rag": "retrieve_from_rag",
    "retrieve_from_web": "retrieve_from_web"
})
```

### Self-RAG LangGraphå®ç°

#### çŠ¶æ€æ‰©å±•

```python
# src/emergency_agents/graph/app.py
class RescueState(TypedDict, total=False):
    # ... ç°æœ‰å­—æ®µ ...

    # Self-RAGä¸“ç”¨å­—æ®µ
    self_rag_decision: Literal["retrieve", "generate", "skip"]
    self_rag_quality: Literal["good", "bad", "uncertain"]
    self_rag_attempt: int
    retrieved_contexts: list[dict]  # å­˜å‚¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
```

#### æ–°å¢èŠ‚ç‚¹

```python
def self_rag_router_node(state: RescueState) -> dict:
    """Self-RAGå†³ç­–èŠ‚ç‚¹ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢"""

    question = state.get("prompt", "")
    situation = state.get("situation", {})

    router_prompt = f"""ä½œä¸ºåº”æ€¥æ•‘æ´ä¸“å®¶ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢å†å²æ¡ˆä¾‹ï¼š

å½“å‰æ€åŠ¿ï¼š{json.dumps(situation, ensure_ascii=False)}
é—®é¢˜ï¼š{question}

å¦‚æœæ¶‰åŠå…·ä½“æ•‘æ´ç»éªŒã€è£…å¤‡é…ç½®ã€æ–¹æ¡ˆç”Ÿæˆï¼Œè¿”å› {{"decision": "retrieve"}}
å¦‚æœæ˜¯ç®€å•çš„è§„åˆ™æ¨ç†ã€æ•°å€¼è®¡ç®—ï¼Œè¿”å› {{"decision": "skip"}}

åªè¿”å›JSONã€‚"""

    response = llm_client.chat.completions.create(
        model=cfg.llm_model,
        messages=[{"role": "user", "content": router_prompt}],
        response_format={"type": "json_object"},
        temperature=0
    )

    decision = json.loads(response.choices[0].message.content)

    return {
        "self_rag_decision": decision.get("decision", "skip"),
        "self_rag_attempt": state.get("self_rag_attempt", 0) + 1
    }

def self_rag_retrieve_node(state: RescueState) -> dict:
    """Self-RAGæ£€ç´¢èŠ‚ç‚¹ï¼šä½¿ç”¨å¢å¼ºå‹RAGæ£€ç´¢"""

    question = state.get("prompt", "")

    # ä½¿ç”¨L1å±‚çš„EnhancedRagPipeline
    contexts = rag_pipeline.query(
        question=question,
        domain="æ¡ˆä¾‹",
        top_k=5,
        hybrid_alpha=0.5
    )

    return {
        "retrieved_contexts": [
            {"text": c.text, "score": c.score, "metadata": c.metadata}
            for c in contexts
        ],
        "rag_case_refs_count": len(contexts)
    }

def self_rag_evaluator_node(state: RescueState) -> dict:
    """Self-RAGè¯„ä¼°èŠ‚ç‚¹ï¼šè¯„ä¼°æ£€ç´¢è´¨é‡"""

    contexts = state.get("retrieved_contexts", [])
    question = state.get("prompt", "")

    if not contexts:
        return {"self_rag_quality": "bad", "self_rag_decision": "generate"}

    eval_prompt = f"""è¯„ä¼°æ£€ç´¢åˆ°çš„æ¡ˆä¾‹æ˜¯å¦èƒ½å›ç­”é—®é¢˜ï¼š

é—®é¢˜ï¼š{question}

æ¡ˆä¾‹ç‰‡æ®µï¼š
{chr(10).join([f"{i+1}. {c['text'][:200]}" for i, c in enumerate(contexts[:3])])}

å¦‚æœæ¡ˆä¾‹ç›¸å…³ä¸”åŒ…å«å¯ç”¨ä¿¡æ¯ï¼Œè¿”å› {{"quality": "good"}}
å¦‚æœæ¡ˆä¾‹ä¸ç›¸å…³æˆ–ä¿¡æ¯ä¸è¶³ï¼Œè¿”å› {{"quality": "bad"}}

åªè¿”å›JSONã€‚"""

    response = llm_client.chat.completions.create(
        model=cfg.llm_model,
        messages=[{"role": "user", "content": eval_prompt}],
        response_format={"type": "json_object"},
        temperature=0
    )

    evaluation = json.loads(response.choices[0].message.content)
    quality = evaluation.get("quality", "uncertain")

    # å¦‚æœè´¨é‡å·®ä¸”å°è¯•æ¬¡æ•°<2ï¼Œé‡æ–°æ£€ç´¢
    if quality == "bad" and state.get("self_rag_attempt", 0) < 2:
        return {
            "self_rag_quality": quality,
            "self_rag_decision": "retrieve"  # è§¦å‘é‡æ£€ç´¢
        }

    return {
        "self_rag_quality": quality,
        "self_rag_decision": "generate"
    }
```

#### æ¡ä»¶è¾¹

```python
def route_self_rag(state: RescueState) -> str:
    """Self-RAGè·¯ç”±å‡½æ•°"""
    decision = state.get("self_rag_decision", "skip")

    if decision == "retrieve":
        return "self_rag_retrieve"
    elif decision == "generate":
        return "risk_prediction"  # ç»§ç»­åŸæœ‰æµç¨‹
    else:
        return "risk_prediction"  # skipä¹Ÿç»§ç»­

# æ„å»ºå›¾ï¼ˆä¿®æ”¹éƒ¨åˆ†ï¼‰
graph.add_node("self_rag_router", self_rag_router_node)
graph.add_node("self_rag_retrieve", self_rag_retrieve_node)
graph.add_node("self_rag_evaluator", self_rag_evaluator_node)

# åŸæ¥: situation â†’ risk_prediction
# ç°åœ¨: situation â†’ self_rag_router â†’ (å¯é€‰)retrieve+eval â†’ risk_prediction
graph.add_edge("situation", "self_rag_router")
graph.add_conditional_edges("self_rag_router", route_self_rag, {
    "self_rag_retrieve": "self_rag_retrieve",
    "risk_prediction": "risk_prediction"
})
graph.add_edge("self_rag_retrieve", "self_rag_evaluator")
graph.add_conditional_edges("self_rag_evaluator", route_self_rag, {
    "retrieve": "self_rag_retrieve",  # é‡æ£€ç´¢å¾ªç¯
    "generate": "risk_prediction"
})
```

### ä¼˜åŠ¿
- âœ… **å¯è§‚æµ‹**: æ¯æ¬¡Self-RAGå†³ç­–éƒ½è®°å½•åœ¨checkpoint
- âœ… **å¯è°ƒè¯•**: LangSmithè¿½è¸ªæ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥è¾“å‡º
- âœ… **å¯ä¸­æ–­**: å¯åœ¨è¯„ä¼°èŠ‚ç‚¹åäººå·¥å®¡æ‰¹
- âœ… **é˜²æ­»å¾ªç¯**: `self_rag_attempt`é™åˆ¶æœ€å¤§é‡è¯•æ¬¡æ•°

### æ³¨æ„äº‹é¡¹
- âš ï¸ å¢åŠ äº†2-3æ¬¡LLMè°ƒç”¨ï¼ˆrouter + evaluatorï¼‰
- âš ï¸ å»¶è¿Ÿå¢åŠ 500-1000ms
- âš ï¸ åªåœ¨"æ–¹æ¡ˆç”Ÿæˆ"ç­‰å¤æ‚åœºæ™¯å¯ç”¨

---

## ğŸ¯ L3å±‚ï¼šå­å›¾ï¼ˆGraph RAGï¼‰

### ä¸ºä»€ä¹ˆéœ€è¦å­å›¾?

**ç†ç”±**: Graph RAGæ˜¯**å¤æ‚çš„å¤šæ­¥éª¤å·¥ä½œæµ**ï¼Œæ¶‰åŠï¼š
1. RAGæ£€ç´¢æ¡ˆä¾‹
2. LLMä»æ¡ˆä¾‹ä¸­æå–å®ä½“
3. Neo4jæŸ¥è¯¢å®ä½“ç›¸å…³èŠ‚ç‚¹
4. å›¾æ¨ç†ï¼ˆè·¯å¾„æŸ¥è¯¢ã€ç¤¾åŒºæ£€æµ‹ï¼‰
5. èåˆKG+RAGç»“æœ

è¿™æ˜¯ä¸€ä¸ª**å®Œæ•´çš„å­ä»»åŠ¡**ï¼Œåº”å°è£…ä¸ºç‹¬ç«‹å­å›¾ã€‚

### LangGraphå­å›¾æ¨¡å¼

ä»æ–‡æ¡£å­¦åˆ°çš„å­å›¾æœ€ä½³å®è·µï¼š

```python
# æ¥è‡ª langgraph/references/agents.md - Subgraphs
from langgraph.graph import StateGraph, START

# å­å›¾çŠ¶æ€ï¼ˆå¯ä»¥ä¸çˆ¶å›¾ä¸åŒï¼‰
class GraphRAGState(TypedDict):
    query: str
    rag_contexts: list[dict]
    kg_entities: list[dict]
    kg_paths: list[dict]
    fused_result: dict

# å®šä¹‰å­å›¾
def build_graph_rag_subgraph():
    subgraph = StateGraph(GraphRAGState)

    subgraph.add_node("rag_retrieve", rag_retrieve_node)
    subgraph.add_node("extract_entities", entity_extraction_node)
    subgraph.add_node("kg_query", kg_query_node)
    subgraph.add_node("graph_reasoning", graph_reasoning_node)
    subgraph.add_node("fusion", fusion_node)

    subgraph.add_edge(START, "rag_retrieve")
    subgraph.add_edge("rag_retrieve", "extract_entities")
    subgraph.add_edge("extract_entities", "kg_query")
    subgraph.add_edge("kg_query", "graph_reasoning")
    subgraph.add_edge("graph_reasoning", "fusion")

    return subgraph.compile()

# çˆ¶å›¾é›†æˆï¼ˆçŠ¶æ€è½¬æ¢å‡½æ•°ï¼‰
def graph_rag_adapter_node(state: RescueState) -> dict:
    """é€‚é…å™¨èŠ‚ç‚¹ï¼šè½¬æ¢çˆ¶å›¾çŠ¶æ€â†’å­å›¾çŠ¶æ€â†’çˆ¶å›¾çŠ¶æ€"""

    # 1. çˆ¶â†’å­çŠ¶æ€è½¬æ¢
    subgraph_input = GraphRAGState(
        query=state.get("prompt", ""),
        rag_contexts=[],
        kg_entities=[],
        kg_paths=[],
        fused_result={}
    )

    # 2. è°ƒç”¨å­å›¾
    graph_rag_subgraph = build_graph_rag_subgraph()
    result = graph_rag_subgraph.invoke(subgraph_input)

    # 3. å­â†’çˆ¶çŠ¶æ€è½¬æ¢
    return {
        "retrieved_contexts": result["rag_contexts"],
        "kg_hits_count": len(result["kg_entities"]),
        "rag_case_refs_count": len(result["rag_contexts"]),
        "graph_rag_result": result["fused_result"]
    }

# çˆ¶å›¾ä¸­ä½¿ç”¨
graph.add_node("graph_rag", graph_rag_adapter_node)
```

### Graph RAGå­å›¾è¯¦ç»†å®ç°

#### å­å›¾èŠ‚ç‚¹å®šä¹‰

```python
# src/emergency_agents/rag/graph_rag.py

from typing import TypedDict, List, Dict, Any
from langgraph.graph import StateGraph, START, END

class GraphRAGState(TypedDict, total=False):
    query: str
    disaster_type: str
    affected_area: str

    # é˜¶æ®µ1: RAGæ£€ç´¢
    rag_contexts: List[Dict[str, Any]]

    # é˜¶æ®µ2: å®ä½“æå–
    extracted_entities: List[Dict[str, str]]  # {type, name, alias}

    # é˜¶æ®µ3: KGæŸ¥è¯¢
    kg_nodes: List[Dict[str, Any]]
    kg_relationships: List[Dict[str, Any]]

    # é˜¶æ®µ4: å›¾æ¨ç†
    reasoning_paths: List[List[str]]  # [[node1, rel, node2, ...]]
    subgraph: Dict[str, Any]  # å±€éƒ¨å­å›¾ç»“æ„

    # é˜¶æ®µ5: èåˆ
    fused_result: Dict[str, Any]

def rag_retrieve_node_graphrag(state: GraphRAGState) -> dict:
    """é˜¶æ®µ1: ä½¿ç”¨å¢å¼ºå‹RAGæ£€ç´¢å†å²æ¡ˆä¾‹"""
    from emergency_agents.rag.enhanced_pipe import EnhancedRagPipeline

    rag_pipeline = EnhancedRagPipeline(...)  # ä»é…ç½®åˆå§‹åŒ–

    contexts = rag_pipeline.query(
        question=state["query"],
        domain="æ¡ˆä¾‹",
        top_k=5,
        hybrid_alpha=0.5
    )

    return {
        "rag_contexts": [
            {
                "text": c.text,
                "score": c.score,
                "metadata": c.metadata
            }
            for c in contexts
        ]
    }

def entity_extraction_node(state: GraphRAGState) -> dict:
    """é˜¶æ®µ2: ä»RAGæ¡ˆä¾‹ä¸­æå–å®ä½“"""
    from emergency_agents.llm.client import get_openai_client

    contexts_text = "\n\n".join([c["text"][:500] for c in state["rag_contexts"]])

    prompt = f"""ä»ä»¥ä¸‹åº”æ€¥æ•‘æ´æ¡ˆä¾‹ä¸­æå–å…³é”®å®ä½“ï¼š

æ¡ˆä¾‹ï¼š
{contexts_text}

æå–è§„åˆ™ï¼š
1. ç¾å®³ç±»å‹ï¼ˆåœ°éœ‡ã€æ´ªæ°´ã€æ»‘å¡ç­‰ï¼‰
2. åœ°ç‚¹ï¼ˆçœå¸‚å¿ã€åœ°æ ‡ï¼‰
3. è£…å¤‡ï¼ˆç”Ÿå‘½æ¢æµ‹ä»ªã€æŒ–æ˜æœºç­‰ï¼‰
4. å•ä½ï¼ˆæ¶ˆé˜²ã€æ­¦è­¦ã€åŒ»ç–—é˜Ÿç­‰ï¼‰

è¿”å›JSONæ ¼å¼ï¼š
{{
  "entities": [
    {{"type": "ç¾å®³", "name": "åœ°éœ‡", "alias": ["earthquake"]}},
    {{"type": "è£…å¤‡", "name": "ç”Ÿå‘½æ¢æµ‹ä»ª", "alias": []}}
  ]
}}

åªè¿”å›JSONã€‚"""

    llm = get_openai_client(...)
    response = llm.chat.completions.create(
        model="glm-4",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"},
        temperature=0
    )

    result = json.loads(response.choices[0].message.content)

    return {"extracted_entities": result.get("entities", [])}

def kg_query_node(state: GraphRAGState) -> dict:
    """é˜¶æ®µ3: åœ¨Neo4jä¸­æŸ¥è¯¢å®ä½“åŠå…¶å…³ç³»"""
    from emergency_agents.graph.kg_service import KGService

    kg_service = KGService(...)

    kg_nodes = []
    kg_relationships = []

    for entity in state["extracted_entities"]:
        # CypheræŸ¥è¯¢ï¼šæŸ¥æ‰¾å®ä½“èŠ‚ç‚¹
        query = """
        MATCH (n)
        WHERE n.name = $name OR $name IN n.aliases
        RETURN n
        LIMIT 5
        """
        nodes = kg_service.execute_cypher(query, name=entity["name"])
        kg_nodes.extend(nodes)

        # CypheræŸ¥è¯¢ï¼šæŸ¥æ‰¾èŠ‚ç‚¹çš„1-2è·³å…³ç³»
        if nodes:
            node_id = nodes[0]["id"]
            rel_query = """
            MATCH (n)-[r]->(m)
            WHERE id(n) = $node_id
            RETURN n, r, m
            LIMIT 10
            """
            rels = kg_service.execute_cypher(rel_query, node_id=node_id)
            kg_relationships.extend(rels)

    return {
        "kg_nodes": kg_nodes,
        "kg_relationships": kg_relationships
    }

def graph_reasoning_node(state: GraphRAGState) -> dict:
    """é˜¶æ®µ4: åœ¨KGå­å›¾ä¸Šæ‰§è¡Œæ¨ç†"""
    from emergency_agents.graph.kg_service import KGService

    kg_service = KGService(...)

    # 4.1 è·¯å¾„æŸ¥è¯¢ï¼šæ‰¾åˆ°å®ä½“é—´çš„æœ€çŸ­è·¯å¾„
    reasoning_paths = []
    entities = state["extracted_entities"]

    if len(entities) >= 2:
        for i in range(len(entities) - 1):
            source = entities[i]["name"]
            target = entities[i + 1]["name"]

            path_query = """
            MATCH path = shortestPath(
                (s {name: $source})-[*..3]-(t {name: $target})
            )
            RETURN [node in nodes(path) | node.name] as path
            LIMIT 1
            """
            paths = kg_service.execute_cypher(path_query, source=source, target=target)
            if paths:
                reasoning_paths.append(paths[0]["path"])

    # 4.2 æ„å»ºå±€éƒ¨å­å›¾ï¼ˆç”¨äºå¯è§†åŒ–å’Œè§£é‡Šï¼‰
    all_node_ids = [n["id"] for n in state["kg_nodes"]]
    subgraph_query = """
    MATCH (n)-[r]-(m)
    WHERE id(n) IN $node_ids AND id(m) IN $node_ids
    RETURN n, r, m
    """
    subgraph_data = kg_service.execute_cypher(subgraph_query, node_ids=all_node_ids)

    return {
        "reasoning_paths": reasoning_paths,
        "subgraph": {"nodes": state["kg_nodes"], "relationships": subgraph_data}
    }

def fusion_node(state: GraphRAGState) -> dict:
    """é˜¶æ®µ5: èåˆRAGæ¡ˆä¾‹ + KGæ¨ç†ç»“æœ"""
    from emergency_agents.llm.client import get_openai_client

    # 5.1 æ„é€ èåˆæç¤ºè¯
    rag_summary = f"æ£€ç´¢åˆ°{len(state['rag_contexts'])}ä¸ªç›¸ä¼¼æ¡ˆä¾‹"
    kg_summary = f"çŸ¥è¯†å›¾è°±åŒ…å«{len(state['kg_nodes'])}ä¸ªç›¸å…³å®ä½“"
    paths_summary = f"å‘ç°{len(state['reasoning_paths'])}æ¡æ¨ç†è·¯å¾„"

    fusion_prompt = f"""åŸºäºGraph RAGåˆ†æç»“æœç”Ÿæˆæ•‘æ´å»ºè®®ï¼š

## RAGå†å²æ¡ˆä¾‹
{chr(10).join([f"- {c['text'][:200]}" for c in state['rag_contexts'][:3]])}

## KGå›¾è°±æ¨ç†
å®ä½“ï¼š{', '.join([e['name'] for e in state['extracted_entities'][:5]])}
æ¨ç†è·¯å¾„ï¼š{state['reasoning_paths'][0] if state['reasoning_paths'] else 'æ— '}

## ç»¼åˆåˆ†æ
è¯·èåˆå†å²ç»éªŒï¼ˆRAGï¼‰å’Œè§„èŒƒçŸ¥è¯†ï¼ˆKGï¼‰ï¼Œç”Ÿæˆï¼š
1. å…³é”®é£é™©ç‚¹ï¼ˆæ¥è‡ªKGæ¨ç†ï¼‰
2. æ¨èæªæ–½ï¼ˆæ¥è‡ªå†å²æ¡ˆä¾‹ï¼‰
3. èµ„æºé…ç½®ï¼ˆæ¥è‡ªKGè£…å¤‡å…³ç³»ï¼‰

è¿”å›JSONï¼š
{{
  "risks": ["é£é™©1", "é£é™©2"],
  "recommendations": ["å»ºè®®1", "å»ºè®®2"],
  "resources": [{{"type": "è£…å¤‡", "name": "...", "source": "KG/RAG"}}]
}}

åªè¿”å›JSONã€‚"""

    llm = get_openai_client(...)
    response = llm.chat.completions.create(
        model="glm-4",
        messages=[{"role": "user", "content": fusion_prompt}],
        response_format={"type": "json_object"},
        temperature=0.3
    )

    fused_result = json.loads(response.choices[0].message.content)

    # 5.2 æ·»åŠ æº¯æºä¿¡æ¯
    fused_result["evidence"] = {
        "rag_cases": [c["text"][:100] for c in state["rag_contexts"]],
        "kg_entities": [e["name"] for e in state["extracted_entities"]],
        "reasoning_paths": state["reasoning_paths"]
    }

    return {"fused_result": fused_result}

# æ„å»ºå­å›¾
def build_graph_rag_subgraph(
    rag_pipeline,
    kg_service,
    llm_client,
    llm_model: str
) -> CompiledGraph:
    """æ„å»ºGraph RAGå­å›¾"""

    subgraph = StateGraph(GraphRAGState)

    # æ³¨å…¥ä¾èµ–åˆ°èŠ‚ç‚¹ï¼ˆé—­åŒ…æ–¹å¼ï¼‰
    def make_rag_node():
        def node(state):
            return rag_retrieve_node_graphrag(state)
        return node

    def make_entity_node():
        def node(state):
            # è¿™é‡Œå¯ä»¥è®¿é—®å¤–éƒ¨çš„llm_client
            return entity_extraction_node(state)
        return node

    # æ·»åŠ èŠ‚ç‚¹
    subgraph.add_node("rag_retrieve", make_rag_node())
    subgraph.add_node("extract_entities", make_entity_node())
    subgraph.add_node("kg_query", kg_query_node)
    subgraph.add_node("graph_reasoning", graph_reasoning_node)
    subgraph.add_node("fusion", fusion_node)

    # æ·»åŠ è¾¹ï¼ˆçº¿æ€§æµç¨‹ï¼‰
    subgraph.add_edge(START, "rag_retrieve")
    subgraph.add_edge("rag_retrieve", "extract_entities")
    subgraph.add_edge("extract_entities", "kg_query")
    subgraph.add_edge("kg_query", "graph_reasoning")
    subgraph.add_edge("graph_reasoning", "fusion")
    subgraph.add_edge("fusion", END)

    return subgraph.compile()
```

#### çˆ¶å›¾é›†æˆ

```python
# src/emergency_agents/graph/app.py

def build_app(...):
    # ... ç°æœ‰ä»£ç  ...

    # æ„å»ºGraph RAGå­å›¾
    graph_rag_subgraph = build_graph_rag_subgraph(
        rag_pipeline=rag_pipeline,
        kg_service=kg_service,
        llm_client=llm_client,
        llm_model=cfg.llm_model
    )

    def graph_rag_adapter_node(state: RescueState) -> dict:
        """é€‚é…å™¨èŠ‚ç‚¹ï¼šè°ƒç”¨Graph RAGå­å›¾"""

        # 1. çˆ¶å›¾çŠ¶æ€ â†’ å­å›¾çŠ¶æ€
        subgraph_input = GraphRAGState(
            query=state.get("prompt", "ç”Ÿæˆæ•‘æ´æ–¹æ¡ˆ"),
            disaster_type=state.get("situation", {}).get("disaster_type", "unknown"),
            affected_area=state.get("situation", {}).get("affected_area", ""),
            rag_contexts=[],
            extracted_entities=[],
            kg_nodes=[],
            kg_relationships=[],
            reasoning_paths=[],
            subgraph={},
            fused_result={}
        )

        # 2. è°ƒç”¨å­å›¾ï¼ˆåŒæ­¥æ‰§è¡Œï¼‰
        result = graph_rag_subgraph.invoke(subgraph_input)

        # 3. å­å›¾çŠ¶æ€ â†’ çˆ¶å›¾çŠ¶æ€
        return {
            "retrieved_contexts": result["rag_contexts"],
            "kg_hits_count": len(result["kg_nodes"]),
            "rag_case_refs_count": len(result["rag_contexts"]),
            "graph_rag_result": result["fused_result"],
            # å­˜å‚¨å®Œæ•´å­å›¾ç»“æœä¾›åç»­å®¡è®¡
            "graph_rag_subgraph_output": result
        }

    # æ·»åŠ åˆ°çˆ¶å›¾
    graph.add_node("graph_rag", graph_rag_adapter_node)

    # è·¯ç”±é€»è¾‘ï¼šå¤æ‚åœºæ™¯ç”¨Graph RAGï¼Œç®€å•åœºæ™¯è·³è¿‡
    def route_to_graph_rag(state: RescueState) -> str:
        intent = state.get("intent", {}).get("type", "unknown")

        # åªåœ¨"æ–¹æ¡ˆç”Ÿæˆ"åœºæ™¯ä½¿ç”¨Graph RAG
        if intent in ("rescue_task_generate", "plan_generation"):
            return "graph_rag"
        else:
            return "risk_prediction"  # ç®€å•åœºæ™¯ç›´æ¥èµ°åŸæµç¨‹

    # ä¿®æ”¹è¾¹
    # åŸæ¥: situation â†’ risk_prediction
    # ç°åœ¨: situation â†’ (æ¡ä»¶)graph_rag â†’ risk_prediction
    graph.add_conditional_edges("situation", route_to_graph_rag, {
        "graph_rag": "graph_rag",
        "risk_prediction": "risk_prediction"
    })
    graph.add_edge("graph_rag", "risk_prediction")
```

### ä¼˜åŠ¿
- âœ… **æ¨¡å—åŒ–**: Graph RAGå°è£…ä¸ºç‹¬ç«‹å­å›¾ï¼Œæ˜“äºæµ‹è¯•å’Œç»´æŠ¤
- âœ… **å¯å¤ç”¨**: å­å›¾å¯åœ¨å¤šä¸ªåœºæ™¯å¤ç”¨ï¼ˆé£é™©é¢„æµ‹ã€æ–¹æ¡ˆç”Ÿæˆï¼‰
- âœ… **å¯è§‚æµ‹**: å­å›¾çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰ç‹¬ç«‹çš„checkpoint
- âœ… **çµæ´»è·¯ç”±**: æ ¹æ®åœºæ™¯å¤æ‚åº¦å†³å®šæ˜¯å¦å¯ç”¨
- âœ… **æº¯æºå®Œæ•´**: `fused_result.evidence`åŒ…å«RAG+KGå®Œæ•´æº¯æºé“¾

### æ³¨æ„äº‹é¡¹
- âš ï¸ å¤æ‚åº¦æœ€é«˜ï¼š5ä¸ªèŠ‚ç‚¹ï¼Œ3-4æ¬¡LLMè°ƒç”¨
- âš ï¸ å»¶è¿Ÿå¢åŠ 2-3ç§’
- âš ï¸ åªåœ¨demoæ¼”ç¤ºçš„"æ ¸å¿ƒåœºæ™¯"å¯ç”¨

---

## ğŸ“Š ä¸‰å±‚æ¶æ„å¯¹æ¯”

| ç»´åº¦ | L1: åº“å‡çº§ | L2: èŠ‚ç‚¹åŒ– | L3: å­å›¾ |
|------|-----------|-----------|---------|
| **æŠ€æœ¯** | Hybrid + ColBERT | Self-RAG | Graph RAG |
| **LangGraphæ”¹åŠ¨** | âŒ æ—  | âš ï¸ +3èŠ‚ç‚¹+2æ¡ä»¶è¾¹ | âš ï¸ +å­å›¾+é€‚é…å™¨èŠ‚ç‚¹ |
| **ç±»å‹å®‰å…¨** | âœ… å®Œå…¨ä¿ç•™ | âœ… ä¿ç•™ | âš ï¸ éœ€è¦çŠ¶æ€è½¬æ¢ |
| **å¯è§‚æµ‹æ€§** | âš ï¸ åº“å†…éƒ¨ | âœ… é«˜ï¼ˆèŠ‚ç‚¹çº§ï¼‰ | âœ… æé«˜ï¼ˆå­å›¾çº§ï¼‰ |
| **å»¶è¿Ÿå¢åŠ ** | +50-100ms | +500-1000ms | +2000-3000ms |
| **LLMè°ƒç”¨å¢åŠ ** | 0æ¬¡ | +2æ¬¡ | +3-4æ¬¡ |
| **å®æ–½æ—¶é—´** | 3å¤© | 5-7å¤© | 8-10å¤© |
| **ROI** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| **é£é™©** | æä½ | ä½ | ä¸­ |

---

## ğŸš€ å®æ–½è·¯çº¿å›¾

### Phase 1: å¿«é€Ÿè§æ•ˆï¼ˆDay 1-3ï¼‰

**ç›®æ ‡**: ä¸æ”¹LangGraphï¼Œåªå‡çº§RAGæ£€ç´¢ç²¾åº¦

```bash
# 1. å®‰è£…ä¾èµ–
pip install ragatouille llama-index-postprocessor-colbert-rerank

# 2. å®ç°EnhancedRagPipeline
# src/emergency_agents/rag/enhanced_pipe.py

# 3. æ›¿æ¢build_appä¸­çš„RagPipelineåˆå§‹åŒ–
rag_pipeline = EnhancedRagPipeline(...)

# 4. æµ‹è¯•
pytest tests/test_enhanced_rag.py -v
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… ç°æœ‰æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼ˆé›¶ç ´åæ€§ï¼‰
- âœ… æ£€ç´¢ç²¾åº¦æå‡20%+ï¼ˆMRRæŒ‡æ ‡ï¼‰
- âœ… å»¶è¿Ÿ<200ms

### Phase 2: Self-RAGå¢å¼ºï¼ˆDay 4-10ï¼‰

**ç›®æ ‡**: æ·»åŠ Self-RAGèŠ‚ç‚¹ï¼Œå®ç°æ™ºèƒ½æ£€ç´¢å†³ç­–

```bash
# 1. æ‰©å±•RescueState
# src/emergency_agents/graph/app.py

# 2. å®ç°3ä¸ªæ–°èŠ‚ç‚¹
def self_rag_router_node(state): ...
def self_rag_retrieve_node(state): ...
def self_rag_evaluator_node(state): ...

# 3. æ·»åŠ æ¡ä»¶è¾¹
graph.add_conditional_edges("self_rag_router", route_self_rag, ...)

# 4. æµ‹è¯•
pytest tests/test_self_rag_integration.py -v
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… èƒ½æ­£ç¡®åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢ï¼ˆå‡†ç¡®ç‡>80%ï¼‰
- âœ… è¯„ä¼°å™¨èƒ½è¯†åˆ«ä½è´¨é‡æ£€ç´¢ç»“æœ
- âœ… é‡æ£€ç´¢å¾ªç¯ä¸è¶…è¿‡2æ¬¡

### Phase 3: Graph RAGå­å›¾ï¼ˆDay 11-20ï¼‰

**ç›®æ ‡**: å®ç°KG+RAGæ·±åº¦èåˆï¼Œæ¼”ç¤ºæŠ€æœ¯äº®ç‚¹

```bash
# 1. å®ç°å­å›¾
# src/emergency_agents/rag/graph_rag.py

# 2. å®ç°é€‚é…å™¨èŠ‚ç‚¹
def graph_rag_adapter_node(state): ...

# 3. æ·»åŠ æ¡ä»¶è·¯ç”±
graph.add_conditional_edges("situation", route_to_graph_rag, ...)

# 4. ç«¯åˆ°ç«¯æµ‹è¯•
pytest tests/test_graph_rag_e2e.py -v
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… èƒ½ä»RAGæ¡ˆä¾‹ä¸­æå–å®ä½“ï¼ˆå‡†ç¡®ç‡>70%ï¼‰
- âœ… èƒ½åœ¨Neo4jä¸­æ‰¾åˆ°å¯¹åº”èŠ‚ç‚¹ï¼ˆå¬å›ç‡>60%ï¼‰
- âœ… èåˆç»“æœåŒ…å«KG+RAGæº¯æºé“¾

---

## ğŸ’¡ æœ€ä½³å®è·µå»ºè®®

### 1. æ¸è¿›å¼éƒ¨ç½²

```python
# ä½¿ç”¨feature flagæ§åˆ¶åŠŸèƒ½å¯ç”¨
class RagFeatureFlags:
    ENABLE_HYBRID_SEARCH = os.getenv("RAG_HYBRID", "true") == "true"
    ENABLE_COLBERT = os.getenv("RAG_COLBERT", "true") == "true"
    ENABLE_SELF_RAG = os.getenv("RAG_SELF_RAG", "false") == "true"
    ENABLE_GRAPH_RAG = os.getenv("RAG_GRAPH_RAG", "false") == "true"

# åœ¨build_appä¸­ä½¿ç”¨
if RagFeatureFlags.ENABLE_SELF_RAG:
    graph.add_node("self_rag_router", ...)
else:
    # è·³è¿‡Self-RAGèŠ‚ç‚¹ï¼Œç›´æ¥è¿æ¥
    graph.add_edge("situation", "risk_prediction")
```

### 2. ç›‘æ§æŒ‡æ ‡

```python
# src/emergency_agents/audit/rag_metrics.py

def log_rag_metrics(
    rescue_id: str,
    agent_name: str,
    rag_type: Literal["basic", "hybrid", "self_rag", "graph_rag"],
    retrieval_time_ms: int,
    top_k: int,
    hit_count: int,
    rerank_enabled: bool
):
    """è®°å½•RAGæ£€ç´¢æŒ‡æ ‡åˆ°å®¡è®¡æ—¥å¿—"""
    metrics = {
        "timestamp": datetime.utcnow().isoformat(),
        "rescue_id": rescue_id,
        "agent": agent_name,
        "rag_type": rag_type,
        "latency_ms": retrieval_time_ms,
        "top_k": top_k,
        "hits": hit_count,
        "rerank": rerank_enabled
    }

    # å†™å…¥PostgreSQLæˆ–Prometheus
    logger.info(f"RAG_METRICS: {json.dumps(metrics)}")
```

### 3. A/Bæµ‹è¯•

```python
# åœ¨ä¸åŒç§Ÿæˆ·ä½¿ç”¨ä¸åŒRAGé…ç½®
def get_rag_config(user_id: str) -> dict:
    """æ ¹æ®ç”¨æˆ·IDè¿”å›RAGé…ç½®"""

    # 50%ç”¨æˆ·ä½¿ç”¨Graph RAGï¼Œ50%ä½¿ç”¨åŸºç¡€RAG
    if hash(user_id) % 2 == 0:
        return {
            "enable_graph_rag": True,
            "enable_self_rag": True,
            "enable_colbert": True
        }
    else:
        return {
            "enable_graph_rag": False,
            "enable_self_rag": False,
            "enable_colbert": False
        }
```

### 4. é”™è¯¯é™çº§

```python
def enhanced_rag_with_fallback(question: str, domain: str) -> list[RagChunk]:
    """å¸¦é™çº§ç­–ç•¥çš„RAGæ£€ç´¢"""

    try:
        # L3: å°è¯•Graph RAG
        if RagFeatureFlags.ENABLE_GRAPH_RAG:
            return graph_rag_retrieve(question, domain)
    except Exception as e:
        logger.warning(f"Graph RAG failed: {e}, fallback to Self-RAG")

    try:
        # L2: é™çº§åˆ°Self-RAG
        if RagFeatureFlags.ENABLE_SELF_RAG:
            return self_rag_retrieve(question, domain)
    except Exception as e:
        logger.warning(f"Self-RAG failed: {e}, fallback to Hybrid")

    try:
        # L1: é™çº§åˆ°Hybrid+ColBERT
        return enhanced_rag_pipeline.query(question, domain)
    except Exception as e:
        logger.error(f"Hybrid RAG failed: {e}, fallback to basic")

    # L0: æœ€åé™çº§åˆ°åŸºç¡€RAG
    return basic_rag_pipeline.query(question, domain)
```

---

## ğŸ¯ å…³é”®å†³ç­–æ€»ç»“

### é—®é¢˜1: æ˜¯å¦éœ€è¦å°†RAGå®ç°ä¸ºLangGraphèŠ‚ç‚¹ï¼Ÿ

**ç­”æ¡ˆ**: **åˆ†æƒ…å†µ**

- âŒ **Hybrid Search + ColBERT**: ä¸éœ€è¦ï¼Œåªæ˜¯æ£€ç´¢ä¼˜åŒ–
- âœ… **Self-RAG**: å¿…é¡»ï¼Œå› ä¸ºæ¶‰åŠ"æ£€ç´¢â†’è¯„ä¼°â†’å†³ç­–"å·¥ä½œæµ
- âœ… **Graph RAG**: å¿…é¡»ï¼Œå› ä¸ºæ˜¯å¤æ‚å¤šæ­¥éª¤å­ä»»åŠ¡

### é—®é¢˜2: çŠ¶æ€ç®¡ç†å¦‚ä½•è®¾è®¡ï¼Ÿ

**ç­”æ¡ˆ**: **æœ€å°åŒ–ä¾µå…¥**

```python
# âœ… æ¨èï¼šåªåœ¨éœ€è¦æ—¶æ·»åŠ å­—æ®µ
class RescueState(TypedDict, total=False):
    # ... ç°æœ‰å­—æ®µ ...

    # Self-RAGå­—æ®µï¼ˆåªåœ¨å¯ç”¨æ—¶ä½¿ç”¨ï¼‰
    self_rag_decision: str | None
    self_rag_quality: str | None

    # Graph RAGå­—æ®µï¼ˆåªåœ¨å¯ç”¨æ—¶ä½¿ç”¨ï¼‰
    graph_rag_result: dict | None
    graph_rag_subgraph_output: dict | None
```

### é—®é¢˜3: å¦‚ä½•ä¿æŒç±»å‹å®‰å…¨ï¼Ÿ

**ç­”æ¡ˆ**: **ä½¿ç”¨TypedDict + mypyéªŒè¯**

```python
# âœ… æ‰€æœ‰çŠ¶æ€å­—æ®µéƒ½æœ‰ç±»å‹æ³¨è§£
class GraphRAGState(TypedDict, total=False):
    query: str
    rag_contexts: List[Dict[str, Any]]
    extracted_entities: List[Dict[str, str]]
    # ...

# âœ… èŠ‚ç‚¹å‡½æ•°è¿”å›ç±»å‹æ˜ç¡®
def entity_extraction_node(state: GraphRAGState) -> dict:
    return {"extracted_entities": [...]}

# âœ… mypyæ£€æŸ¥
# mypy src/emergency_agents/graph/app.py --strict
```

### é—®é¢˜4: æ€§èƒ½å¼€é”€å¦‚ä½•æ§åˆ¶ï¼Ÿ

**ç­”æ¡ˆ**: **åˆ†å±‚å¯ç”¨ + ç¼“å­˜**

```python
# 1. åˆ†å±‚å¯ç”¨ï¼ˆæ ¹æ®åœºæ™¯å¤æ‚åº¦ï¼‰
if intent == "simple_query":
    use_basic_rag()  # å»¶è¿Ÿ<100ms
elif intent == "case_retrieval":
    use_hybrid_colbert()  # å»¶è¿Ÿ<200ms
elif intent == "plan_generation":
    use_graph_rag()  # å»¶è¿Ÿ<3000ms

# 2. ç¼“å­˜LLMè°ƒç”¨
@lru_cache(maxsize=100)
def entity_extraction_cached(text: str) -> list[dict]:
    return llm_extract_entities(text)
```

---

## âœ… éªŒæ”¶æ ‡å‡†

### L1: åº“å‡çº§éªŒæ”¶

- [ ] `EnhancedRagPipeline`é€šè¿‡æ‰€æœ‰ç°æœ‰å•å…ƒæµ‹è¯•
- [ ] Hybridæ£€ç´¢ç²¾åº¦ï¼ˆMRRï¼‰æå‡20%+
- [ ] ColBERTé‡æ’åºç²¾åº¦ï¼ˆMRRï¼‰å†æå‡10%+
- [ ] æ€»å»¶è¿Ÿ<200msï¼ˆp95ï¼‰

### L2: Self-RAGéªŒæ”¶

- [ ] `self_rag_router_node`å†³ç­–å‡†ç¡®ç‡>80%
- [ ] `self_rag_evaluator_node`èƒ½è¯†åˆ«ä½è´¨é‡ç»“æœ
- [ ] é‡æ£€ç´¢å¾ªç¯æ­£ç¡®ç»ˆæ­¢ï¼ˆæœ€å¤š2æ¬¡ï¼‰
- [ ] LangSmithè¿½è¸ªæ˜¾ç¤ºå®Œæ•´èŠ‚ç‚¹æµè½¬

### L3: Graph RAGéªŒæ”¶

- [ ] ä»RAGæ¡ˆä¾‹ä¸­æå–å®ä½“å‡†ç¡®ç‡>70%
- [ ] Neo4jå®ä½“åŒ¹é…å¬å›ç‡>60%
- [ ] èåˆç»“æœåŒ…å«KG+RAGå®Œæ•´æº¯æºé“¾
- [ ] å­å›¾å¯è§†åŒ–æ­£ç¡®å±•ç¤ºï¼ˆMermaidæ ¼å¼ï¼‰

---

## ğŸ“š å‚è€ƒèµ„æº

1. **LangGraphå®˜æ–¹æ–‡æ¡£**
   - Adaptive RAG: https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag_local/
   - Subgraphs: https://langchain-ai.github.io/langgraph/how-tos/subgraph/
   - Evaluator-Optimizer: https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#evaluator-optimizer

2. **RAGæŠ€æœ¯è®ºæ–‡**
   - Self-RAG: https://arxiv.org/abs/2310.11511
   - ColBERT: https://arxiv.org/abs/2004.12832
   - GraphRAG (Microsoft): https://arxiv.org/abs/2404.16130

3. **LlamaIndexæ–‡æ¡£**
   - Hybrid Retrieval: https://docs.llamaindex.ai/en/stable/examples/retrievers/bm25_retriever/
   - ColBERT Rerank: https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/ColbertRerank/

4. **é¡¹ç›®å†…éƒ¨æ–‡æ¡£**
   - åœºæ™¯åŒ–åˆ†æ: `docs/åˆ†ææŠ¥å‘Š/åº”æ€¥æ•‘ç¾åœºæ™¯åŒ–RAGæŠ€æœ¯é€‰å‹æ–¹æ¡ˆ.md`
   - æ··åˆæ£€ç´¢åˆ†æ: `docs/åˆ†ææŠ¥å‘Š/RAGæ¶æ„æ·±åº¦åˆ†æ-LlamaIndexæ··åˆæ£€ç´¢ç³»ç»Ÿ.md`

---

## ğŸ”„ åç»­æ¼”è¿›æ–¹å‘

### çŸ­æœŸï¼ˆ1-2ä¸ªæœˆï¼‰
- [ ] å®ç°HyDEï¼ˆå‡è®¾æ–‡æ¡£åµŒå…¥ï¼‰ç”¨äºé¢„æ¡ˆæœç´¢
- [ ] æ·»åŠ Query RewriteèŠ‚ç‚¹ä¼˜åŒ–æŸ¥è¯¢æ„å›¾
- [ ] å®ç°RAGç»“æœçš„Fact CheckingèŠ‚ç‚¹

### ä¸­æœŸï¼ˆ3-6ä¸ªæœˆï¼‰
- [ ] å¤šæ¨¡æ€Graph RAGï¼ˆèåˆå›¾åƒã€è§†é¢‘ï¼‰
- [ ] è”é‚¦å­¦ä¹ RAGï¼ˆå¤šåœ°å¸‚æ•°æ®èåˆï¼‰
- [ ] å®æ—¶RAGï¼ˆWebSocketæµå¼è¿”å›ï¼‰

### é•¿æœŸï¼ˆ6-12ä¸ªæœˆï¼‰
- [ ] Agent-RAGï¼ˆRAGç»“æœè§¦å‘Agentè¡ŒåŠ¨ï¼‰
- [ ] è‡ªé€‚åº”RAGï¼ˆæ ¹æ®ç”¨æˆ·åé¦ˆè‡ªåŠ¨ä¼˜åŒ–ï¼‰
- [ ] RAG-as-a-Serviceï¼ˆç‹¬ç«‹å¾®æœåŠ¡ï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-10-27
**ç»´æŠ¤è€…**: AIåº”æ€¥å¤§è„‘å›¢é˜Ÿ
