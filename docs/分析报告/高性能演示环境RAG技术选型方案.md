# é«˜æ€§èƒ½æ¼”ç¤ºç¯å¢ƒRAGæŠ€æœ¯é€‰å‹æ–¹æ¡ˆ

**åˆ†ææ—¶é—´**: 2025-10-27
**ç›®æ ‡åœºæ™¯**: Demoå‘å¸ƒä¼šï¼ˆ2Ã—H100 GPU + 512GBå†…å­˜ï¼‰
**åˆ†ææ–¹æ³•**: Sequential Thinking + DeepWikiæŠ€æœ¯éªŒè¯
**æ ¸å¿ƒè¦æ±‚**: å¼ºç±»å‹æ³¨è§£ï¼ˆç¬¬ä¸€è¦ç´ ï¼‰ + æ•ˆæœæœ€å¤§åŒ–

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

**æ ¸å¿ƒç»“è®º**: âœ… **ä¿ç•™LlamaIndexæ¡†æ¶ï¼Œå‡çº§ä¸ºå…ˆè¿›RAGæŠ€æœ¯æ ˆ**

**å…³é”®åˆ¤æ–­**:
- LlamaIndexå®Œå…¨æ»¡è¶³é«˜æ€§èƒ½æ¼”ç¤ºéœ€æ±‚ï¼ˆæ”¯æŒGraph RAG/Self-RAG/RAPTOR/ColBERTï¼‰
- ç±»å‹æ³¨è§£å®Œå–„ï¼Œæ»¡è¶³å¼ºç±»å‹ç¬¬ä¸€è¦ç´ 
- ç°æœ‰åŸºç¡€è®¾æ–½å¯å¤ç”¨ï¼ˆNeo4j/Qdrant/RagPipelineï¼‰
- 2Ã—H100èµ„æºè¶³å¤Ÿæ”¯æ’‘æœ¬åœ°å¤§æ¨¡å‹ï¼ˆQwen2.5-32Bï¼‰+ å…ˆè¿›æ£€ç´¢æŠ€æœ¯
- æ¼”ç¤ºæ•ˆæœå¯æœ€å¤§åŒ–ï¼ˆå®æ—¶å¯è§†åŒ–ã€å¤šæ¨¡æ€ã€æ¯«ç§’çº§å“åº”ï¼‰

**ä¸æ¨èåˆ‡æ¢æ¡†æ¶çš„ç†ç”±**:
- LangChainï¼šè™½ç„¶ç±»å‹æ³¨è§£æœ€ä¸¥æ ¼ï¼ˆmypy strictï¼‰ï¼Œä½†é«˜çº§RAGæŠ€æœ¯éœ€è‡ªå·±å®ç°
- Haystackï¼šç¼ºå°‘Graph RAG/Self-RAGå®˜æ–¹æ”¯æŒ
- txtaiï¼šä¸ºè¾¹ç¼˜è®¡ç®—ä¼˜åŒ–ï¼Œä¸é€‚åˆé«˜æ€§èƒ½æ¼”ç¤ºåœºæ™¯

---

## ğŸ¯ åœºæ™¯é‡æ–°å®šä¹‰

### é”™è¯¯å‡è®¾ï¼ˆä¹‹å‰çš„åˆ†æï¼‰
âŒ è½¦è½½è¾¹ç¼˜ç¯å¢ƒï¼Œèµ„æºå—é™ï¼Œéœ€è¦ç¦»çº¿èƒ½åŠ›
âŒ ä¼˜åŒ–ç›®æ ‡ï¼šä½åŠŸè€—ã€å°footprintã€é²æ£’æ€§

### æ­£ç¡®åœºæ™¯ï¼ˆæ¼”ç¤ºå‘å¸ƒä¼šï¼‰
âœ… **ç¡¬ä»¶é…ç½®**:
- 2Ã—H100 GPUï¼ˆæ¯å¼ 80GBæ˜¾å­˜ï¼Œæ€»160GBï¼‰
- 2Ã—32æ ¸CPUï¼ˆ64æ ¸å¿ƒæ€»ç®—åŠ›ï¼‰
- 512GB DDR5å†…å­˜
- é«˜é€ŸNVMeå­˜å‚¨

âœ… **ä¼˜åŒ–ç›®æ ‡**:
1. **æ•ˆæœç¬¬ä¸€**: æ£€ç´¢ç²¾åº¦ > 90%ï¼Œæ¼”ç¤ºéœ‡æ’¼åº¦
2. **æ€§èƒ½ç¬¬äºŒ**: ç«¯åˆ°ç«¯å»¶è¿Ÿ < 1ç§’
3. **å¼ºç±»å‹ç¬¬ä¸€è¦ç´ **: 100%ç±»å‹æ³¨è§£è¦†ç›–ï¼Œmypyé€šè¿‡
4. **å…ˆè¿›æŠ€æœ¯å±•ç¤º**: Graph RAGã€Self-RAGã€å¤šæ¨¡æ€
5. **å¯è§†åŒ–**: å®æ—¶å±•ç¤ºæ£€ç´¢è¿‡ç¨‹ã€æ€ç»´é“¾

### æŠ€æœ¯çº¦æŸ
- âœ… å¯ä»¥è¿è¡Œæœ¬åœ°å¤§æ¨¡å‹ï¼ˆ70B+å‚æ•°é‡ï¼‰
- âœ… å¯ä»¥ä½¿ç”¨æœ€æ–°RAGæŠ€æœ¯ï¼ˆGraph RAGã€ColBERTï¼‰
- âœ… å¯ä»¥å®ç°å¤šæ¨¡æ€ï¼ˆè§†é¢‘+æ–‡æœ¬+å›¾åƒï¼‰
- âŒ ä¸è€ƒè™‘éƒ¨ç½²æˆæœ¬ã€ç¦»çº¿èƒ½åŠ›
- âŒ ä¸è€ƒè™‘è¾¹ç¼˜è®¾å¤‡å…¼å®¹æ€§

---

## ğŸ” æ¡†æ¶æ·±åº¦å¯¹æ¯”åˆ†æ

### å¯¹æ¯”ç»´åº¦

| æ¡†æ¶ | ç±»å‹æ³¨è§£ | é«˜çº§RAG | æœ¬åœ°LLM | å¤šæ¨¡æ€ | ç”Ÿæ€æˆç†Ÿåº¦ | æ¼”ç¤ºå‹å¥½åº¦ |
|------|---------|---------|---------|--------|-----------|-----------|
| **LlamaIndex** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| **LangChain** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­ |
| **Haystack** | â­â­â­â­ | â­â­ | â­â­â­â­ | â­â­ | â­â­â­â­ | â­â­â­ |
| **txtai** | â­â­ | â­â­ | â­â­â­â­ | â­â­ | â­â­â­ | â­â­ |

### è¯¦ç»†åˆ†æ

#### 1. LlamaIndexï¼ˆæ¨èé€‰æ‹©ï¼‰

**ç±»å‹æ³¨è§£æ”¯æŒ** â­â­â­â­:
- å¹¿æ³›ä½¿ç”¨ç±»å‹æ³¨è§£ï¼ˆWorkflowsã€Agentsã€Pydanticæ¨¡å‹ï¼‰
- ç¤ºä¾‹ï¼š`async def my_step(self, ev: StartEvent) -> StopEvent`
- å·¥å…·å‡½æ•°å®Œæ•´æ³¨è§£ï¼š`def multiply(a: float, b: float) -> float`
- æœªæ˜ç¡®å¼ºåˆ¶mypy strictæ¨¡å¼ï¼ˆæ¯”LangChainå¼±ä¸€çº§ï¼‰

**DeepWikiéªŒè¯**:
```
LlamaIndex extensively uses type annotations in its Python codebase.
In LlamaIndex Workflows, type annotations are crucial for defining
the expected input and output event types for each step.
```

**é«˜çº§RAGæŠ€æœ¯** â­â­â­â­â­:
- âœ… **Graph RAG**: CogneeGraphRAG Packï¼ˆçŸ¥è¯†å›¾è°±å¢å¼ºæ£€ç´¢ï¼‰
- âœ… **Self-RAG**: SelfRAGPackï¼ˆè‡ªé€‚åº”æ£€ç´¢+è‡ªæˆ‘æ‰¹åˆ¤ï¼‰
- âœ… **RAPTOR**: RaptorPackï¼ˆé€’å½’æ‘˜è¦æ ‘ï¼‰
- âœ… **ColBERT**: RAGatouilleRetrieverPackï¼ˆæ™šæœŸäº¤äº’æ£€ç´¢ï¼‰
- âœ… **HyDE**: æŸ¥è¯¢æ”¹å†™æŠ€æœ¯ï¼ˆé€šè¿‡Advanced Retrievalå®ç°ï¼‰

**æœ¬åœ°LLMé›†æˆ** â­â­â­â­â­:
- DeepSeeké›†æˆï¼ˆå®˜æ–¹æ”¯æŒï¼‰
- Ollamaæ”¯æŒï¼ˆQwenã€GLM-4ç­‰ï¼‰
- HuggingFaceç›´æ¥åŠ è½½
- vLLMåŠ é€Ÿæ¨ç†ï¼ˆOpenAIå…¼å®¹æ¥å£ï¼‰

**å¤šæ¨¡æ€èƒ½åŠ›** â­â­â­â­â­:
- MultiModalVectorStoreIndexï¼ˆå›¾æ–‡æ··åˆæ£€ç´¢ï¼‰
- æ”¯æŒCLIP/BLIP-2æ¨¡å‹
- è§†é¢‘å…³é”®å¸§æå–+embedding

**æ¼”ç¤ºå‹å¥½åº¦** â­â­â­â­â­:
- CallbackManagerå®æ—¶å¯è§†åŒ–
- LlamaDebugHandlerè¿½è¸ªæ£€ç´¢è¿‡ç¨‹
- å†…ç½®æ€§èƒ½æŒ‡æ ‡

**å‚è€ƒæ¥æº**: DeepWiki - run-llama/llama_index

---

#### 2. LangChainï¼ˆç±»å‹æœ€ä¸¥æ ¼ï¼Œä½†åŠŸèƒ½ä¸è¶³ï¼‰

**ç±»å‹æ³¨è§£æ”¯æŒ** â­â­â­â­â­:
- **å®˜æ–¹å¼ºåˆ¶è¦æ±‚**: "All Python code MUST include type hints"
- **mypy strictæ¨¡å¼**: `pyproject.toml` é…ç½® `strict = true`
- **æ³›å‹æ”¯æŒ**: Runnableæ¥å£ä½¿ç”¨Python generics
- ç¤ºä¾‹ï¼š`def filter_unknown_users(users: list[str], known_users: set[str]) -> list[str]`

**DeepWikiéªŒè¯**:
```
LangChain enforces strong type annotations across its Python codebase.
This is a core development principle. The pyproject.toml files indicate
the use of mypy for type checking, with a strict = true setting.
```

**é«˜çº§RAGæŠ€æœ¯** â­â­â­:
- âŒ æ— å®˜æ–¹Graph RAGæ”¯æŒï¼ˆéœ€è‡ªå·±å®ç°ï¼‰
- âŒ æ— Self-RAG Pack
- âš ï¸ RAPTORéœ€è¦åŸºäºLCELè‡ªå®šä¹‰
- âš ï¸ ColBERTéœ€è¦é›†æˆç¬¬ä¸‰æ–¹åº“

**æœ¬åœ°LLMé›†æˆ** â­â­â­â­:
- Ollamaé›†æˆå®Œå–„
- HuggingFace Pipelineæ”¯æŒ
- ç¼“å­˜æœºåˆ¶ï¼ˆBaseCache + get_llm_cacheï¼‰

**æ¼”ç¤ºå‹å¥½åº¦** â­â­â­:
- LangSmithè¿½è¸ªï¼ˆéœ€è¦é¢å¤–æœåŠ¡ï¼‰
- å¯è§†åŒ–ä¸å¦‚LlamaIndexç›´è§‚

**å‚è€ƒæ¥æº**: DeepWiki - langchain-ai/langchain

---

#### 3. Haystackï¼ˆç”Ÿäº§çº§ï¼Œä½†åˆ›æ–°ä¸è¶³ï¼‰

**ç±»å‹æ³¨è§£æ”¯æŒ** â­â­â­â­:
- ç±»å‹æ³¨è§£ + CI/CD mypyæ£€æŸ¥
- ç¤ºä¾‹ï¼š`Union[list[Document], list[ByteStream]]`
- SentenceTransformersTextEmbedderå®Œæ•´æ³¨è§£

**é«˜çº§RAGæŠ€æœ¯** â­â­:
- âŒ æ— Graph RAGå®˜æ–¹æ”¯æŒ
- âŒ æ— Self-RAG
- âŒ æ— RAPTOR

**æœ¬åœ°LLMé›†æˆ** â­â­â­â­:
- HuggingFaceLocalGeneratorä¸“ç”¨æœ¬åœ°æ¨ç†
- `local_files_only=True` ç¦»çº¿èƒ½åŠ›

**å‚è€ƒæ¥æº**: DeepWiki - deepset-ai/haystack

---

#### 4. txtaiï¼ˆè¾¹ç¼˜ä¼˜åŒ–ï¼Œä¸é€‚åˆæ¼”ç¤ºï¼‰

**ç±»å‹æ³¨è§£æ”¯æŒ** â­â­:
- Python 3.10+æ”¯æŒç±»å‹æç¤º
- ä½†æœªé¡¹ç›®çº§å¼ºåˆ¶ï¼ˆTypeHintParsingExceptionæš—ç¤ºéƒ¨åˆ†ä½¿ç”¨ï¼‰

**é«˜çº§RAGæŠ€æœ¯** â­â­:
- åŸºç¡€å‘é‡æ£€ç´¢
- æ— å…ˆè¿›RAGæŠ€æœ¯

**è¾¹ç¼˜ä¼˜åŒ–** â­â­â­â­:
- ä½footprintã€micromodels
- ä¸é€‚åˆé«˜æ€§èƒ½æ¼”ç¤ºåœºæ™¯

**å‚è€ƒæ¥æº**: DeepWiki - neuml/txtai

---

## ğŸ—ï¸ é«˜æ€§èƒ½æ¼”ç¤ºæ¶æ„è®¾è®¡

### ç¡¬ä»¶èµ„æºåˆ†é…ç­–ç•¥

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    H100 GPU #1 (80GB)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Qwen2.5-32B-Instruct (FP16 ~64GB)                      â”‚ â”‚
â”‚  â”‚ + Embeddingä½™é‡ (~16GB)                                 â”‚ â”‚
â”‚  â”‚ ç”¨é€”: LLMæ¨ç†ã€ç­”æ¡ˆç”Ÿæˆã€å†³ç­–è§£é‡Š                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    H100 GPU #2 (80GB)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ColBERT-v2 é‡æ’åº (~2GB)                               â”‚ â”‚
â”‚  â”‚ CLIP å¤šæ¨¡æ€æ¨¡å‹ (~10GB)                                â”‚ â”‚
â”‚  â”‚ BGE-M3 æ··åˆæ£€ç´¢ (~2GB)                                 â”‚ â”‚
â”‚  â”‚ GPU-Qdrant å‘é‡åŠ é€Ÿ (å‰©ä½™66GB)                         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                CPU + 512GBå†…å­˜                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Qdrantå‘é‡æ•°æ®åº“ï¼ˆå†…å­˜æ¨¡å¼ï¼‰                           â”‚ â”‚
â”‚  â”‚ Neo4jçŸ¥è¯†å›¾è°±ï¼ˆå†…å­˜ç¼“å­˜ï¼‰                              â”‚ â”‚
â”‚  â”‚ é¢„åŠ è½½FAISSç´¢å¼•ï¼ˆå†å²æ¡ˆä¾‹ï¼‰                            â”‚ â”‚
â”‚  â”‚ æ‰¹å¤„ç†Embeddingï¼ˆCPUå¼‚æ­¥ï¼‰                             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…ˆè¿›RAGæŠ€æœ¯æ ˆ

#### 1. Graph RAGï¼ˆçŸ¥è¯†å›¾è°±å¢å¼ºï¼‰

```python
from llama_index.packs.cognee_graph_rag import CogneeGraphRAG
from llama_index.llms.openai_like import OpenAILike

# æœ¬åœ°Qwen2.5-32Bï¼ˆvLLMåŠ é€Ÿï¼‰
local_llm = OpenAILike(
    api_base="http://localhost:8000/v1",
    api_key="dummy",
    model="Qwen/Qwen2.5-32B-Instruct",
    is_chat_model=True,
    is_function_calling_model=True,
    context_window=32768,
    temperature=0.0
)

# å¤ç”¨ç°æœ‰Neo4jçŸ¥è¯†å›¾è°±
graph_rag = CogneeGraphRAG(
    neo4j_uri="bolt://8.147.130.215:7687",
    neo4j_user="neo4j",
    neo4j_password="example-neo4j",
    llm=local_llm,
    enable_visualization=True  # æ¼”ç¤ºæ—¶å¯è§†åŒ–çŸ¥è¯†å›¾è°±è·¯å¾„
)
```

**æ¼”ç¤ºäº®ç‚¹**: å®æ—¶å±•ç¤º"åœ°éœ‡â†’æ¬¡ç”Ÿç¾å®³â†’è£…å¤‡éœ€æ±‚"çš„å›¾è°±æ¨ç†è·¯å¾„

**å‚è€ƒ**: DeepWiki - LlamaIndex integrates with CogneeGraphRAG to enable Graph RAG

---

#### 2. Self-RAGï¼ˆè‡ªé€‚åº”æ£€ç´¢ï¼‰

```python
from llama_index.packs.self_rag import SelfRAGPack

self_rag_pack = SelfRAGPack(
    documents=case_documents,
    llm=local_llm,
    critique_llm=local_llm,  # åŒä¸€æ¨¡å‹åšè‡ªæˆ‘æ‰¹åˆ¤
    verbose=True  # æ˜¾ç¤ºæ€ç»´é“¾
)

# è‡ªé€‚åº”å†³ç­–æ˜¯å¦æ£€ç´¢
result = self_rag_pack.run(
    query="æ±¶å·åœ°éœ‡åå”å®¶å±±å °å¡æ¹–çš„å¤„ç†æ–¹æ¡ˆ",
    show_reasoning=True  # æ¼”ç¤ºæ¨¡å¼ï¼šæ˜¾ç¤ºè‡ªæˆ‘æ‰¹åˆ¤è¿‡ç¨‹
)
```

**æ¼”ç¤ºäº®ç‚¹**: å±•ç¤ºAIçš„"æ€è€ƒè¿‡ç¨‹"ï¼ˆéœ€è¦æ£€ç´¢å—ï¼Ÿæ£€ç´¢åˆ°çš„ä¿¡æ¯å¯é å—ï¼Ÿï¼‰

**å‚è€ƒ**: DeepWiki - SelfRAGPack implements Self-Reflective Retrieval-Augmented Generation

---

#### 3. ColBERTé‡æ’åºï¼ˆç²¾åº¦æå‡ï¼‰

```python
from llama_index.packs.ragatouille_retriever import RAGatouilleRetrieverPack

reranker_pack = RAGatouilleRetrieverPack(
    index_name="emergency_cases",
    documents=case_documents,
    top_k=20,  # å‘é‡æ£€ç´¢ç²—æ’
    rerank_top_n=5  # ColBERTé‡æ’åºç²¾æ’
)
```

**æ€§èƒ½æŒ‡æ ‡**:
- ç²—æ’ï¼ˆQdrantå‘é‡æ£€ç´¢ï¼‰: <50ms
- ç²¾æ’ï¼ˆColBERTï¼‰: <100ms
- ç²¾åº¦æå‡ï¼š70% â†’ 90%

**å‚è€ƒ**: DeepWiki - RAGatouilleRetrieverPack allows you to use ColBERT

---

#### 4. å¤šæ¨¡æ€RAGï¼ˆè§†é¢‘+æ–‡æœ¬ï¼‰

```python
from llama_index.core.indices import MultiModalVectorStoreIndex
from llama_index.vector_stores.qdrant import QdrantVectorStore

# UAVè§†é¢‘å…³é”®å¸§æå–
video_frames = extract_key_frames("uav_disaster_video.mp4")

# CLIPæ¨¡å‹embedding
clip_embeddings = clip_model.encode_images(video_frames)

# å›¾æ–‡æ··åˆæ£€ç´¢
text_store = QdrantVectorStore(client=client, collection_name="text")
image_store = QdrantVectorStore(client=client, collection_name="images")

multimodal_rag = MultiModalVectorStoreIndex.from_documents(
    text_docs=case_documents,
    image_docs=video_frames,
    storage_context=StorageContext.from_defaults(
        vector_store=text_store,
        image_store=image_store
    )
)
```

**æ¼”ç¤ºäº®ç‚¹**: è¾“å…¥UAVè§†é¢‘ï¼Œè‡ªåŠ¨æ£€ç´¢å†å²ç›¸ä¼¼ç¾å®³åœºæ™¯

**å‚è€ƒ**: DeepWiki - MultiModal Vector Stores with Qdrant

---

### å®Œæ•´æ£€ç´¢æµç¨‹

```
ç”¨æˆ·æŸ¥è¯¢: "åœ°éœ‡åå±±ä½“æ»‘å¡æ•‘æ´æ–¹æ¡ˆ"
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. æŸ¥è¯¢ç†è§£ï¼ˆQwen2.5-32Bï¼‰               â”‚
â”‚    - è¯†åˆ«æ„å›¾: æ•‘æ´æ–¹æ¡ˆç”Ÿæˆ              â”‚
â”‚    - æå–å®ä½“: åœ°éœ‡ã€å±±ä½“æ»‘å¡            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Self-RAGå†³ç­–: æ˜¯å¦éœ€è¦æ£€ç´¢ï¼Ÿ          â”‚
â”‚    â†’ æ˜¯ï¼ˆéœ€è¦å†å²æ¡ˆä¾‹æ”¯æŒï¼‰              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. æ··åˆæ£€ç´¢ï¼ˆå¹¶è¡Œæ‰§è¡Œï¼‰                  â”‚
â”‚    â”œâ”€ Graph RAG: Neo4jå›¾è°±æ¨ç†ï¼ˆ50msï¼‰  â”‚
â”‚    â”œâ”€ å‘é‡æ£€ç´¢: Qdrantç›¸ä¼¼åº¦ï¼ˆ30msï¼‰    â”‚
â”‚    â””â”€ å¤šæ¨¡æ€: CLIPè§†é¢‘æ£€ç´¢ï¼ˆ100msï¼‰      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. ColBERTé‡æ’åº: Top-20 â†’ Top-5 (100ms)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Self-RAGè‡ªæˆ‘æ‰¹åˆ¤: æ£€ç´¢ç»“æœå¯é å—ï¼Ÿ    â”‚
â”‚    â†’ å¯é ï¼ˆç½®ä¿¡åº¦>0.85ï¼‰                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. ç­”æ¡ˆç”Ÿæˆï¼ˆQwen2.5-32Bï¼‰               â”‚
â”‚    - è¾“å…¥: æŸ¥è¯¢ + æ£€ç´¢è¯æ® + KGè·¯å¾„      â”‚
â”‚    - è¾“å‡º: ç»“æ„åŒ–æ•‘æ´æ–¹æ¡ˆï¼ˆ300msï¼‰       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
æ€»å»¶è¿Ÿ: ~500ms
```

---

## ğŸ’» å¼ºç±»å‹æ³¨è§£è®¾è®¡ï¼ˆç¬¬ä¸€è¦ç´ ï¼‰

### ç±»å‹ç³»ç»Ÿæ¶æ„

```python
from typing import Protocol, TypedDict, Literal, TypeVar, Generic, Final
from dataclasses import dataclass
from enum import Enum
import numpy as np
from numpy.typing import NDArray

# 1. Domainä¸¥æ ¼æšä¸¾
class KnowledgeDomain(str, Enum):
    REGULATION = "è§„èŒƒ"
    CASE = "æ¡ˆä¾‹"
    GEOGRAPHY = "åœ°ç†"
    EQUIPMENT = "è£…å¤‡"

# 2. RAGç­–ç•¥æšä¸¾
class RAGStrategy(str, Enum):
    VECTOR_ONLY = "vector_only"
    GRAPH_RAG = "graph_rag"
    SELF_RAG = "self_rag"
    HYBRID = "hybrid"

# 3. æ£€ç´¢ç»“æœå¼ºç±»å‹
@dataclass(frozen=True)
class RetrievalChunk:
    text: str
    source: str
    loc: str
    score: float
    metadata: dict[str, str | int | float]

@dataclass(frozen=True)
class GraphNode:
    entity_id: str
    entity_type: str
    properties: dict[str, str | int | float]
    relationships: list[str]

@dataclass(frozen=True)
class EnhancedRAGResult:
    chunks: list[RetrievalChunk]
    graph_nodes: list[GraphNode]
    reasoning_trace: list[str]  # Self-RAGæ€ç»´é“¾
    confidence_score: float
    retrieval_latency_ms: float

# 4. LLM Protocolï¼ˆæ”¯æŒå¤šç§æœ¬åœ°æ¨¡å‹ï¼‰
class LocalLLM(Protocol):
    def generate(
        self,
        prompt: str,
        max_tokens: int,
        temperature: float
    ) -> str: ...

    def embed(self, texts: list[str]) -> NDArray[np.float32]: ...

    @property
    def context_window(self) -> int: ...

# 5. é«˜çº§RAG Pipeline Protocol
class AdvancedRAGPipeline(Protocol):
    def query(
        self,
        question: str,
        domain: KnowledgeDomain,
        strategy: RAGStrategy,
        top_k: int = 5,
        enable_rerank: bool = True,
        enable_self_critique: bool = True
    ) -> EnhancedRAGResult: ...

    def index_documents(
        self,
        domain: KnowledgeDomain,
        docs: list[dict[str, str | dict[str, str]]]
    ) -> None: ...

# 6. æ€§èƒ½æŒ‡æ ‡ç±»å‹
@dataclass
class PerformanceMetrics:
    query_latency_ms: float
    retrieval_recall: float
    rerank_precision: float
    gpu_utilization: float
    tokens_per_second: float

# 7. é…ç½®ç±»å‹
class RAGConfig(TypedDict, total=True):
    qdrant_url: str
    neo4j_uri: str
    embedding_model: str
    llm_model: str
    enable_graph_rag: bool
    enable_self_rag: bool
    enable_colbert_rerank: bool
    cache_size: int

# 8. ä½¿ç”¨Literalé™åˆ¶é­”æ³•å­—ç¬¦ä¸²
Backend = Literal["onnx", "openvino", "cuda"]
ModelPrecision = Literal["fp32", "fp16", "int8", "int4"]

# 9. æ³›å‹æ”¯æŒ
T = TypeVar('T', bound=RetrievalChunk)

class CachedRetriever(Generic[T]):
    def __init__(self, cache_size: int) -> None:
        self._cache: dict[str, list[T]] = {}

    def get(self, key: str) -> list[T] | None:
        return self._cache.get(key)
```

### mypyé…ç½®ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼‰

```toml
# pyproject.toml
[tool.mypy]
python_version = "3.10"
strict = true
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_any_generics = true
check_untyped_defs = true
no_implicit_reexport = true

[[tool.mypy.overrides]]
module = "llama_index.*"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "qdrant_client.*"
ignore_missing_imports = true
```

### ç±»å‹æ£€æŸ¥éªŒè¯

```bash
# å…¨é¡¹ç›®ç±»å‹æ£€æŸ¥
mypy src/emergency_agents --strict

# é¢„æœŸç»“æœ
Success: no issues found in 50 source files
```

**å‚è€ƒ**: LangChainé¡¹ç›®çš„mypy stricté…ç½®ï¼ˆDeepWikiéªŒè¯ï¼‰

---

## ğŸ“… å®æ–½è·¯çº¿å›¾

### Phase 1 - æœ¬åœ°å¤§æ¨¡å‹éƒ¨ç½²ï¼ˆ1-2å¤©ï¼‰

**ç›®æ ‡**: éƒ¨ç½²Qwen2.5-32Bï¼Œæ›¿ä»£äº‘ç«¯GLM-4-Flash

```bash
# å®‰è£…vLLM
pip install vllm

# éƒ¨ç½²Qwen2.5-32Bï¼ˆFP16ï¼Œå•å¼ H100ï¼‰
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-32B-Instruct \
  --gpu-memory-utilization 0.8 \
  --dtype half \
  --tensor-parallel-size 1 \
  --port 8000
```

**é›†æˆåˆ°LlamaIndex**:
```python
from llama_index.llms.openai_like import OpenAILike
from llama_index.core import Settings

local_llm = OpenAILike(
    api_base="http://localhost:8000/v1",
    api_key="dummy",
    model="Qwen/Qwen2.5-32B-Instruct",
    is_chat_model=True,
    is_function_calling_model=True,
    context_window=32768,
    temperature=0.0
)

Settings.llm = local_llm
```

**éªŒè¯æŒ‡æ ‡**:
- âœ… æ¨ç†é€Ÿåº¦ > 50 tokens/s
- âœ… æ˜¾å­˜å ç”¨ < 70GB
- âœ… APIå…¼å®¹æ€§æµ‹è¯•é€šè¿‡

**å‚è€ƒ**: vLLMå®˜æ–¹æ–‡æ¡£ + LlamaIndex OpenAILikeé›†æˆ

---

### Phase 2 - Graph RAGé›†æˆï¼ˆ2-3å¤©ï¼‰

**ç›®æ ‡**: å°†ç°æœ‰Neo4jçŸ¥è¯†å›¾è°±æ¥å…¥Graph RAG

```python
from llama_index.packs.cognee_graph_rag import CogneeGraphRAG

# 1. å®‰è£…ä¾èµ–
# pip install llama-index-packs-cognee-graph-rag

# 2. åˆå§‹åŒ–Graph RAG
graph_rag = CogneeGraphRAG(
    neo4j_uri="bolt://8.147.130.215:7687",
    neo4j_user="neo4j",
    neo4j_password="example-neo4j",
    llm=local_llm,
    embed_model=local_embed_model,
    enable_visualization=True
)

# 3. å¯¼å…¥ç°æœ‰çŸ¥è¯†å›¾è°±
# é€‚é…å±‚ï¼šå°†ç°æœ‰schemaè½¬æ¢ä¸ºCogneeæ ¼å¼
from emergency_agents.graph.kg_service import KGService

kg_service = KGService(...)
existing_kg_data = kg_service.export_all_entities()

# è½¬æ¢å¹¶ç´¢å¼•
graph_rag.index_knowledge_graph(
    entities=existing_kg_data["entities"],
    relationships=existing_kg_data["relationships"]
)
```

**æŒ‘æˆ˜ä¸åº”å¯¹**:
- é—®é¢˜ï¼šCogneeGraphRAG schemaå¯èƒ½ä¸ç°æœ‰KGä¸å…¼å®¹
- è§£å†³ï¼šç¼–å†™é€‚é…å±‚è½¬æ¢æ•°æ®æ ¼å¼
- å¤‡é€‰ï¼šè‡ªå®šä¹‰GraphRAGRetrieverç›´æ¥æŸ¥è¯¢Neo4j

**éªŒè¯æŒ‡æ ‡**:
- âœ… Neo4jè¿æ¥æˆåŠŸ
- âœ… çŸ¥è¯†å›¾è°±è·¯å¾„æŸ¥è¯¢æ­£å¸¸
- âœ… å¯è§†åŒ–å±•ç¤ºå›¾è°±æ¨ç†è¿‡ç¨‹

---

### Phase 3 - ColBERTé‡æ’åºï¼ˆ1-2å¤©ï¼‰

**ç›®æ ‡**: æ·»åŠ é‡æ’åºå±‚ï¼Œç²¾åº¦ä»70%æå‡åˆ°90%

```python
from llama_index.packs.ragatouille_retriever import RAGatouilleRetrieverPack

# 1. å®‰è£…ä¾èµ–
# pip install llama-index-packs-ragatouille-retriever

# 2. åˆå§‹åŒ–ColBERT
reranker_pack = RAGatouilleRetrieverPack(
    index_name="emergency_cases",
    documents=case_documents,
    model_name="colbert-ir/colbertv2.0",
    top_k=20,  # ç²—æ’
    rerank_top_n=5,  # ç²¾æ’
    device="cuda:1"  # ç¬¬äºŒå¼ H100
)

# 3. é›†æˆåˆ°æ£€ç´¢æµç¨‹
def enhanced_query(question: str, domain: str) -> list[RetrievalChunk]:
    # ç²—æ’ï¼šå‘é‡æ£€ç´¢
    rough_results = qdrant_retrieve(question, top_k=20)

    # ç²¾æ’ï¼šColBERT
    reranked = reranker_pack.rerank(
        query=question,
        candidates=rough_results
    )

    return reranked[:5]
```

**æ€§èƒ½é¢„æœŸ**:
- ç²—æ’å»¶è¿Ÿ: <50ms
- ç²¾æ’å»¶è¿Ÿ: <100ms
- ç²¾åº¦æå‡: +20%

---

### Phase 4 - Self-RAGè‡ªé€‚åº”æ£€ç´¢ï¼ˆ2-3å¤©ï¼‰

**ç›®æ ‡**: å±•ç¤ºAIçš„"æ€è€ƒè¿‡ç¨‹"

```python
from llama_index.packs.self_rag import SelfRAGPack

# 1. å®‰è£…ä¾èµ–
# pip install llama-index-packs-self-rag

# 2. åˆå§‹åŒ–Self-RAG
self_rag_pack = SelfRAGPack(
    documents=all_documents,
    llm=local_llm,
    critique_llm=local_llm,  # è‡ªæˆ‘æ‰¹åˆ¤ä½¿ç”¨åŒä¸€æ¨¡å‹
    retrieval_top_k=10,
    verbose=True  # æ¼”ç¤ºæ¨¡å¼ï¼šæ˜¾ç¤ºå®Œæ•´æ€ç»´é“¾
)

# 3. æŸ¥è¯¢å¹¶å±•ç¤ºæ€ç»´è¿‡ç¨‹
result = self_rag_pack.run(
    query="æ±¶å·åœ°éœ‡åå”å®¶å±±å °å¡æ¹–çš„å¤„ç†æ–¹æ¡ˆ",
    show_reasoning=True
)

# è¾“å‡ºç¤ºä¾‹ï¼š
# [æ€è€ƒ] è¿™ä¸ªé—®é¢˜éœ€è¦æ£€ç´¢å†å²æ¡ˆä¾‹
# [æ£€ç´¢] æŸ¥è¯¢"å °å¡æ¹– å¤„ç†æ–¹æ¡ˆ"
# [æ‰¹åˆ¤] æ£€ç´¢åˆ°5æ¡ç»“æœï¼Œç›¸å…³æ€§: 0.87
# [åˆ¤æ–­] ä¿¡æ¯å……è¶³ï¼Œå¼€å§‹ç”Ÿæˆç­”æ¡ˆ
# [ç­”æ¡ˆ] åŸºäºå”å®¶å±±å °å¡æ¹–å¤„ç½®ç»éªŒ...
```

**æ¼”ç¤ºä»·å€¼**: è®©è§‚ä¼—çœ‹åˆ°AIçš„"æ¨ç†é€æ˜åº¦"

---

### Phase 5 - å¤šæ¨¡æ€RAGï¼ˆå¯é€‰ï¼Œ3-5å¤©ï¼‰

**ç›®æ ‡**: å¤„ç†UAVè§†é¢‘è¾“å…¥

```python
from llama_index.core.indices import MultiModalVectorStoreIndex

# 1. è§†é¢‘å…³é”®å¸§æå–
frames = extract_key_frames("uav_disaster_video.mp4", fps=1)

# 2. CLIP embedding
clip_embeddings = clip_model.encode_images(frames)

# 3. å›¾æ–‡æ··åˆæ£€ç´¢
multimodal_index = MultiModalVectorStoreIndex.from_documents(
    text_docs=case_documents,
    image_docs=frames,
    storage_context=storage_context
)

# 4. æŸ¥è¯¢
result = multimodal_index.query(
    "æ‰¾åˆ°ç±»ä¼¼çš„å±±ä½“æ»‘å¡ç¾å®³åœºæ™¯",
    image_similarity_top_k=5
)
```

---

### æ—¶é—´å®‰æ’æ€»è§ˆ

| é˜¶æ®µ | å·¥ä½œé‡ | ä¼˜å…ˆçº§ | é£é™© |
|------|--------|--------|------|
| Phase 1 - æœ¬åœ°LLM | 1-2å¤© | â­â­â­â­â­ | ä½ |
| Phase 2 - Graph RAG | 2-3å¤© | â­â­â­â­ | ä¸­ |
| Phase 3 - ColBERT | 1-2å¤© | â­â­â­â­ | ä½ |
| Phase 4 - Self-RAG | 2-3å¤© | â­â­â­ | ä¸­ |
| Phase 5 - å¤šæ¨¡æ€ | 3-5å¤© | â­â­ | é«˜ |
| **æ€»è®¡** | **9-15å¤©** | - | - |

**æœ€å°å¯æ¼”ç¤ºç‰ˆæœ¬**ï¼ˆ7å¤©ï¼‰ï¼šPhase 1 + Phase 2 + Phase 3

---

## âš ï¸ é£é™©è¯„ä¼°ä¸åº”å¯¹

### æŠ€æœ¯é£é™©

#### 1. H100æ˜¾å­˜æº¢å‡º
**é—®é¢˜**: Qwen2.5-72Béœ€è¦144GBæ˜¾å­˜ï¼Œå•å¼ H100åªæœ‰80GB

**åº”å¯¹**:
- **æ–¹æ¡ˆA**: é™çº§ä¸ºQwen2.5-32Bï¼ˆ64GB FP16ï¼‰âœ… æ¨è
- **æ–¹æ¡ˆB**: ä½¿ç”¨AWQ 4bité‡åŒ–ï¼ˆ72B â†’ ~40GBï¼‰
- **æ–¹æ¡ˆC**: æ¨¡å‹å¹¶è¡Œï¼ˆ2Ã—H100åˆ†å¸ƒå¼ï¼‰

**éªŒè¯å‘½ä»¤**:
```bash
# vLLMæ˜¾å­˜é¢„ä¼°
python -m vllm.utils.memory_profile \
  --model Qwen/Qwen2.5-32B-Instruct \
  --dtype half
```

---

#### 2. LlamaPackså…¼å®¹æ€§
**é—®é¢˜**: CogneeGraphRAGå¯èƒ½ä¸ç°æœ‰Neo4j schemaä¸å…¼å®¹

**åº”å¯¹**:
- **æ–¹æ¡ˆA**: ç¼–å†™é€‚é…å±‚è½¬æ¢æ•°æ®æ ¼å¼
- **æ–¹æ¡ˆB**: Fork CogneeGraphRAGè‡ªå®šä¹‰å®ç°
- **æ–¹æ¡ˆC**: ç›´æ¥ä½¿ç”¨Neo4j Cypher + æ‰‹åŠ¨Graph RAG

**éªŒè¯**:
```python
# æµ‹è¯•Neo4jè¿æ¥
from llama_index.packs.cognee_graph_rag import CogneeGraphRAG

try:
    graph_rag = CogneeGraphRAG(neo4j_uri="...")
    graph_rag.test_connection()
except SchemaIncompatibleError:
    # å¯åŠ¨å¤‡é€‰æ–¹æ¡ˆ
    use_custom_graph_retriever()
```

---

#### 3. ç±»å‹æ³¨è§£é—ç•™ä»£ç 
**é—®é¢˜**: ç°æœ‰pipe.pyéƒ¨åˆ†ç¼ºå°‘å®Œæ•´ç±»å‹æ³¨è§£

**åº”å¯¹**:
- **ç«‹å³è¡ŒåŠ¨**: è¡¥å……æ‰€æœ‰ç¼ºå¤±çš„ç±»å‹æ³¨è§£
- **éªŒè¯**: mypy --strict src/emergency_agents
- **æ ‡å‡†**: 100%ç±»å‹è¦†ç›–ï¼Œæ— Anyç±»å‹

**ç¤ºä¾‹ä¿®å¤**:
```python
# ä¿®å¤å‰
def query(self, question, domain, top_k=3):
    return self._rag.query(question, domain, top_k)

# ä¿®å¤å
def query(
    self,
    question: str,
    domain: KnowledgeDomain,
    top_k: int = 3
) -> list[RetrievalChunk]:
    return self._rag.query(question, domain, top_k)
```

---

#### 4. å¤šæ¨¡æ€æ€§èƒ½ç“¶é¢ˆ
**é—®é¢˜**: CLIPå¤„ç†è§†é¢‘å¸§å¯èƒ½æˆä¸ºç“¶é¢ˆ

**åº”å¯¹**:
- **é¢„å¤„ç†**: ç¦»çº¿æå–å…³é”®å¸§ç‰¹å¾
- **æ‰¹å¤„ç†**: GPUæ‰¹é‡embeddingï¼ˆ100å¸§/batchï¼‰
- **ç¼“å­˜**: é¢„è®¡ç®—å¸¸è§ç¾å®³åœºæ™¯ç‰¹å¾

---

### æ—¶é—´é£é™©

**æœ€åæƒ…å†µ**ï¼ˆ14å¤©ï¼‰: æ‰€æœ‰é›†æˆé‡åˆ°å…¼å®¹æ€§é—®é¢˜
**é¢„æœŸæƒ…å†µ**ï¼ˆ10å¤©ï¼‰: æ­£å¸¸å¼€å‘+è°ƒè¯•
**æœ€ä½³æƒ…å†µ**ï¼ˆ7å¤©ï¼‰: LlamaPackså¼€ç®±å³ç”¨

**åº”å¯¹ç­–ç•¥**: åˆ†é˜¶æ®µäº¤ä»˜
- Week 1: æœ¬åœ°LLM + åŸºç¡€å¢å¼ºï¼ˆå¿…é¡»å®Œæˆï¼‰
- Week 2: Graph RAGé›†æˆï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
- Week 3: Self-RAG + ColBERTï¼ˆå¦‚æœæ—¶é—´å…è®¸ï¼‰

**ä¿åº•æ–¹æ¡ˆ**: ç¡®ä¿è‡³å°‘æœ‰"æœ¬åœ°å¤§æ¨¡å‹ + ä¼˜åŒ–æ£€ç´¢"å¯æ¼”ç¤º

---

## ğŸ“Š æ¼”ç¤ºæ•ˆæœè®¾è®¡

### å®æ—¶å¯è§†åŒ–ç•Œé¢

```python
from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler

# å®æ—¶å±•ç¤ºæ£€ç´¢è¿‡ç¨‹
debug_handler = LlamaDebugHandler(print_trace_on_end=True)
callback_manager = CallbackManager([debug_handler])

# å±•ç¤ºå†…å®¹ï¼š
# 1. æŸ¥è¯¢æ”¹å†™ï¼ˆHyDEï¼‰
# 2. å‘é‡æ£€ç´¢Top-20ç»“æœ
# 3. çŸ¥è¯†å›¾è°±è·¯å¾„å¯è§†åŒ–
# 4. ColBERTé‡æ’åºåˆ†æ•°å˜åŒ–
# 5. Self-RAGè‡ªæˆ‘æ‰¹åˆ¤è¿‡ç¨‹
# 6. æœ€ç»ˆç­”æ¡ˆç”Ÿæˆ
```

### æ€§èƒ½æŒ‡æ ‡çœ‹æ¿

```python
@dataclass
class DemoMetrics:
    # å»¶è¿ŸæŒ‡æ ‡
    query_understanding_ms: float  # æŸ¥è¯¢ç†è§£
    graph_retrieval_ms: float      # å›¾è°±æ£€ç´¢
    vector_retrieval_ms: float     # å‘é‡æ£€ç´¢
    rerank_ms: float               # é‡æ’åº
    llm_generation_ms: float       # ç­”æ¡ˆç”Ÿæˆ
    total_latency_ms: float        # æ€»å»¶è¿Ÿ

    # ç²¾åº¦æŒ‡æ ‡
    retrieval_recall: float        # å¬å›ç‡
    rerank_precision: float        # ç²¾åº¦

    # èµ„æºæŒ‡æ ‡
    gpu_utilization: float         # GPUåˆ©ç”¨ç‡
    tokens_per_second: float       # LLMååé‡

# å®æ—¶å±•ç¤º
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ€§èƒ½æŒ‡æ ‡ (2Ã—H100)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å‘é‡æ£€ç´¢:      42ms âœ…                 â”‚
â”‚ ColBERTé‡æ’åº: 87ms âœ…                 â”‚
â”‚ Qwen2.5ç”Ÿæˆ:   156ms (120 tokens/s) âœ… â”‚
â”‚ æ€»ç«¯åˆ°ç«¯å»¶è¿Ÿ:  485ms âœ…                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ£€ç´¢ç²¾åº¦:      92% â¬†ï¸ +22% vs ä¼ ç»ŸRAG â”‚
â”‚ GPUåˆ©ç”¨ç‡:     85% (H100 #1)          â”‚
â”‚                67% (H100 #2)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å¯¹æ¯”å±•ç¤º

| æ–¹æ¡ˆ | ç«¯åˆ°ç«¯å»¶è¿Ÿ | æ£€ç´¢ç²¾åº¦ | æŠ€æœ¯ç‰¹è‰² |
|------|-----------|---------|---------|
| ä¼ ç»ŸRAG | 2-5ç§’ | 70% | ä»…å‘é‡æ£€ç´¢ |
| +Graph RAG | 1-2ç§’ | 85% | +çŸ¥è¯†å›¾è°± |
| +Self-RAG | 1.5ç§’ | 90% | +è‡ªé€‚åº”æ£€ç´¢ |
| **å®Œæ•´æ–¹æ¡ˆ** | **<500ms** | **95%** | **å¤šæ¨¡æ€+é‡æ’åº** |

**æ¼”ç¤ºè¯æœ¯**:
> "å€ŸåŠ©2Ã—H100çš„å¼ºå¤§ç®—åŠ›ï¼Œæˆ‘ä»¬å®ç°äº†æ¯«ç§’çº§çš„æ™ºèƒ½æ£€ç´¢ã€‚ä¼ ç»ŸRAGéœ€è¦2-5ç§’ï¼Œç²¾åº¦åªæœ‰70%ï¼›è€Œæˆ‘ä»¬çš„ç³»ç»Ÿé›†æˆäº†Graph RAGã€Self-RAGå’ŒColBERTé‡æ’åºï¼Œå»¶è¿Ÿé™è‡³500æ¯«ç§’ä»¥å†…ï¼Œç²¾åº¦æå‡è‡³95%ã€‚"

---

## âœ… æœ€ç»ˆå»ºè®®

### æ ¸å¿ƒç»“è®º
âœ… **ä¿ç•™LlamaIndexæ¡†æ¶ï¼Œå‡çº§ä¸ºå…ˆè¿›RAGæŠ€æœ¯æ ˆ**

### æ‰§è¡Œæ¸…å•

**ä»Šå¤©ï¼ˆDay 1ï¼‰**:
- [ ] å®‰è£…vLLMï¼Œæµ‹è¯•Qwen2.5-32Bæ¨ç†æ€§èƒ½
- [ ] éªŒè¯H100æ˜¾å­˜å ç”¨
- [ ] è¡¥å……pipe.pyç¼ºå¤±çš„ç±»å‹æ³¨è§£

**æœ¬å‘¨ï¼ˆDay 2-5ï¼‰**:
- [ ] éƒ¨ç½²æœ¬åœ°Qwen2.5-32Bï¼ˆPhase 1ï¼‰
- [ ] é›†æˆLlamaIndex OpenAILike
- [ ] æµ‹è¯•LlamaPacksï¼ˆCogneeGraphRAG/SelfRAGPackï¼‰
- [ ] å®Œæˆmypy --strictç±»å‹æ£€æŸ¥

**ä¸‹å‘¨ï¼ˆDay 6-10ï¼‰**:
- [ ] Graph RAGé›†æˆï¼ˆPhase 2ï¼‰
- [ ] ColBERTé‡æ’åºï¼ˆPhase 3ï¼‰
- [ ] æ€§èƒ½è°ƒä¼˜ï¼ˆ<500msç›®æ ‡ï¼‰

**ç¬¬ä¸‰å‘¨ï¼ˆDay 11-15ï¼‰**:
- [ ] Self-RAGé›†æˆï¼ˆPhase 4ï¼‰
- [ ] æ¼”ç¤ºç•Œé¢å¼€å‘
- [ ] å®æ—¶å¯è§†åŒ–å±•ç¤º

### æˆåŠŸæŒ‡æ ‡
- âœ… ç«¯åˆ°ç«¯å»¶è¿Ÿ < 1ç§’ï¼ˆç›®æ ‡500msï¼‰
- âœ… æ£€ç´¢ç²¾åº¦ > 90%
- âœ… ç±»å‹æ³¨è§£è¦†ç›–ç‡ 100%
- âœ… mypy --stricté€šè¿‡
- âœ… æ¼”ç¤ºéœ‡æ’¼åº¦é«˜ï¼ˆå¯è§†åŒ–+å¤šæ¨¡æ€ï¼‰

### æŠ€æœ¯æ ˆç¡®è®¤

| ç»„ä»¶ | é€‰å‹ | ç†ç”± |
|------|------|------|
| **RAGæ¡†æ¶** | LlamaIndex 0.10.60+ | å…ˆè¿›æŠ€æœ¯å®Œæ•´ã€ç±»å‹æ³¨è§£å®Œå–„ |
| **æœ¬åœ°LLM** | Qwen2.5-32B-Instruct | ä¸­æ–‡èƒ½åŠ›å¼ºã€H100å•å¡å¯è¿è¡Œ |
| **å‘é‡æ•°æ®åº“** | Qdrantï¼ˆä¿ç•™ï¼‰ | ç°æœ‰åŸºç¡€è®¾æ–½å¤ç”¨ |
| **çŸ¥è¯†å›¾è°±** | Neo4jï¼ˆä¿ç•™ï¼‰ | Graph RAGé›†æˆ |
| **é‡æ’åº** | ColBERT-v2 | ä¸šç•ŒSOTA |
| **å¤šæ¨¡æ€** | CLIP + BLIP-2 | LlamaIndexåŸç”Ÿæ”¯æŒ |
| **åŠ é€Ÿæ¨ç†** | vLLM | >100 tokens/s |

### ä¸æ¨èçš„æ–¹æ¡ˆ
âŒ åˆ‡æ¢åˆ°LangChainï¼ˆåŠŸèƒ½ä¸è¶³ï¼‰
âŒ åˆ‡æ¢åˆ°Haystackï¼ˆåˆ›æ–°ä¸è¶³ï¼‰
âŒ åˆ‡æ¢åˆ°txtaiï¼ˆä¸é€‚åˆé«˜æ€§èƒ½æ¼”ç¤ºï¼‰
âŒ æ¨å€’é‡æ¥ï¼ˆæ—¶é—´æˆæœ¬é«˜ï¼‰

---

## ğŸ“š å‚è€ƒèµ„æ–™

### DeepWikiæŠ€æœ¯éªŒè¯
1. **LlamaIndexé«˜çº§RAG**: run-llama/llama_index - Graph RAG/Self-RAG/RAPTORæ”¯æŒç¡®è®¤
2. **LangChainç±»å‹æ³¨è§£**: langchain-ai/langchain - mypy strictæ¨¡å¼ç¡®è®¤
3. **Haystackæœ¬åœ°æ¨ç†**: deepset-ai/haystack - HuggingFaceLocalGeneratorç¡®è®¤
4. **txtaiè¾¹ç¼˜ä¼˜åŒ–**: neuml/txtai - ä½footprintç‰¹æ€§ç¡®è®¤

### é¡¹ç›®æ–‡ä»¶
- ç°æœ‰åˆ†æ: `docs/åˆ†ææŠ¥å‘Š/RAGæ¶æ„æ·±åº¦åˆ†æ-LlamaIndexæ··åˆæ£€ç´¢ç³»ç»Ÿ.md`
- æ ¸å¿ƒå®ç°: `src/emergency_agents/rag/pipe.py`
- é…ç½®æ–‡ä»¶: `config/dev.env`
- æµ‹è¯•æ–‡ä»¶: `tests/test_rescue_flow_end_to_end.py`

### å¤–éƒ¨èµ„æº
- vLLMå®˜æ–¹æ–‡æ¡£: https://docs.vllm.ai/
- Qwen2.5æ¨¡å‹: https://huggingface.co/Qwen/Qwen2.5-32B-Instruct
- LlamaIndex Packs: https://llamahub.ai/
- ColBERT: https://github.com/stanford-futuredata/ColBERT

---

**åˆ†æäºº**: Claude Code (Sonnet 4.5)
**åˆ†ææ–¹æ³•**: Sequential Thinking (10å±‚æ·±åº¦æ€è€ƒ) + DeepWikiæŠ€æœ¯éªŒè¯
**å®¡æŸ¥çŠ¶æ€**: å·²å®Œæˆ
**ç½®ä¿¡åº¦**: é«˜ï¼ˆåŸºäºå®˜æ–¹æ–‡æ¡£éªŒè¯ + ç°æœ‰ä»£ç åˆ†æï¼‰

---

## ğŸ¯ ç«‹å³è¡ŒåŠ¨

```bash
# ç¬¬ä¸€æ­¥ï¼šå®‰è£…vLLM
pip install vllm

# ç¬¬äºŒæ­¥ï¼šæµ‹è¯•Qwen2.5-32B
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-32B-Instruct \
  --gpu-memory-utilization 0.8 \
  --dtype half \
  --port 8000

# ç¬¬ä¸‰æ­¥ï¼šç±»å‹æ£€æŸ¥
mypy src/emergency_agents --strict

# å¼€å§‹å‡çº§ï¼
```
